{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Approach for Unet Training \n",
    "\n",
    "------\n",
    "\n",
    "## Overview\n",
    "\n",
    "\n",
    "\n",
    "The purpose of this learn.ipynb notebook is to investigate whether an image can exhibit a preference for being segmented more effectively using a UNet model trained on polar or cartesian-dominant images.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Structure\n",
    "```\n",
    "data\n",
    "└── endoscopic\n",
    "    ├── cartesian\n",
    "    │   ├── image\n",
    "    │   └── label\n",
    "    └── polar\n",
    "        ├── image\n",
    "        └── label\n",
    "```\n",
    "\n",
    "Inside of each end folder there are 956 images, named as `0.tif` to `955.tif`\n",
    "and I believe, for now, the naming of the images are one to one correctly matched, meaning the ``/data/endoscopic/**cartesian**/image/0.tif`` is transformed from `/data/endoscopic/**polar**/image/0.tif`\n",
    "\n",
    "Instead of putting a seperate set of images aside to be test set, we chose to use k-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from defines import *\n",
    "from model import *\n",
    "from data import *\n",
    "import sys\n",
    "\n",
    "from numpy import loadtxt\n",
    "from keras.models import load_model\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from skimage.io import imread\n",
    "from skimage import img_as_ubyte\n",
    "from skimage.transform import resize\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import math\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from files and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this if your computer has a cuda visible device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PARAM_SYSTEM_TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test code for correct file structure setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize folder tree in current directory\n",
    "os.system(\"tree -d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count of files in data directories\n",
    "os.system(\"ls \" + os.path.join(PARAM_PATH_CARTE,PARAM_IMG_FOLDER) + \" | wc -l\")\n",
    "os.system(\"ls \" + os.path.join(PARAM_PATH_CARTE,PARAM_MSK_FOLDER) + \" | wc -l\")\n",
    "os.system(\"ls \" + os.path.join(PARAM_PATH_POLAR,PARAM_IMG_FOLDER) + \" | wc -l\")\n",
    "os.system(\"ls \" + os.path.join(PARAM_PATH_POLAR,PARAM_MSK_FOLDER) + \" | wc -l\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Expected Output:\n",
    "7404\\\n",
    "7404\\\n",
    "7404\\\n",
    "7404\\\n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Relocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following block, the code loads in one analysis file from previous research. \n",
    "\n",
    "### #File name postfix\n",
    "_C_ is the dice scores of the predictions generated by Unet C: this Unet C is trained using all 7404 images, in their cartesian form. The raw image was directly input into the Unet and the prediction was generated.\n",
    "\n",
    "_P_ is the dice scores of the predictions generated by Unet P: this Unet P is trained using all 7404 images but in their polar form. The raw images were transformed, and then input for prediction. The prediction is in polar space.\n",
    "\n",
    "_P2C_ is the dice scores of the predictions generated by the same Unet P as mentioned above, but the dice score is generated by transforming the prediction back to cartesian, and compared to their original label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name = 'analysis_dice_back_Test_C.npy'\n",
    "#file_name = 'analysis_dice_back_Test_P.npy'\n",
    "#file_name = 'analysis_dice_back_Test_P2C.npy'\n",
    "file_name = 'analysis_dice_back_Train_C.npy'\n",
    "\n",
    "np_file = os.path.join(PARAM_PATH_SCORES, file_name)\n",
    "#load npy file\n",
    "img_score = np.load(np_file)\n",
    "\n",
    "#sort scores in descending order and store index\n",
    "sorted_score = np.flip(np.argsort(img_score))\n",
    "\n",
    "#------DEBUG--------\n",
    "#print(len(sorted_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sorted_score` should be a list of length 7404, sorted by the method we chose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_score = pd.DataFrame(sorted_score)\n",
    "\n",
    "#fetch top polar dominant and non-polar dominant image\n",
    "num_polar = round(len(sorted_score)/2)\n",
    "num_cartesian = len(sorted_score) - num_polar\n",
    "dfPolar = sorted_score.head(num_polar)\n",
    "dfCartesian = sorted_score.tail(num_cartesian)\n",
    "#print(\"Polar: \\n\", dfPolar)\n",
    "#print(\"Cartesian: \\n\", dfCartesian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in `dfPolar` should be the best half of images (filename), which performs better than the other half, according to the data we used above. In `dfCartesian` there's the other half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from filePrep import *\n",
    "\n",
    "K = 5\n",
    "\n",
    "checkNcreateTempFolder(PARAM_PATH_TEMP_POLAR, K)\n",
    "checkNcreateTempFolder(PARAM_PATH_TEMP_CARTE, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two lines creates the new temporary folder.\n",
    "Instead of making kfolds later, the kfolds is assigned now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits = K, shuffle = True, random_state = 42) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the KFold package to assign the paramters like n_splits and so.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for train_index,test_index in kf.split(dfPolar):\n",
    "    fillFolder(test_index, dfPolar, PARAM_PATH_POLAR, PARAM_PATH_CARTE, PARAM_PATH_TEMP_POLAR, i)\n",
    "    i += 1\n",
    "i = 0\n",
    "print('------------------------------------')\n",
    "for train_index,test_index in kf.split(dfCartesian):\n",
    "    fillFolder(test_index, dfCartesian, PARAM_PATH_POLAR, PARAM_PATH_CARTE, PARAM_PATH_TEMP_CARTE, i)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we should have all `k-folds` set up. The next step is to write a training loop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the limitation of flow_from_directory, which, it does not allow the combination of multiple directories. And, the limitation of my knowledge of flow_from_dataframe. **I believe this is solvable using flow_from_dataframe** I decide to move training set into a separate temporary folder when training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2961 images belonging to 1 classes.\n",
      "Found 2961 images belonging to 1 classes.\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f841c5e80e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f841c5e80e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6402 - accuracy: 0.4389 - dice_coef_loss: 0.6402\n",
      "Epoch 1: loss improved from inf to 0.64016, saving model to ./temp/polar_Dom/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 264ms/step - loss: 0.6402 - accuracy: 0.4389 - dice_coef_loss: 0.6402\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6135 - accuracy: 0.6664 - dice_coef_loss: 0.6135\n",
      "Epoch 2: loss improved from 0.64016 to 0.61351, saving model to ./temp/polar_Dom/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 269ms/step - loss: 0.6135 - accuracy: 0.6664 - dice_coef_loss: 0.6135\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5999 - accuracy: 0.7434 - dice_coef_loss: 0.5999\n",
      "Epoch 3: loss improved from 0.61351 to 0.59986, saving model to ./temp/polar_Dom/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 269ms/step - loss: 0.5999 - accuracy: 0.7434 - dice_coef_loss: 0.5999\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5876 - accuracy: 0.7619 - dice_coef_loss: 0.5876\n",
      "Epoch 4: loss improved from 0.59986 to 0.58756, saving model to ./temp/polar_Dom/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 269ms/step - loss: 0.5876 - accuracy: 0.7619 - dice_coef_loss: 0.5876\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5882 - accuracy: 0.7885 - dice_coef_loss: 0.5882\n",
      "Epoch 5: loss did not improve from 0.58756\n",
      "100/100 [==============================] - 26s 261ms/step - loss: 0.5882 - accuracy: 0.7885 - dice_coef_loss: 0.5882\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5856 - accuracy: 0.7912 - dice_coef_loss: 0.5856\n",
      "Epoch 6: loss improved from 0.58756 to 0.58557, saving model to ./temp/polar_Dom/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 269ms/step - loss: 0.5856 - accuracy: 0.7912 - dice_coef_loss: 0.5856\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5909 - accuracy: 0.7993 - dice_coef_loss: 0.5909\n",
      "Epoch 7: loss did not improve from 0.58557\n",
      "100/100 [==============================] - 26s 261ms/step - loss: 0.5909 - accuracy: 0.7993 - dice_coef_loss: 0.5909\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5744 - accuracy: 0.8089 - dice_coef_loss: 0.5748\n",
      "Epoch 8: loss improved from 0.58557 to 0.57439, saving model to ./temp/polar_Dom/checkpoint.hdf5\n",
      "100/100 [==============================] - 28s 282ms/step - loss: 0.5744 - accuracy: 0.8089 - dice_coef_loss: 0.5748\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5736 - accuracy: 0.8034 - dice_coef_loss: 0.5736\n",
      "Epoch 9: loss improved from 0.57439 to 0.57357, saving model to ./temp/polar_Dom/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 270ms/step - loss: 0.5736 - accuracy: 0.8034 - dice_coef_loss: 0.5736\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5859 - accuracy: 0.8083 - dice_coef_loss: 0.5859\n",
      "Epoch 10: loss did not improve from 0.57357\n",
      "100/100 [==============================] - 26s 261ms/step - loss: 0.5859 - accuracy: 0.8083 - dice_coef_loss: 0.5859\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5837 - accuracy: 0.8145 - dice_coef_loss: 0.5837\n",
      "Epoch 11: loss did not improve from 0.57357\n",
      "100/100 [==============================] - 26s 261ms/step - loss: 0.5837 - accuracy: 0.8145 - dice_coef_loss: 0.5837\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5644 - accuracy: 0.8203 - dice_coef_loss: 0.5644\n",
      "Epoch 12: loss improved from 0.57357 to 0.56442, saving model to ./temp/polar_Dom/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 269ms/step - loss: 0.5644 - accuracy: 0.8203 - dice_coef_loss: 0.5644\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5705 - accuracy: 0.8205 - dice_coef_loss: 0.5705\n",
      "Epoch 13: loss did not improve from 0.56442\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5705 - accuracy: 0.8205 - dice_coef_loss: 0.5705\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5687 - accuracy: 0.8324 - dice_coef_loss: 0.5687\n",
      "Epoch 14: loss did not improve from 0.56442\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5687 - accuracy: 0.8324 - dice_coef_loss: 0.5687\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5771 - accuracy: 0.8143 - dice_coef_loss: 0.5768\n",
      "Epoch 15: loss did not improve from 0.56442\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5771 - accuracy: 0.8143 - dice_coef_loss: 0.5768\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5704 - accuracy: 0.8312 - dice_coef_loss: 0.5704\n",
      "Epoch 16: loss did not improve from 0.56442\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5704 - accuracy: 0.8312 - dice_coef_loss: 0.5704\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5751 - accuracy: 0.8341 - dice_coef_loss: 0.5751\n",
      "Epoch 17: loss did not improve from 0.56442\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5751 - accuracy: 0.8341 - dice_coef_loss: 0.5751\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5687 - accuracy: 0.8292 - dice_coef_loss: 0.5687\n",
      "Epoch 18: loss did not improve from 0.56442\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5687 - accuracy: 0.8292 - dice_coef_loss: 0.5687\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5716 - accuracy: 0.8300 - dice_coef_loss: 0.5716\n",
      "Epoch 19: loss did not improve from 0.56442\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5716 - accuracy: 0.8300 - dice_coef_loss: 0.5716\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5621 - accuracy: 0.8298 - dice_coef_loss: 0.5621\n",
      "Epoch 20: loss improved from 0.56442 to 0.56214, saving model to ./temp/polar_Dom/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.5621 - accuracy: 0.8298 - dice_coef_loss: 0.5621\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5632 - accuracy: 0.8412 - dice_coef_loss: 0.5632\n",
      "Epoch 21: loss did not improve from 0.56214\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5632 - accuracy: 0.8412 - dice_coef_loss: 0.5632\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5587 - accuracy: 0.8350 - dice_coef_loss: 0.5587\n",
      "Epoch 22: loss improved from 0.56214 to 0.55871, saving model to ./temp/polar_Dom/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.5587 - accuracy: 0.8350 - dice_coef_loss: 0.5587\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5739 - accuracy: 0.8366 - dice_coef_loss: 0.5737\n",
      "Epoch 23: loss did not improve from 0.55871\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.5739 - accuracy: 0.8366 - dice_coef_loss: 0.5737\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5680 - accuracy: 0.8400 - dice_coef_loss: 0.5680\n",
      "Epoch 24: loss did not improve from 0.55871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5680 - accuracy: 0.8400 - dice_coef_loss: 0.5680\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5598 - accuracy: 0.8380 - dice_coef_loss: 0.5598\n",
      "Epoch 25: loss did not improve from 0.55871\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5598 - accuracy: 0.8380 - dice_coef_loss: 0.5598\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5598 - accuracy: 0.8400 - dice_coef_loss: 0.5598\n",
      "Epoch 26: loss did not improve from 0.55871\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5598 - accuracy: 0.8400 - dice_coef_loss: 0.5598\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5485 - accuracy: 0.8428 - dice_coef_loss: 0.5485\n",
      "Epoch 27: loss improved from 0.55871 to 0.54852, saving model to ./temp/polar_Dom/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.5485 - accuracy: 0.8428 - dice_coef_loss: 0.5485\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5615 - accuracy: 0.8475 - dice_coef_loss: 0.5615\n",
      "Epoch 28: loss did not improve from 0.54852\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5615 - accuracy: 0.8475 - dice_coef_loss: 0.5615\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5680 - accuracy: 0.8486 - dice_coef_loss: 0.5680\n",
      "Epoch 29: loss did not improve from 0.54852\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5680 - accuracy: 0.8486 - dice_coef_loss: 0.5680\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5604 - accuracy: 0.8421 - dice_coef_loss: 0.5598\n",
      "Epoch 30: loss did not improve from 0.54852\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.5604 - accuracy: 0.8421 - dice_coef_loss: 0.5598\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5566 - accuracy: 0.8536 - dice_coef_loss: 0.5566\n",
      "Epoch 31: loss did not improve from 0.54852\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5566 - accuracy: 0.8536 - dice_coef_loss: 0.5566\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5532 - accuracy: 0.8464 - dice_coef_loss: 0.5532\n",
      "Epoch 32: loss did not improve from 0.54852\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5532 - accuracy: 0.8464 - dice_coef_loss: 0.5532\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5611 - accuracy: 0.8460 - dice_coef_loss: 0.5611\n",
      "Epoch 33: loss did not improve from 0.54852\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5611 - accuracy: 0.8460 - dice_coef_loss: 0.5611\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5614 - accuracy: 0.8550 - dice_coef_loss: 0.5614\n",
      "Epoch 34: loss did not improve from 0.54852\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5614 - accuracy: 0.8550 - dice_coef_loss: 0.5614\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5615 - accuracy: 0.8454 - dice_coef_loss: 0.5615\n",
      "Epoch 35: loss did not improve from 0.54852\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5615 - accuracy: 0.8454 - dice_coef_loss: 0.5615\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5666 - accuracy: 0.8423 - dice_coef_loss: 0.5666\n",
      "Epoch 36: loss did not improve from 0.54852\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5666 - accuracy: 0.8423 - dice_coef_loss: 0.5666\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5568 - accuracy: 0.8485 - dice_coef_loss: 0.5568\n",
      "Epoch 37: loss did not improve from 0.54852\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5568 - accuracy: 0.8485 - dice_coef_loss: 0.5568\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5629 - accuracy: 0.8542 - dice_coef_loss: 0.5619\n",
      "Epoch 38: loss did not improve from 0.54852\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.5629 - accuracy: 0.8542 - dice_coef_loss: 0.5619\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5635 - accuracy: 0.8504 - dice_coef_loss: 0.5635\n",
      "Epoch 39: loss did not improve from 0.54852\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5635 - accuracy: 0.8504 - dice_coef_loss: 0.5635\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5594 - accuracy: 0.8552 - dice_coef_loss: 0.5594\n",
      "Epoch 40: loss did not improve from 0.54852\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5594 - accuracy: 0.8552 - dice_coef_loss: 0.5594\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5557 - accuracy: 0.8539 - dice_coef_loss: 0.5557\n",
      "Epoch 41: loss did not improve from 0.54852\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5557 - accuracy: 0.8539 - dice_coef_loss: 0.5557\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5590 - accuracy: 0.8494 - dice_coef_loss: 0.5590\n",
      "Epoch 42: loss did not improve from 0.54852\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5590 - accuracy: 0.8494 - dice_coef_loss: 0.5590\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5493 - accuracy: 0.8614 - dice_coef_loss: 0.5493\n",
      "Epoch 43: loss did not improve from 0.54852\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5493 - accuracy: 0.8614 - dice_coef_loss: 0.5493\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5475 - accuracy: 0.8646 - dice_coef_loss: 0.5475\n",
      "Epoch 44: loss improved from 0.54852 to 0.54750, saving model to ./temp/polar_Dom/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.5475 - accuracy: 0.8646 - dice_coef_loss: 0.5475\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5648 - accuracy: 0.8555 - dice_coef_loss: 0.5680\n",
      "Epoch 45: loss did not improve from 0.54750\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.5648 - accuracy: 0.8555 - dice_coef_loss: 0.5680\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5580 - accuracy: 0.8651 - dice_coef_loss: 0.5580\n",
      "Epoch 46: loss did not improve from 0.54750\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5580 - accuracy: 0.8651 - dice_coef_loss: 0.5580\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5468 - accuracy: 0.8523 - dice_coef_loss: 0.5468\n",
      "Epoch 47: loss improved from 0.54750 to 0.54683, saving model to ./temp/polar_Dom/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.5468 - accuracy: 0.8523 - dice_coef_loss: 0.5468\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5577 - accuracy: 0.8548 - dice_coef_loss: 0.5577\n",
      "Epoch 48: loss did not improve from 0.54683\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5577 - accuracy: 0.8548 - dice_coef_loss: 0.5577\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5521 - accuracy: 0.8641 - dice_coef_loss: 0.5521\n",
      "Epoch 49: loss did not improve from 0.54683\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5521 - accuracy: 0.8641 - dice_coef_loss: 0.5521\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5558 - accuracy: 0.8615 - dice_coef_loss: 0.5558\n",
      "Epoch 50: loss did not improve from 0.54683\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5558 - accuracy: 0.8615 - dice_coef_loss: 0.5558\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5473 - accuracy: 0.8641 - dice_coef_loss: 0.5473\n",
      "Epoch 51: loss did not improve from 0.54683\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5473 - accuracy: 0.8641 - dice_coef_loss: 0.5473\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5580 - accuracy: 0.8626 - dice_coef_loss: 0.5576\n",
      "Epoch 52: loss did not improve from 0.54683\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.5580 - accuracy: 0.8626 - dice_coef_loss: 0.5576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5533 - accuracy: 0.8574 - dice_coef_loss: 0.5533\n",
      "Epoch 53: loss did not improve from 0.54683\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5533 - accuracy: 0.8574 - dice_coef_loss: 0.5533\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5472 - accuracy: 0.8611 - dice_coef_loss: 0.5472\n",
      "Epoch 54: loss did not improve from 0.54683\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5472 - accuracy: 0.8611 - dice_coef_loss: 0.5472\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5621 - accuracy: 0.8709 - dice_coef_loss: 0.5621\n",
      "Epoch 55: loss did not improve from 0.54683\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5621 - accuracy: 0.8709 - dice_coef_loss: 0.5621\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5440 - accuracy: 0.8630 - dice_coef_loss: 0.5440\n",
      "Epoch 56: loss improved from 0.54683 to 0.54396, saving model to ./temp/polar_Dom/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.5440 - accuracy: 0.8630 - dice_coef_loss: 0.5440\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5532 - accuracy: 0.8686 - dice_coef_loss: 0.5532\n",
      "Epoch 57: loss did not improve from 0.54396\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5532 - accuracy: 0.8686 - dice_coef_loss: 0.5532\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5491 - accuracy: 0.8696 - dice_coef_loss: 0.5491\n",
      "Epoch 58: loss did not improve from 0.54396\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5491 - accuracy: 0.8696 - dice_coef_loss: 0.5491\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5493 - accuracy: 0.8617 - dice_coef_loss: 0.5493\n",
      "Epoch 59: loss did not improve from 0.54396\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5493 - accuracy: 0.8617 - dice_coef_loss: 0.5493\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5593 - accuracy: 0.8645 - dice_coef_loss: 0.5587\n",
      "Epoch 60: loss did not improve from 0.54396\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.5593 - accuracy: 0.8645 - dice_coef_loss: 0.5587\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5436 - accuracy: 0.8647 - dice_coef_loss: 0.5436\n",
      "Epoch 61: loss improved from 0.54396 to 0.54363, saving model to ./temp/polar_Dom/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5436 - accuracy: 0.8647 - dice_coef_loss: 0.5436\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5448 - accuracy: 0.8693 - dice_coef_loss: 0.5448\n",
      "Epoch 62: loss did not improve from 0.54363\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5448 - accuracy: 0.8693 - dice_coef_loss: 0.5448\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5474 - accuracy: 0.8679 - dice_coef_loss: 0.5474\n",
      "Epoch 63: loss did not improve from 0.54363\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5474 - accuracy: 0.8679 - dice_coef_loss: 0.5474\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5428 - accuracy: 0.8725 - dice_coef_loss: 0.5428\n",
      "Epoch 64: loss improved from 0.54363 to 0.54284, saving model to ./temp/polar_Dom/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.5428 - accuracy: 0.8725 - dice_coef_loss: 0.5428\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5642 - accuracy: 0.8698 - dice_coef_loss: 0.5642\n",
      "Epoch 65: loss did not improve from 0.54284\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5642 - accuracy: 0.8698 - dice_coef_loss: 0.5642\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5491 - accuracy: 0.8736 - dice_coef_loss: 0.5491\n",
      "Epoch 66: loss did not improve from 0.54284\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5491 - accuracy: 0.8736 - dice_coef_loss: 0.5491\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5412 - accuracy: 0.8685 - dice_coef_loss: 0.5413\n",
      "Epoch 67: loss improved from 0.54284 to 0.54124, saving model to ./temp/polar_Dom/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 0.5412 - accuracy: 0.8685 - dice_coef_loss: 0.5413\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5526 - accuracy: 0.8632 - dice_coef_loss: 0.5526\n",
      "Epoch 68: loss did not improve from 0.54124\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5526 - accuracy: 0.8632 - dice_coef_loss: 0.5526\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5387 - accuracy: 0.8732 - dice_coef_loss: 0.5387\n",
      "Epoch 69: loss improved from 0.54124 to 0.53872, saving model to ./temp/polar_Dom/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5387 - accuracy: 0.8732 - dice_coef_loss: 0.5387\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5574 - accuracy: 0.8671 - dice_coef_loss: 0.5574\n",
      "Epoch 70: loss did not improve from 0.53872\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5574 - accuracy: 0.8671 - dice_coef_loss: 0.5574\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5437 - accuracy: 0.8725 - dice_coef_loss: 0.5437\n",
      "Epoch 71: loss did not improve from 0.53872\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5437 - accuracy: 0.8725 - dice_coef_loss: 0.5437\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5517 - accuracy: 0.8723 - dice_coef_loss: 0.5517\n",
      "Epoch 72: loss did not improve from 0.53872\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5517 - accuracy: 0.8723 - dice_coef_loss: 0.5517\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5451 - accuracy: 0.8760 - dice_coef_loss: 0.5451\n",
      "Epoch 73: loss did not improve from 0.53872\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5451 - accuracy: 0.8760 - dice_coef_loss: 0.5451\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5519 - accuracy: 0.8732 - dice_coef_loss: 0.5519\n",
      "Epoch 74: loss did not improve from 0.53872\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5519 - accuracy: 0.8732 - dice_coef_loss: 0.5519\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5466 - accuracy: 0.8626 - dice_coef_loss: 0.5457\n",
      "Epoch 75: loss did not improve from 0.53872\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.5466 - accuracy: 0.8626 - dice_coef_loss: 0.5457\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5419 - accuracy: 0.8718 - dice_coef_loss: 0.5419\n",
      "Epoch 76: loss did not improve from 0.53872\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5419 - accuracy: 0.8718 - dice_coef_loss: 0.5419\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5505 - accuracy: 0.8752 - dice_coef_loss: 0.5505\n",
      "Epoch 77: loss did not improve from 0.53872\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5505 - accuracy: 0.8752 - dice_coef_loss: 0.5505\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5504 - accuracy: 0.8665 - dice_coef_loss: 0.5504\n",
      "Epoch 78: loss did not improve from 0.53872\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5504 - accuracy: 0.8665 - dice_coef_loss: 0.5504\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5538 - accuracy: 0.8771 - dice_coef_loss: 0.5538\n",
      "Epoch 79: loss did not improve from 0.53872\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5538 - accuracy: 0.8771 - dice_coef_loss: 0.5538\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5341 - accuracy: 0.8808 - dice_coef_loss: 0.5341\n",
      "Epoch 80: loss improved from 0.53872 to 0.53415, saving model to ./temp/polar_Dom/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5341 - accuracy: 0.8808 - dice_coef_loss: 0.5341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5408 - accuracy: 0.8785 - dice_coef_loss: 0.5408\n",
      "Epoch 81: loss did not improve from 0.53415\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5408 - accuracy: 0.8785 - dice_coef_loss: 0.5408\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5419 - accuracy: 0.8679 - dice_coef_loss: 0.5415\n",
      "Epoch 82: loss did not improve from 0.53415\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.5419 - accuracy: 0.8679 - dice_coef_loss: 0.5415\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5308 - accuracy: 0.8767 - dice_coef_loss: 0.5308\n",
      "Epoch 83: loss improved from 0.53415 to 0.53082, saving model to ./temp/polar_Dom/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5308 - accuracy: 0.8767 - dice_coef_loss: 0.5308\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5411 - accuracy: 0.8786 - dice_coef_loss: 0.5411\n",
      "Epoch 84: loss did not improve from 0.53082\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5411 - accuracy: 0.8786 - dice_coef_loss: 0.5411\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5453 - accuracy: 0.8752 - dice_coef_loss: 0.5453\n",
      "Epoch 85: loss did not improve from 0.53082\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5453 - accuracy: 0.8752 - dice_coef_loss: 0.5453\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5373 - accuracy: 0.8786 - dice_coef_loss: 0.5373\n",
      "Epoch 86: loss did not improve from 0.53082\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5373 - accuracy: 0.8786 - dice_coef_loss: 0.5373\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5462 - accuracy: 0.8729 - dice_coef_loss: 0.5462\n",
      "Epoch 87: loss did not improve from 0.53082\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5462 - accuracy: 0.8729 - dice_coef_loss: 0.5462\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5451 - accuracy: 0.8811 - dice_coef_loss: 0.5451\n",
      "Epoch 88: loss did not improve from 0.53082\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5451 - accuracy: 0.8811 - dice_coef_loss: 0.5451\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5600 - accuracy: 0.8775 - dice_coef_loss: 0.5594\n",
      "Epoch 89: loss did not improve from 0.53082\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.5600 - accuracy: 0.8775 - dice_coef_loss: 0.5594\n",
      "Epoch 90/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5418 - accuracy: 0.8736 - dice_coef_loss: 0.5418\n",
      "Epoch 90: loss did not improve from 0.53082\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5418 - accuracy: 0.8736 - dice_coef_loss: 0.5418\n",
      "Epoch 91/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5549 - accuracy: 0.8795 - dice_coef_loss: 0.5549\n",
      "Epoch 91: loss did not improve from 0.53082\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5549 - accuracy: 0.8795 - dice_coef_loss: 0.5549\n",
      "Epoch 92/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5329 - accuracy: 0.8779 - dice_coef_loss: 0.5329\n",
      "Epoch 92: loss did not improve from 0.53082\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5329 - accuracy: 0.8779 - dice_coef_loss: 0.5329\n",
      "Epoch 93/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5368 - accuracy: 0.8849 - dice_coef_loss: 0.5368\n",
      "Epoch 93: loss did not improve from 0.53082\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5368 - accuracy: 0.8849 - dice_coef_loss: 0.5368\n",
      "Epoch 94/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5289 - accuracy: 0.8801 - dice_coef_loss: 0.5289\n",
      "Epoch 94: loss improved from 0.53082 to 0.52887, saving model to ./temp/polar_Dom/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.5289 - accuracy: 0.8801 - dice_coef_loss: 0.5289\n",
      "Epoch 95/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5608 - accuracy: 0.8801 - dice_coef_loss: 0.5608\n",
      "Epoch 95: loss did not improve from 0.52887\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5608 - accuracy: 0.8801 - dice_coef_loss: 0.5608\n",
      "Epoch 96/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5402 - accuracy: 0.8795 - dice_coef_loss: 0.5402\n",
      "Epoch 96: loss did not improve from 0.52887\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5402 - accuracy: 0.8795 - dice_coef_loss: 0.5402\n",
      "Epoch 97/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5377 - accuracy: 0.8805 - dice_coef_loss: 0.5400\n",
      "Epoch 97: loss did not improve from 0.52887\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.5377 - accuracy: 0.8805 - dice_coef_loss: 0.5400\n",
      "Epoch 98/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5429 - accuracy: 0.8847 - dice_coef_loss: 0.5429\n",
      "Epoch 98: loss did not improve from 0.52887\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5429 - accuracy: 0.8847 - dice_coef_loss: 0.5429\n",
      "Epoch 99/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5417 - accuracy: 0.8798 - dice_coef_loss: 0.5417\n",
      "Epoch 99: loss did not improve from 0.52887\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5417 - accuracy: 0.8798 - dice_coef_loss: 0.5417\n",
      "Epoch 100/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5335 - accuracy: 0.8829 - dice_coef_loss: 0.5335\n",
      "Epoch 100: loss did not improve from 0.52887\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5335 - accuracy: 0.8829 - dice_coef_loss: 0.5335\n",
      "Found 2961 images belonging to 1 classes.\n",
      "Found 2961 images belonging to 1 classes.\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f837f0ceef0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f837f0ceef0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6312 - accuracy: 0.5240 - dice_coef_loss: 0.6312\n",
      "Epoch 1: loss improved from inf to 0.63121, saving model to ./temp/polar_Dom/checkpoint.hdf5\n",
      "100/100 [==============================] - 28s 267ms/step - loss: 0.6312 - accuracy: 0.5240 - dice_coef_loss: 0.6312\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6102 - accuracy: 0.7055 - dice_coef_loss: 0.6102\n",
      "Epoch 2: loss improved from 0.63121 to 0.61017, saving model to ./temp/polar_Dom/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.6102 - accuracy: 0.7055 - dice_coef_loss: 0.6102\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5999 - accuracy: 0.7581 - dice_coef_loss: 0.5999\n",
      "Epoch 3: loss improved from 0.61017 to 0.59995, saving model to ./temp/polar_Dom/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5999 - accuracy: 0.7581 - dice_coef_loss: 0.5999\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5827 - accuracy: 0.7661 - dice_coef_loss: 0.5827\n",
      "Epoch 4: loss improved from 0.59995 to 0.58266, saving model to ./temp/polar_Dom/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5827 - accuracy: 0.7661 - dice_coef_loss: 0.5827\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - ETA: 0s - loss: 0.5803 - accuracy: 0.7855 - dice_coef_loss: 0.5803\n",
      "Epoch 5: loss improved from 0.58266 to 0.58026, saving model to ./temp/polar_Dom/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5803 - accuracy: 0.7855 - dice_coef_loss: 0.5803\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5822 - accuracy: 0.7864 - dice_coef_loss: 0.5822\n",
      "Epoch 6: loss did not improve from 0.58026\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5822 - accuracy: 0.7864 - dice_coef_loss: 0.5822\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5764 - accuracy: 0.8063 - dice_coef_loss: 0.5764\n",
      "Epoch 7: loss improved from 0.58026 to 0.57639, saving model to ./temp/polar_Dom/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5764 - accuracy: 0.8063 - dice_coef_loss: 0.5764\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5798 - accuracy: 0.8024 - dice_coef_loss: 0.5787\n",
      "Epoch 8: loss did not improve from 0.57639\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.5798 - accuracy: 0.8024 - dice_coef_loss: 0.5787\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5729 - accuracy: 0.8092 - dice_coef_loss: 0.5729\n",
      "Epoch 9: loss improved from 0.57639 to 0.57288, saving model to ./temp/polar_Dom/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 269ms/step - loss: 0.5729 - accuracy: 0.8092 - dice_coef_loss: 0.5729\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5709 - accuracy: 0.8097 - dice_coef_loss: 0.5709\n",
      "Epoch 10: loss improved from 0.57288 to 0.57085, saving model to ./temp/polar_Dom/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5709 - accuracy: 0.8097 - dice_coef_loss: 0.5709\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5780 - accuracy: 0.8192 - dice_coef_loss: 0.5780\n",
      "Epoch 11: loss did not improve from 0.57085\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5780 - accuracy: 0.8192 - dice_coef_loss: 0.5780\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5695 - accuracy: 0.8230 - dice_coef_loss: 0.5695\n",
      "Epoch 12: loss improved from 0.57085 to 0.56948, saving model to ./temp/polar_Dom/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.5695 - accuracy: 0.8230 - dice_coef_loss: 0.5695\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5762 - accuracy: 0.8166 - dice_coef_loss: 0.5762\n",
      "Epoch 13: loss did not improve from 0.56948\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5762 - accuracy: 0.8166 - dice_coef_loss: 0.5762\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5589 - accuracy: 0.8273 - dice_coef_loss: 0.5589\n",
      "Epoch 14: loss improved from 0.56948 to 0.55889, saving model to ./temp/polar_Dom/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5589 - accuracy: 0.8273 - dice_coef_loss: 0.5589\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5767 - accuracy: 0.8198 - dice_coef_loss: 0.5751\n",
      "Epoch 15: loss did not improve from 0.55889\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.5767 - accuracy: 0.8198 - dice_coef_loss: 0.5751\n",
      "Epoch 16/100\n",
      "  6/100 [>.............................] - ETA: 24s - loss: 0.5912 - accuracy: 0.8500 - dice_coef_loss: 0.5912"
     ]
    }
   ],
   "source": [
    "working_mother_folder = PARAM_PATH_TEMP_POLAR\n",
    "batch_size = 4\n",
    "PARAM_BETA_TEST_NUM = 6\n",
    "data_gen_args = dict(rotation_range = 80,      # TODO: improve the data augmentation\n",
    "                width_shift_range =0.02,\n",
    "                height_shift_range =0.02,\n",
    "                shear_range = 0.35,\n",
    "                zoom_range = 0.075,\n",
    "                horizontal_flip = True,\n",
    "                fill_mode = 'nearest',\n",
    "                rescale = 1./255)\n",
    "score = []\n",
    "for i in range(K):\n",
    "    working_test_folder_i = os.path.join(working_mother_folder, str(i), PARAM_SUB_FOLDER_POLAR)\n",
    "    temp_folder_path = os.path.join(working_mother_folder,'temp')\n",
    "    os.mkdir(temp_folder_path)\n",
    "    for j in range(K):\n",
    "        if i != j:\n",
    "            for subfolder_name in ['image','label']:\n",
    "                subfolder_path = os.path.join(working_mother_folder,str(j),'polar',subfolder_name)\n",
    "                temp_subfolder_path = os.path.join(temp_folder_path,subfolder_name)\n",
    "                for root, dirs, files in os.walk(subfolder_path):\n",
    "                    for file in files:\n",
    "                        src_file = os.path.join(root, file)\n",
    "                        dest_file = os.path.join(temp_subfolder_path,os.path.relpath(src_file, subfolder_path))\n",
    "                        os.makedirs(os.path.dirname(dest_file), exist_ok=True)\n",
    "                        shutil.copy(src_file, dest_file)\n",
    "    test_gene = trainGenerator(batch_size, temp_folder_path, PARAM_IMG_FOLDER, PARAM_MSK_FOLDER, data_gen_args)\n",
    "    model = unet(PARAM_BETA1[PARAM_BETA_TEST_NUM], PARAM_BETA2[PARAM_BETA_TEST_NUM]) \n",
    "    model_checkpoint = ModelCheckpoint(os.path.join(working_mother_folder,'checkpoint.hdf5'), monitor = 'loss', verbose=1, save_best_only=True)\n",
    "    test_run = model.fit(test_gene, verbose = 1, steps_per_epoch = 100, epochs = 100, callbacks = [model_checkpoint])\n",
    "    score.append(test_run)\n",
    "    shutil.rmtree(temp_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuiling's K-fold function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K fold Validation (obtain training & testing sets)\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold = KFold(n_splits=PARAM_SPLIT_NUM)\n",
    "for train_index,test_index in kfold.split(dfPolar):\n",
    "    polar_train,polar_test=dfPolar.iloc[train_index, :],dfPolar.iloc[test_index, :]\n",
    "    cartesian_train,cartesian_test=dfCartesian.iloc[train_index, :],dfCartesian.iloc[test_index, :]\n",
    "    #print(\"polar train: \", polar_train, \"polar test: \", polar_test)\n",
    "    #print(\"cartesian train\" , cartesian_train, \"cartesian test\", cartesian_test)\n",
    "    batch_size = 3\n",
    "    PARAM_BETA_TEST_NUM = 6\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "    data_gen_args = dict(rotation_range = 80,      # TODO: improve the data augmentation\n",
    "                width_shift_range =0.02,\n",
    "                height_shift_range =0.02,\n",
    "                shear_range = 0.35,\n",
    "                zoom_range = 0.075,\n",
    "                horizontal_flip = True,\n",
    "                fill_mode = 'nearest',\n",
    "                rescale = 1./255)\n",
    "    print('-------------------------------------------------------------')\n",
    "    test_model = unet(PARAM_BETA1[PARAM_BETA_TEST_NUM], PARAM_BETA2[PARAM_BETA_TEST_NUM]) \n",
    "    test_run = test_model.fit(polar_train, polar_test, verbose = 1, steps_per_epoch = 50, epochs = 5, callbacks = [early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training test\n",
    "\n",
    "This part is used to see if we can train a model using the current configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Superparameters (temporary) for a test run of model training test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "PARAM_BETA_TEST_NUM = 6\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "data_gen_args = dict(rotation_range = 80,      # TODO: improve the data augmentation\n",
    "                width_shift_range =0.02,\n",
    "                height_shift_range =0.02,\n",
    "                shear_range = 0.35,\n",
    "                zoom_range = 0.075,\n",
    "                horizontal_flip = True,\n",
    "                fill_mode = 'nearest',\n",
    "                rescale = 1./255)\n",
    "test_gene = trainGenerator(batch_size, PARAM_PATH_CARTE, PARAM_IMG_FOLDER, PARAM_MSK_FOLDER, data_gen_args)\n",
    "test_model = unet(PARAM_BETA1[PARAM_BETA_TEST_NUM], PARAM_BETA2[PARAM_BETA_TEST_NUM]) \n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output\n",
    "<details>\n",
    "    <summary><b><font color=\"green\">Click here to expand</font></b></summary>\n",
    "    <code>\n",
    "Model: \"model_5\"\n",
    "__________________________________________________________________________________________________\n",
    " Layer (type)                   Output Shape         Param #     Connected to                     \n",
    "==================================================================================================\n",
    " input_6 (InputLayer)           [(None, 256, 256, 3  0           []                               \n",
    "                                )]                                                                \n",
    "                                                                                                  \n",
    " conv2d_120 (Conv2D)            (None, 256, 256, 64  1792        ['input_6[0][0]']                \n",
    "                                )                                                                 \n",
    "                                                                                                  \n",
    " conv2d_121 (Conv2D)            (None, 256, 256, 64  36928       ['conv2d_120[0][0]']             \n",
    "                                )                                                                 \n",
    "                                                                                                  \n",
    " max_pooling2d_20 (MaxPooling2D  (None, 128, 128, 64  0          ['conv2d_121[0][0]']             \n",
    " )                              )                                                                 \n",
    "                                                                                                  \n",
    " conv2d_122 (Conv2D)            (None, 128, 128, 12  73856       ['max_pooling2d_20[0][0]']       \n",
    "                                8)                                                                \n",
    "                                                                                                  \n",
    " conv2d_123 (Conv2D)            (None, 128, 128, 12  147584      ['conv2d_122[0][0]']             \n",
    "                                8)                                                                \n",
    "                                                                                                  \n",
    " max_pooling2d_21 (MaxPooling2D  (None, 64, 64, 128)  0          ['conv2d_123[0][0]']             \n",
    " )                                                                                                \n",
    "                                                                                                  \n",
    " conv2d_124 (Conv2D)            (None, 64, 64, 256)  295168      ['max_pooling2d_21[0][0]']       \n",
    "                                                                                                  \n",
    " conv2d_125 (Conv2D)            (None, 64, 64, 256)  590080      ['conv2d_124[0][0]']             \n",
    "                                                                                                  \n",
    " max_pooling2d_22 (MaxPooling2D  (None, 32, 32, 256)  0          ['conv2d_125[0][0]']             \n",
    " )                                                                                                \n",
    "                                                                                                  \n",
    " conv2d_126 (Conv2D)            (None, 32, 32, 512)  1180160     ['max_pooling2d_22[0][0]']       \n",
    "                                                                                                  \n",
    " conv2d_127 (Conv2D)            (None, 32, 32, 512)  2359808     ['conv2d_126[0][0]']             \n",
    "                                                                                                  \n",
    " dropout_10 (Dropout)           (None, 32, 32, 512)  0           ['conv2d_127[0][0]']             \n",
    "                                                                                                  \n",
    " max_pooling2d_23 (MaxPooling2D  (None, 16, 16, 512)  0          ['dropout_10[0][0]']             \n",
    " )                                                                                                \n",
    "                                                                                                  \n",
    " conv2d_128 (Conv2D)            (None, 16, 16, 1024  4719616     ['max_pooling2d_23[0][0]']       \n",
    "                                )                                                                 \n",
    "                                                                                                  \n",
    " conv2d_129 (Conv2D)            (None, 16, 16, 1024  9438208     ['conv2d_128[0][0]']             \n",
    "                                )                                                                 \n",
    "                                                                                                  \n",
    " dropout_11 (Dropout)           (None, 16, 16, 1024  0           ['conv2d_129[0][0]']             \n",
    "                                )                                                                 \n",
    "                                                                                                  \n",
    " up_sampling2d_20 (UpSampling2D  (None, 32, 32, 1024  0          ['dropout_11[0][0]']             \n",
    " )                              )                                                                 \n",
    "                                                                                                  \n",
    " conv2d_130 (Conv2D)            (None, 32, 32, 512)  2097664     ['up_sampling2d_20[0][0]']       \n",
    "                                                                                                  \n",
    " concatenate_20 (Concatenate)   (None, 32, 32, 1024  0           ['dropout_10[0][0]',             \n",
    "                                )                                 'conv2d_130[0][0]']             \n",
    "                                                                                                  \n",
    " conv2d_131 (Conv2D)            (None, 32, 32, 512)  4719104     ['concatenate_20[0][0]']         \n",
    "                                                                                                  \n",
    " conv2d_132 (Conv2D)            (None, 32, 32, 512)  2359808     ['conv2d_131[0][0]']             \n",
    "                                                                                                  \n",
    " up_sampling2d_21 (UpSampling2D  (None, 64, 64, 512)  0          ['conv2d_132[0][0]']             \n",
    " )                                                                                                \n",
    "                                                                                                  \n",
    " conv2d_133 (Conv2D)            (None, 64, 64, 256)  524544      ['up_sampling2d_21[0][0]']       \n",
    "                                                                                                  \n",
    " concatenate_21 (Concatenate)   (None, 64, 64, 512)  0           ['conv2d_125[0][0]',             \n",
    "                                                                  'conv2d_133[0][0]']             \n",
    "                                                                                                  \n",
    " conv2d_134 (Conv2D)            (None, 64, 64, 256)  1179904     ['concatenate_21[0][0]']         \n",
    "                                                                                                  \n",
    " conv2d_135 (Conv2D)            (None, 64, 64, 256)  590080      ['conv2d_134[0][0]']             \n",
    "                                                                                                  \n",
    " up_sampling2d_22 (UpSampling2D  (None, 128, 128, 25  0          ['conv2d_135[0][0]']             \n",
    " )                              6)                                                                \n",
    "                                                                                                  \n",
    " conv2d_136 (Conv2D)            (None, 128, 128, 12  131200      ['up_sampling2d_22[0][0]']       \n",
    "                                8)                                                                \n",
    "                                                                                                  \n",
    " concatenate_22 (Concatenate)   (None, 128, 128, 25  0           ['conv2d_123[0][0]',             \n",
    "                                6)                                'conv2d_136[0][0]']             \n",
    "                                                                                                  \n",
    " conv2d_137 (Conv2D)            (None, 128, 128, 12  295040      ['concatenate_22[0][0]']         \n",
    "                                8)                                                                \n",
    "                                                                                                  \n",
    " conv2d_138 (Conv2D)            (None, 128, 128, 12  147584      ['conv2d_137[0][0]']             \n",
    "                                8)                                                                \n",
    "                                                                                                  \n",
    " up_sampling2d_23 (UpSampling2D  (None, 256, 256, 12  0          ['conv2d_138[0][0]']             \n",
    " )                              8)                                                                \n",
    "                                                                                                  \n",
    " conv2d_139 (Conv2D)            (None, 256, 256, 64  32832       ['up_sampling2d_23[0][0]']       \n",
    "                                )                                                                 \n",
    "                                                                                                  \n",
    " concatenate_23 (Concatenate)   (None, 256, 256, 12  0           ['conv2d_121[0][0]',             \n",
    "                                8)                                'conv2d_139[0][0]']             \n",
    "                                                                                                  \n",
    " conv2d_140 (Conv2D)            (None, 256, 256, 64  73792       ['concatenate_23[0][0]']         \n",
    "                                )                                                                 \n",
    "                                                                                                  \n",
    " conv2d_141 (Conv2D)            (None, 256, 256, 64  36928       ['conv2d_140[0][0]']             \n",
    "                                )                                                                 \n",
    "                                                                                                  \n",
    " conv2d_142 (Conv2D)            (None, 256, 256, 9)  5193        ['conv2d_141[0][0]']             \n",
    "                                                                                                  \n",
    " conv2d_143 (Conv2D)            (None, 256, 256, 3)  30          ['conv2d_142[0][0]']             \n",
    "                                                                                                  \n",
    "==================================================================================================\n",
    "Total params: 31,036,903\n",
    "Trainable params: 31,036,903\n",
    "Non-trainable params: 0\n",
    "</code>\n",
    "</details>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = ModelCheckpoint('unet_endoscopic.hdf5', monitor = 'loss', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item1, item2 in test_gene:\n",
    "    print(item1.shape)\n",
    "    print(item2.shape)\n",
    "    print('----------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_run = test_model.fit(test_gene, verbose = 1, steps_per_epoch = 100, epochs = 100, callbacks = [model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(test_run.history.keys())\n",
    "plt.plot(test_run.history['loss'])\n",
    "plt.plot(test_run.history['accuracy'])\n",
    "plt.title('Test Run')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['loss', 'accuracy'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAM_PATH_TEST = './test'\n",
    "image_name = '14.tif'\n",
    "img = io.imread(os.path.join(PARAM_PATH_TEST,image_name),as_gray = False)\n",
    "img = trans.resize(img,[256,256])\n",
    "img = np.reshape(img,(1,)+img.shape)\n",
    "\n",
    "results = test_model.predict(img,1,verbose=1)\n",
    "#saveResult(Path,results)\n",
    "img = results[0,:,:]\n",
    "print(results.shape)\n",
    "io.imsave(os.path.join(PARAM_PATH_TEST,\"result.png\"),img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legacy Code Below\n",
    "### written by Wenfan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from defines import *\n",
    "from model import *\n",
    "from data import *\n",
    "import sys\n",
    "\n",
    "from numpy import loadtxt\n",
    "from keras.models import load_model\n",
    "from PIL import Image\n",
    "\n",
    "import glob\n",
    "import os \n",
    "import math\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data_jupyter'# main folder path\n",
    "\n",
    "D = 'all_images'\n",
    "L = 'all_labels'\n",
    "\n",
    "D_P_im = 'polar_im'\n",
    "D_C_im = 'car_im'\n",
    "D_P = 'polar_l'\n",
    "D_C = 'car_l'\n",
    "\n",
    "prev_number_of_D_P = -1\n",
    "prev_number_of_D_C = -1\n",
    "\n",
    "diff = math.inf\n",
    "\n",
    "model_P = unet(PARAM_BETA1[PARAM_BETA_TEST_NUM], PARAM_BETA2[PARAM_BETA_TEST_NUM])\n",
    "model_C = unet(PARAM_BETA1[PARAM_BETA_TEST_NUM], PARAM_BETA2[PARAM_BETA_TEST_NUM])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill in the weights here\n",
    "model_P.load_weights() \n",
    "model_C.load_weights() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "#change file name of D, L\n",
    "for count, filename in enumerate(os.listdir(path+'/'+D)): \n",
    "    dst = str(count) + \".png\"\n",
    "    src = path+'/'+D+'/'+filename\n",
    "    dst = path+'/'+D+'/'+dst\n",
    "\n",
    "    os.rename(src, dst) \n",
    "    \n",
    "for count, filename in enumerate(os.listdir(path+'/'+L)): \n",
    "    dst = str(count) + \".tif\"\n",
    "    src = path+'/'+L+'/'+filename\n",
    "    dst = path+'/'+L+'/'+dst\n",
    "\n",
    "    os.rename(src, dst) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dscore(im, im_name):\n",
    "    label = Image.open(os.path.join(L, im_name))\n",
    "    \n",
    "    pixelIm = im.load()\n",
    "    pixelLabel = label.load()\n",
    "    \n",
    "    upper = 0\n",
    "    lower = im.size[0] * im.size[1]\n",
    "    \n",
    "    for i in range(im.size[0]):\n",
    "        for j in range(im.size[1]):\n",
    "            if pixelIm[i,j] == pixelLabel[i,j]:\n",
    "                upper = upper + 1\n",
    "            \n",
    "    upper = upper * 2\n",
    "    \n",
    "    return upper / lower\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while (diff > 0): \n",
    "    #load model\n",
    "    #if count > 0:\n",
    "    #    model_P.load_weights('model_P_weights_' + (count - 1))\n",
    "    #    model_C.load_weights('model_C_weights_' + (count - 1))\n",
    "    \n",
    "    count = count + 1\n",
    "    \n",
    "    testGene_P, testGene_C = testGenerator(path,D,L)\n",
    "    \n",
    "    #perdict\n",
    "    results_P = model_P.predict(testGene_P, PARAM_N_TESTS, verbose=1)\n",
    "    results_C = model_C.predict(testGene_C, PARAM_N_TESTS, verbose=1)\n",
    "\n",
    "    np.save(PARAM_PATH_TEST_NPY_P, results_P)\n",
    "    saveResult(PARAM_PATH_TEST_RESULTS_P, results_P)\n",
    "    np.save(PARAM_PATH_TEST_NPY_C, results_C)\n",
    "    saveResult(PARAM_PATH_TEST_RESULTS_C, results_C)\n",
    "    \n",
    "    mergeIm(path, D, L, PARAM_PATH_TEST_RESULTS_P, path+'/'+D_P)\n",
    "    mergeIm(path, D, L, PARAM_PATH_TEST_RESULTS_C, path+'/'+D_C)\n",
    "    \n",
    "    #change file name of D_P, D_C\n",
    "    for count, filename in enumerate(os.listdir(path+'/'+D_P)): \n",
    "        dst = str(count) + \".tif\"\n",
    "        src = path+'/'+D_P+'/'+filenaLme\n",
    "        dst = path+'/'+D_P+'/'+dst\n",
    "\n",
    "        os.rename(src, dst) \n",
    "  \n",
    "    for count, filename in enumerate(os.listdir(path+'/'+D_C)): \n",
    "        dst = str(count) + \".tif\"\n",
    "        src = path+'/'+D_C+'/'+filename\n",
    "        dst = path+'/'+D_C+'/'+dst\n",
    "\n",
    "        os.rename(src, dst) \n",
    "    \n",
    "    #find the better one (based on L) and modify D_P, D_P_im, D_C, D_C_im\n",
    "    for file in os.listdir(D):\n",
    "        im_P = Image.open(path+'/'+D_P+'/'+file)\n",
    "        im_C = Image.open(path+'/'+D_C+'/'+file)\n",
    "        \n",
    "        if dscore(im_P, file) > dscore(im_C, file):\n",
    "            shutil.copyfile(path+'/'+D+'/'+file, path+'/'+D_P_im+'/'+file)\n",
    "            os.remove(path+'/'+D_C+'/'+file)\n",
    "        else:\n",
    "            shutil.copyfile(path+'/'+D+'/'+file, path+'/'+D_C_im+'/'+file)\n",
    "            os.remove(path+'/'+D_P+'/'+file)\n",
    "            \n",
    "    number_of_D_P = len(glob.glob(D_P))\n",
    "    number_of_D_C = len(glob.glob(D_C))\n",
    "\n",
    "    # file numbers difference\n",
    "    diff = Math.abs(prev_number_of_D_P - number_of_D_P + prev_number_of_D_C - number_of_D_C) / 2\n",
    "    \n",
    "    prev_number_of_D_P = number_of_D_P\n",
    "    prev_number_of_D_C = number_of_D_P\n",
    "    \n",
    "    #train model_P only on D_P, model_C only on D_C\n",
    "    myGene_P = trainGenerator(PARAM_BATCHES, \n",
    "                            path, \n",
    "                            D_P, \n",
    "                            D_P_im, \n",
    "                            PARAM_DATA_ARGS, \n",
    "                            save_to_dir = PARAM_AUG_FOLDER_P)\n",
    "    \n",
    "    myGene_C = trainGenerator(PARAM_BATCHES, \n",
    "                            path, \n",
    "                            D_C, \n",
    "                            D_C_im, \n",
    "                            PARAM_DATA_ARGS, \n",
    "                            save_to_dir = PARAM_AUG_FOLDER_C)\n",
    "    \n",
    "    model_checkpoint_P = ModelCheckpoint( PARAM_SAVED_MODEL, \n",
    "                                         monitor = PARAM_METRICS, \n",
    "                                         verbose = 1, \n",
    "                                         save_best_only = PARAM_SAVE_BEST_ONLY)\n",
    "    \n",
    "    model_P.fit_generator(myGene_P,\n",
    "                        steps_per_epoch = PARAM_EPOCH_STEPS,\n",
    "                        epochs = PARAM_N_EPOCHS,\n",
    "                        callbacks = [model_checkpoint])\n",
    "    model_C.fit_generator(myGene_C,\n",
    "                        steps_per_epoch = PARAM_EPOCH_STEPS,\n",
    "                        epochs = PARAM_N_EPOCHS,\n",
    "                        callbacks = [model_checkpoint])\n",
    "    \n",
    "    model_P.save_weights('model_P_weights_' + count)\n",
    "    model_C.save_weights('model_C_weights_' + count)\n",
    "    \n",
    "    shutil.rmtree(path+'/'+D_P)\n",
    "    shutil.rmtree(path+'/'+D_C)\n",
    "    shutil.rmtree(path+'/'+D_P_im)\n",
    "    shutil.rmtree(path+'/'+D_C_im)\n",
    "    \n",
    "\n",
    "save_model(model_P, 'model_P.h5')\n",
    "save_model(model_C, 'model_C.h5')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
