{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Approach for Unet Training \n",
    "\n",
    "------\n",
    "\n",
    "## Overview\n",
    "\n",
    "\n",
    "\n",
    "The purpose of this learn.ipynb notebook is to investigate whether an image can exhibit a preference for being segmented more effectively using a UNet model trained on polar or cartesian-dominant images.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Structure\n",
    "```\n",
    "data\n",
    "└── endoscopic\n",
    "    ├── cartesian\n",
    "    │   ├── image\n",
    "    │   └── label\n",
    "    └── polar\n",
    "        ├── image\n",
    "        └── label\n",
    "```\n",
    "\n",
    "Inside of each end folder there are 956 images, named as `0.tif` to `955.tif`\n",
    "and I believe, for now, the naming of the images are one to one correctly matched, meaning the ``/data/endoscopic/**cartesian**/image/0.tif`` is transformed from `/data/endoscopic/**polar**/image/0.tif`\n",
    "\n",
    "Instead of putting a seperate set of images aside to be test set, we chose to use k-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from defines import *\n",
    "from model import *\n",
    "from data import *\n",
    "import sys\n",
    "\n",
    "import cv2\n",
    "from numpy import loadtxt\n",
    "from keras.models import load_model\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from skimage.io import imread\n",
    "from skimage import img_as_ubyte\n",
    "from skimage.transform import resize\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import math\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from files and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this if your computer has a cuda visible device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PARAM_SYSTEM_TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test code for correct file structure setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize folder tree in current directory\n",
    "os.system(\"tree -d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count of files in data directories\n",
    "os.system(\"ls \" + os.path.join(PARAM_PATH_CARTE,PARAM_IMG_FOLDER) + \" | wc -l\")\n",
    "os.system(\"ls \" + os.path.join(PARAM_PATH_CARTE,PARAM_MSK_FOLDER) + \" | wc -l\")\n",
    "os.system(\"ls \" + os.path.join(PARAM_PATH_POLAR,PARAM_IMG_FOLDER) + \" | wc -l\")\n",
    "os.system(\"ls \" + os.path.join(PARAM_PATH_POLAR,PARAM_MSK_FOLDER) + \" | wc -l\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Expected Output:\n",
    "7404\\\n",
    "7404\\\n",
    "7404\\\n",
    "7404\\\n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Relocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following block, the code loads in one analysis file from previous research. \n",
    "\n",
    "### #File name postfix\n",
    "_C_ is the dice scores of the predictions generated by Unet C: this Unet C is trained using all 7404 images, in their cartesian form. The raw image was directly input into the Unet and the prediction was generated.\n",
    "\n",
    "_P_ is the dice scores of the predictions generated by Unet P: this Unet P is trained using all 7404 images but in their polar form. The raw images were transformed, and then input for prediction. The prediction is in polar space.\n",
    "\n",
    "_P2C_ is the dice scores of the predictions generated by the same Unet P as mentioned above, but the dice score is generated by transforming the prediction back to cartesian, and compared to their original label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name = 'analysis_dice_back_Test_C.npy'\n",
    "#file_name = 'analysis_dice_back_Test_P.npy'\n",
    "#file_name = 'analysis_dice_back_Test_P2C.npy'\n",
    "file_name = 'analysis_dice_back_Train_P.npy'\n",
    "\n",
    "np_file = os.path.join(PARAM_PATH_SCORES, file_name)\n",
    "#load npy file\n",
    "img_score = np.load(np_file)\n",
    "\n",
    "#sort scores in descending order and store index\n",
    "sorted_score = np.flip(np.argsort(img_score))\n",
    "\n",
    "#------DEBUG--------\n",
    "#print(len(sorted_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x7fcdd0ff1690>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#This sort the image score and visulize it\n",
    "x_range = 7404\n",
    "img_score_visual = np.sort(img_score)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(img_score_visual[0:x_range])\n",
    "ax.axvline(x=(x_range/2), color='r', linestyle='--', label='Horizontal Line at y=5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n",
      "!\n",
      "!\n",
      "!\n",
      "!\n",
      "!\n",
      "!\n",
      "!\n",
      "!\n",
      "!\n",
      "!\n",
      "!\n",
      "!\n",
      "!\n",
      "!\n",
      "!\n",
      "!\n",
      "!\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for item in img_score:\n",
    "    if item == 1:\n",
    "        print('!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sorted_score` should be a list of length 7404, sorted by the method we chose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_score = pd.DataFrame(sorted_score)\n",
    "\n",
    "#fetch top polar dominant and non-polar dominant image\n",
    "num_polar = round(len(sorted_score)/2)\n",
    "num_cartesian = len(sorted_score) - num_polar\n",
    "dfPolar = sorted_score.head(num_polar)\n",
    "dfCartesian = sorted_score.tail(num_cartesian)\n",
    "#print(\"Polar: \\n\", dfPolar)\n",
    "#print(\"Cartesian: \\n\", dfCartesian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in `dfPolar` should be the best half of images (filename), which performs better than the other half, according to the data we used above. In `dfCartesian` there's the other half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from filePrep import *\n",
    "\n",
    "K = 5\n",
    "\n",
    "checkNcreateTempFolder(PARAM_PATH_TEMP_POLAR, K)\n",
    "checkNcreateTempFolder(PARAM_PATH_TEMP_CARTE, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two lines creates the new temporary folder.\n",
    "Instead of making kfolds later, the kfolds is assigned now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits = K, shuffle = True, random_state = 42) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the KFold package to assign the paramters like n_splits and so.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for train_index,test_index in kf.split(dfPolar):\n",
    "    fillFolder(test_index, dfPolar, PARAM_PATH_POLAR, PARAM_PATH_CARTE, PARAM_PATH_TEMP_POLAR, i)\n",
    "    i += 1\n",
    "i = 0\n",
    "print('------------------------------------')\n",
    "for train_index,test_index in kf.split(dfCartesian):\n",
    "    fillFolder(test_index, dfCartesian, PARAM_PATH_POLAR, PARAM_PATH_CARTE, PARAM_PATH_TEMP_CARTE, i)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we should have all `k-folds` set up. The next step is to write a training loop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the limitation of flow_from_directory, which, it does not allow the combination of multiple directories. And, the limitation of my knowledge of flow_from_dataframe. **I believe this is solvable using flow_from_dataframe** I decide to move training set into a separate temporary folder when training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-27 20:14:45.942045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-27 20:14:45.964479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-27 20:14:45.964614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-27 20:14:45.965174: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-27 20:14:45.965632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-27 20:14:45.965737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-27 20:14:45.965825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-27 20:14:46.273388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-27 20:14:46.273536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-27 20:14:46.273628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-27 20:14:46.273699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6832 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2961 images belonging to 1 classes.\n",
      "Found 2961 images belonging to 1 classes.\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcdd0ec3830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcdd0ec3830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function dice_coef_loss at 0x7fce801657a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function dice_coef_loss at 0x7fce801657a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-27 20:14:49.035829: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8401\n",
      "2023-09-27 20:14:49.284688: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - ETA: 0s - loss: 0.7795 - accuracy: 0.8602 - dice_coef_loss: 0.7795\n",
      "Epoch 1: loss improved from inf to 0.77948, saving model to ./temp/polar_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 31s 262ms/step - loss: 0.7795 - accuracy: 0.8602 - dice_coef_loss: 0.7795\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6982 - accuracy: 0.8527 - dice_coef_loss: 0.6982\n",
      "Epoch 2: loss improved from 0.77948 to 0.69818, saving model to ./temp/polar_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.6982 - accuracy: 0.8527 - dice_coef_loss: 0.6982\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6716 - accuracy: 0.8607 - dice_coef_loss: 0.6716\n",
      "Epoch 3: loss improved from 0.69818 to 0.67161, saving model to ./temp/polar_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.6716 - accuracy: 0.8607 - dice_coef_loss: 0.6716\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6574 - accuracy: 0.8548 - dice_coef_loss: 0.6574\n",
      "Epoch 4: loss improved from 0.67161 to 0.65736, saving model to ./temp/polar_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 269ms/step - loss: 0.6574 - accuracy: 0.8548 - dice_coef_loss: 0.6574\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6591 - accuracy: 0.8626 - dice_coef_loss: 0.6591\n",
      "Epoch 5: loss did not improve from 0.65736\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.6591 - accuracy: 0.8626 - dice_coef_loss: 0.6591\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6534 - accuracy: 0.8749 - dice_coef_loss: 0.6534\n",
      "Epoch 6: loss improved from 0.65736 to 0.65340, saving model to ./temp/polar_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.6534 - accuracy: 0.8749 - dice_coef_loss: 0.6534\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6455 - accuracy: 0.8811 - dice_coef_loss: 0.6455\n",
      "Epoch 7: loss improved from 0.65340 to 0.64553, saving model to ./temp/polar_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.6455 - accuracy: 0.8811 - dice_coef_loss: 0.6455\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6539 - accuracy: 0.8648 - dice_coef_loss: 0.6521\n",
      "Epoch 8: loss did not improve from 0.64553\n",
      "100/100 [==============================] - 27s 273ms/step - loss: 0.6539 - accuracy: 0.8648 - dice_coef_loss: 0.6521\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6243 - accuracy: 0.8992 - dice_coef_loss: 0.6243\n",
      "Epoch 9: loss improved from 0.64553 to 0.62435, saving model to ./temp/polar_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 272ms/step - loss: 0.6243 - accuracy: 0.8992 - dice_coef_loss: 0.6243\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6229 - accuracy: 0.8841 - dice_coef_loss: 0.6229\n",
      "Epoch 10: loss improved from 0.62435 to 0.62290, saving model to ./temp/polar_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 270ms/step - loss: 0.6229 - accuracy: 0.8841 - dice_coef_loss: 0.6229\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6215 - accuracy: 0.8980 - dice_coef_loss: 0.6215\n",
      "Epoch 11: loss improved from 0.62290 to 0.62146, saving model to ./temp/polar_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 270ms/step - loss: 0.6215 - accuracy: 0.8980 - dice_coef_loss: 0.6215\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6169 - accuracy: 0.8877 - dice_coef_loss: 0.6169\n",
      "Epoch 12: loss improved from 0.62146 to 0.61687, saving model to ./temp/polar_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 270ms/step - loss: 0.6169 - accuracy: 0.8877 - dice_coef_loss: 0.6169\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6036 - accuracy: 0.8972 - dice_coef_loss: 0.6036\n",
      "Epoch 13: loss improved from 0.61687 to 0.60363, saving model to ./temp/polar_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 269ms/step - loss: 0.6036 - accuracy: 0.8972 - dice_coef_loss: 0.6036\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6263 - accuracy: 0.8914 - dice_coef_loss: 0.6263\n",
      "Epoch 14: loss did not improve from 0.60363\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6263 - accuracy: 0.8914 - dice_coef_loss: 0.6263\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6082 - accuracy: 0.8897 - dice_coef_loss: 0.6068\n",
      "Epoch 15: loss did not improve from 0.60363\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6082 - accuracy: 0.8897 - dice_coef_loss: 0.6068\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6084 - accuracy: 0.8976 - dice_coef_loss: 0.6084\n",
      "Epoch 16: loss did not improve from 0.60363\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6084 - accuracy: 0.8976 - dice_coef_loss: 0.6084\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6221 - accuracy: 0.8831 - dice_coef_loss: 0.6221\n",
      "Epoch 17: loss did not improve from 0.60363\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6221 - accuracy: 0.8831 - dice_coef_loss: 0.6221\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6111 - accuracy: 0.8996 - dice_coef_loss: 0.6111\n",
      "Epoch 18: loss did not improve from 0.60363\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6111 - accuracy: 0.8996 - dice_coef_loss: 0.6111\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5943 - accuracy: 0.9031 - dice_coef_loss: 0.5943\n",
      "Epoch 19: loss improved from 0.60363 to 0.59427, saving model to ./temp/polar_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5943 - accuracy: 0.9031 - dice_coef_loss: 0.5943\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5951 - accuracy: 0.9060 - dice_coef_loss: 0.5951\n",
      "Epoch 20: loss did not improve from 0.59427\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5951 - accuracy: 0.9060 - dice_coef_loss: 0.5951\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5997 - accuracy: 0.8933 - dice_coef_loss: 0.5997\n",
      "Epoch 21: loss did not improve from 0.59427\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5997 - accuracy: 0.8933 - dice_coef_loss: 0.5997\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5754 - accuracy: 0.9154 - dice_coef_loss: 0.5754\n",
      "Epoch 22: loss improved from 0.59427 to 0.57542, saving model to ./temp/polar_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5754 - accuracy: 0.9154 - dice_coef_loss: 0.5754\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5761 - accuracy: 0.9100 - dice_coef_loss: 0.5757\n",
      "Epoch 23: loss did not improve from 0.57542\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.5761 - accuracy: 0.9100 - dice_coef_loss: 0.5757\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5919 - accuracy: 0.8912 - dice_coef_loss: 0.5919\n",
      "Epoch 24: loss did not improve from 0.57542\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5919 - accuracy: 0.8912 - dice_coef_loss: 0.5919\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5756 - accuracy: 0.9103 - dice_coef_loss: 0.5756\n",
      "Epoch 25: loss did not improve from 0.57542\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5756 - accuracy: 0.9103 - dice_coef_loss: 0.5756\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5966 - accuracy: 0.9019 - dice_coef_loss: 0.5966\n",
      "Epoch 26: loss did not improve from 0.57542\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5966 - accuracy: 0.9019 - dice_coef_loss: 0.5966\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - ETA: 0s - loss: 0.5907 - accuracy: 0.9130 - dice_coef_loss: 0.5907\n",
      "Epoch 27: loss did not improve from 0.57542\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5907 - accuracy: 0.9130 - dice_coef_loss: 0.5907\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5822 - accuracy: 0.9078 - dice_coef_loss: 0.5822\n",
      "Epoch 28: loss did not improve from 0.57542\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5822 - accuracy: 0.9078 - dice_coef_loss: 0.5822\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5623 - accuracy: 0.9138 - dice_coef_loss: 0.5623\n",
      "Epoch 29: loss improved from 0.57542 to 0.56226, saving model to ./temp/polar_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5623 - accuracy: 0.9138 - dice_coef_loss: 0.5623\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5810 - accuracy: 0.9032 - dice_coef_loss: 0.5806\n",
      "Epoch 30: loss did not improve from 0.56226\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.5810 - accuracy: 0.9032 - dice_coef_loss: 0.5806\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5647 - accuracy: 0.9117 - dice_coef_loss: 0.5647\n",
      "Epoch 31: loss did not improve from 0.56226\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5647 - accuracy: 0.9117 - dice_coef_loss: 0.5647\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5695 - accuracy: 0.9154 - dice_coef_loss: 0.5695\n",
      "Epoch 32: loss did not improve from 0.56226\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5695 - accuracy: 0.9154 - dice_coef_loss: 0.5695\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5645 - accuracy: 0.9117 - dice_coef_loss: 0.5645\n",
      "Epoch 33: loss did not improve from 0.56226\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5645 - accuracy: 0.9117 - dice_coef_loss: 0.5645\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5626 - accuracy: 0.9139 - dice_coef_loss: 0.5626\n",
      "Epoch 34: loss did not improve from 0.56226\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5626 - accuracy: 0.9139 - dice_coef_loss: 0.5626\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5754 - accuracy: 0.9183 - dice_coef_loss: 0.5754\n",
      "Epoch 35: loss did not improve from 0.56226\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5754 - accuracy: 0.9183 - dice_coef_loss: 0.5754\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5824 - accuracy: 0.9176 - dice_coef_loss: 0.5824\n",
      "Epoch 36: loss did not improve from 0.56226\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5824 - accuracy: 0.9176 - dice_coef_loss: 0.5824\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5481 - accuracy: 0.9178 - dice_coef_loss: 0.5481\n",
      "Epoch 37: loss improved from 0.56226 to 0.54812, saving model to ./temp/polar_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5481 - accuracy: 0.9178 - dice_coef_loss: 0.5481\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5683 - accuracy: 0.8990 - dice_coef_loss: 0.5676\n",
      "Epoch 38: loss did not improve from 0.54812\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.5683 - accuracy: 0.8990 - dice_coef_loss: 0.5676\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5595 - accuracy: 0.9153 - dice_coef_loss: 0.5595\n",
      "Epoch 39: loss did not improve from 0.54812\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5595 - accuracy: 0.9153 - dice_coef_loss: 0.5595\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5610 - accuracy: 0.9190 - dice_coef_loss: 0.5610\n",
      "Epoch 40: loss did not improve from 0.54812\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5610 - accuracy: 0.9190 - dice_coef_loss: 0.5610\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5372 - accuracy: 0.9252 - dice_coef_loss: 0.5372\n",
      "Epoch 41: loss improved from 0.54812 to 0.53717, saving model to ./temp/polar_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5372 - accuracy: 0.9252 - dice_coef_loss: 0.5372\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5692 - accuracy: 0.9076 - dice_coef_loss: 0.5692\n",
      "Epoch 42: loss did not improve from 0.53717\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5692 - accuracy: 0.9076 - dice_coef_loss: 0.5692\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5440 - accuracy: 0.9205 - dice_coef_loss: 0.5440\n",
      "Epoch 43: loss did not improve from 0.53717\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5440 - accuracy: 0.9205 - dice_coef_loss: 0.5440\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5588 - accuracy: 0.9265 - dice_coef_loss: 0.5588\n",
      "Epoch 44: loss did not improve from 0.53717\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5588 - accuracy: 0.9265 - dice_coef_loss: 0.5588\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5652 - accuracy: 0.9220 - dice_coef_loss: 0.5682\n",
      "Epoch 45: loss did not improve from 0.53717\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.5652 - accuracy: 0.9220 - dice_coef_loss: 0.5682\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5653 - accuracy: 0.9153 - dice_coef_loss: 0.5653\n",
      "Epoch 46: loss did not improve from 0.53717\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5653 - accuracy: 0.9153 - dice_coef_loss: 0.5653\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5423 - accuracy: 0.9178 - dice_coef_loss: 0.5423\n",
      "Epoch 47: loss did not improve from 0.53717\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5423 - accuracy: 0.9178 - dice_coef_loss: 0.5423\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5305 - accuracy: 0.9248 - dice_coef_loss: 0.5305\n",
      "Epoch 48: loss improved from 0.53717 to 0.53054, saving model to ./temp/polar_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5305 - accuracy: 0.9248 - dice_coef_loss: 0.5305\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5460 - accuracy: 0.9215 - dice_coef_loss: 0.5460\n",
      "Epoch 49: loss did not improve from 0.53054\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5460 - accuracy: 0.9215 - dice_coef_loss: 0.5460\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5597 - accuracy: 0.9122 - dice_coef_loss: 0.5597\n",
      "Epoch 50: loss did not improve from 0.53054\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5597 - accuracy: 0.9122 - dice_coef_loss: 0.5597\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5639 - accuracy: 0.9135 - dice_coef_loss: 0.5639\n",
      "Epoch 51: loss did not improve from 0.53054\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5639 - accuracy: 0.9135 - dice_coef_loss: 0.5639\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5441 - accuracy: 0.9193 - dice_coef_loss: 0.5444\n",
      "Epoch 52: loss did not improve from 0.53054\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.5441 - accuracy: 0.9193 - dice_coef_loss: 0.5444\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5510 - accuracy: 0.9190 - dice_coef_loss: 0.5510\n",
      "Epoch 53: loss did not improve from 0.53054\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5510 - accuracy: 0.9190 - dice_coef_loss: 0.5510\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5434 - accuracy: 0.9251 - dice_coef_loss: 0.5434\n",
      "Epoch 54: loss did not improve from 0.53054\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5434 - accuracy: 0.9251 - dice_coef_loss: 0.5434\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - ETA: 0s - loss: 0.5402 - accuracy: 0.9186 - dice_coef_loss: 0.5402\n",
      "Epoch 55: loss did not improve from 0.53054\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5402 - accuracy: 0.9186 - dice_coef_loss: 0.5402\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5393 - accuracy: 0.9202 - dice_coef_loss: 0.5393\n",
      "Epoch 56: loss did not improve from 0.53054\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5393 - accuracy: 0.9202 - dice_coef_loss: 0.5393\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5482 - accuracy: 0.9240 - dice_coef_loss: 0.5482\n",
      "Epoch 57: loss did not improve from 0.53054\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5482 - accuracy: 0.9240 - dice_coef_loss: 0.5482\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5394 - accuracy: 0.9181 - dice_coef_loss: 0.5394\n",
      "Epoch 58: loss did not improve from 0.53054\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5394 - accuracy: 0.9181 - dice_coef_loss: 0.5394\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5235 - accuracy: 0.9296 - dice_coef_loss: 0.5235\n",
      "Epoch 59: loss improved from 0.53054 to 0.52347, saving model to ./temp/polar_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5235 - accuracy: 0.9296 - dice_coef_loss: 0.5235\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5284 - accuracy: 0.9256 - dice_coef_loss: 0.5275\n",
      "Epoch 60: loss did not improve from 0.52347\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.5284 - accuracy: 0.9256 - dice_coef_loss: 0.5275\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5139 - accuracy: 0.9349 - dice_coef_loss: 0.5139\n",
      "Epoch 61: loss improved from 0.52347 to 0.51386, saving model to ./temp/polar_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5139 - accuracy: 0.9349 - dice_coef_loss: 0.5139\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5250 - accuracy: 0.9272 - dice_coef_loss: 0.5250\n",
      "Epoch 62: loss did not improve from 0.51386\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5250 - accuracy: 0.9272 - dice_coef_loss: 0.5250\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5372 - accuracy: 0.9160 - dice_coef_loss: 0.5372\n",
      "Epoch 63: loss did not improve from 0.51386\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5372 - accuracy: 0.9160 - dice_coef_loss: 0.5372\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5456 - accuracy: 0.9177 - dice_coef_loss: 0.5456\n",
      "Epoch 64: loss did not improve from 0.51386\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5456 - accuracy: 0.9177 - dice_coef_loss: 0.5456\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5258 - accuracy: 0.9192 - dice_coef_loss: 0.5258\n",
      "Epoch 65: loss did not improve from 0.51386\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5258 - accuracy: 0.9192 - dice_coef_loss: 0.5258\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5346 - accuracy: 0.9215 - dice_coef_loss: 0.5346\n",
      "Epoch 66: loss did not improve from 0.51386\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5346 - accuracy: 0.9215 - dice_coef_loss: 0.5346\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5210 - accuracy: 0.9199 - dice_coef_loss: 0.5200\n",
      "Epoch 67: loss did not improve from 0.51386\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.5210 - accuracy: 0.9199 - dice_coef_loss: 0.5200\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5365 - accuracy: 0.9195 - dice_coef_loss: 0.5365\n",
      "Epoch 68: loss did not improve from 0.51386\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5365 - accuracy: 0.9195 - dice_coef_loss: 0.5365\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5426 - accuracy: 0.9147 - dice_coef_loss: 0.5426\n",
      "Epoch 69: loss did not improve from 0.51386\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5426 - accuracy: 0.9147 - dice_coef_loss: 0.5426\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5270 - accuracy: 0.9298 - dice_coef_loss: 0.5270\n",
      "Epoch 70: loss did not improve from 0.51386\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5270 - accuracy: 0.9298 - dice_coef_loss: 0.5270\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5289 - accuracy: 0.9183 - dice_coef_loss: 0.5289\n",
      "Epoch 71: loss did not improve from 0.51386\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5289 - accuracy: 0.9183 - dice_coef_loss: 0.5289\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5317 - accuracy: 0.9300 - dice_coef_loss: 0.5317\n",
      "Epoch 72: loss did not improve from 0.51386\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5317 - accuracy: 0.9300 - dice_coef_loss: 0.5317\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5200 - accuracy: 0.9290 - dice_coef_loss: 0.5200\n",
      "Epoch 73: loss did not improve from 0.51386\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5200 - accuracy: 0.9290 - dice_coef_loss: 0.5200\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5259 - accuracy: 0.9215 - dice_coef_loss: 0.5259\n",
      "Epoch 74: loss did not improve from 0.51386\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5259 - accuracy: 0.9215 - dice_coef_loss: 0.5259\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5218 - accuracy: 0.9289 - dice_coef_loss: 0.5206\n",
      "Epoch 75: loss did not improve from 0.51386\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.5218 - accuracy: 0.9289 - dice_coef_loss: 0.5206\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5177 - accuracy: 0.9279 - dice_coef_loss: 0.5177\n",
      "Epoch 76: loss did not improve from 0.51386\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5177 - accuracy: 0.9279 - dice_coef_loss: 0.5177\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5280 - accuracy: 0.9285 - dice_coef_loss: 0.5280\n",
      "Epoch 77: loss did not improve from 0.51386\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5280 - accuracy: 0.9285 - dice_coef_loss: 0.5280\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5198 - accuracy: 0.9322 - dice_coef_loss: 0.5198\n",
      "Epoch 78: loss did not improve from 0.51386\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5198 - accuracy: 0.9322 - dice_coef_loss: 0.5198\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5026 - accuracy: 0.9347 - dice_coef_loss: 0.5026\n",
      "Epoch 79: loss improved from 0.51386 to 0.50256, saving model to ./temp/polar_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5026 - accuracy: 0.9347 - dice_coef_loss: 0.5026\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5132 - accuracy: 0.9261 - dice_coef_loss: 0.5132\n",
      "Epoch 80: loss did not improve from 0.50256\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5132 - accuracy: 0.9261 - dice_coef_loss: 0.5132\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5286 - accuracy: 0.9281 - dice_coef_loss: 0.5286\n",
      "Epoch 81: loss did not improve from 0.50256\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5286 - accuracy: 0.9281 - dice_coef_loss: 0.5286\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5189 - accuracy: 0.9285 - dice_coef_loss: 0.5201\n",
      "Epoch 82: loss did not improve from 0.50256\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.5189 - accuracy: 0.9285 - dice_coef_loss: 0.5201\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - ETA: 0s - loss: 0.5212 - accuracy: 0.9271 - dice_coef_loss: 0.5212\n",
      "Epoch 83: loss did not improve from 0.50256\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5212 - accuracy: 0.9271 - dice_coef_loss: 0.5212\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5165 - accuracy: 0.9196 - dice_coef_loss: 0.5165\n",
      "Epoch 84: loss did not improve from 0.50256\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5165 - accuracy: 0.9196 - dice_coef_loss: 0.5165\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4875 - accuracy: 0.9426 - dice_coef_loss: 0.4875\n",
      "Epoch 85: loss improved from 0.50256 to 0.48747, saving model to ./temp/polar_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.4875 - accuracy: 0.9426 - dice_coef_loss: 0.4875\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5015 - accuracy: 0.9309 - dice_coef_loss: 0.5015\n",
      "Epoch 86: loss did not improve from 0.48747\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5015 - accuracy: 0.9309 - dice_coef_loss: 0.5015\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5119 - accuracy: 0.9313 - dice_coef_loss: 0.5119\n",
      "Epoch 87: loss did not improve from 0.48747\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5119 - accuracy: 0.9313 - dice_coef_loss: 0.5119\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5160 - accuracy: 0.9287 - dice_coef_loss: 0.5160\n",
      "Epoch 88: loss did not improve from 0.48747\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5160 - accuracy: 0.9287 - dice_coef_loss: 0.5160\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5185 - accuracy: 0.9250 - dice_coef_loss: 0.5174\n",
      "Epoch 89: loss did not improve from 0.48747\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.5185 - accuracy: 0.9250 - dice_coef_loss: 0.5174\n",
      "Epoch 90/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5134 - accuracy: 0.9279 - dice_coef_loss: 0.5134\n",
      "Epoch 90: loss did not improve from 0.48747\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5134 - accuracy: 0.9279 - dice_coef_loss: 0.5134\n",
      "Epoch 91/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5153 - accuracy: 0.9279 - dice_coef_loss: 0.5153\n",
      "Epoch 91: loss did not improve from 0.48747\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5153 - accuracy: 0.9279 - dice_coef_loss: 0.5153\n",
      "Epoch 92/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5356 - accuracy: 0.9195 - dice_coef_loss: 0.5356\n",
      "Epoch 92: loss did not improve from 0.48747\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5356 - accuracy: 0.9195 - dice_coef_loss: 0.5356\n",
      "Epoch 93/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5163 - accuracy: 0.9273 - dice_coef_loss: 0.5163\n",
      "Epoch 93: loss did not improve from 0.48747\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5163 - accuracy: 0.9273 - dice_coef_loss: 0.5163\n",
      "Epoch 94/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5008 - accuracy: 0.9327 - dice_coef_loss: 0.5008\n",
      "Epoch 94: loss did not improve from 0.48747\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5008 - accuracy: 0.9327 - dice_coef_loss: 0.5008\n",
      "Epoch 95/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4975 - accuracy: 0.9323 - dice_coef_loss: 0.4975\n",
      "Epoch 95: loss did not improve from 0.48747\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.4975 - accuracy: 0.9323 - dice_coef_loss: 0.4975\n",
      "Epoch 96/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5114 - accuracy: 0.9331 - dice_coef_loss: 0.5114\n",
      "Epoch 96: loss did not improve from 0.48747\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5114 - accuracy: 0.9331 - dice_coef_loss: 0.5114\n",
      "Epoch 97/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4966 - accuracy: 0.9308 - dice_coef_loss: 0.4957\n",
      "Epoch 97: loss did not improve from 0.48747\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.4966 - accuracy: 0.9308 - dice_coef_loss: 0.4957\n",
      "Epoch 98/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5053 - accuracy: 0.9261 - dice_coef_loss: 0.5053\n",
      "Epoch 98: loss did not improve from 0.48747\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5053 - accuracy: 0.9261 - dice_coef_loss: 0.5053\n",
      "Epoch 99/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4855 - accuracy: 0.9336 - dice_coef_loss: 0.4855\n",
      "Epoch 99: loss improved from 0.48747 to 0.48553, saving model to ./temp/polar_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.4855 - accuracy: 0.9336 - dice_coef_loss: 0.4855\n",
      "Epoch 100/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5072 - accuracy: 0.9305 - dice_coef_loss: 0.5072\n",
      "Epoch 100: loss did not improve from 0.48553\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5072 - accuracy: 0.9305 - dice_coef_loss: 0.5072\n",
      "Found 2961 images belonging to 1 classes.\n",
      "Found 2961 images belonging to 1 classes.\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcd5de8c830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcd5de8c830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.7375 - accuracy: 0.6337 - dice_coef_loss: 0.7375\n",
      "Epoch 1: loss improved from inf to 0.73750, saving model to ./temp/polar_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 262ms/step - loss: 0.7375 - accuracy: 0.6337 - dice_coef_loss: 0.7375\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5707 - accuracy: 0.8347 - dice_coef_loss: 0.5707\n",
      "Epoch 2: loss improved from 0.73750 to 0.57068, saving model to ./temp/polar_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5707 - accuracy: 0.8347 - dice_coef_loss: 0.5707\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5460 - accuracy: 0.8651 - dice_coef_loss: 0.5460\n",
      "Epoch 3: loss improved from 0.57068 to 0.54599, saving model to ./temp/polar_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5460 - accuracy: 0.8651 - dice_coef_loss: 0.5460\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5421 - accuracy: 0.8694 - dice_coef_loss: 0.5421\n",
      "Epoch 4: loss improved from 0.54599 to 0.54210, saving model to ./temp/polar_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5421 - accuracy: 0.8694 - dice_coef_loss: 0.5421\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4940 - accuracy: 0.8930 - dice_coef_loss: 0.4940\n",
      "Epoch 5: loss improved from 0.54210 to 0.49400, saving model to ./temp/polar_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.4940 - accuracy: 0.8930 - dice_coef_loss: 0.4940\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5023 - accuracy: 0.8848 - dice_coef_loss: 0.5023\n",
      "Epoch 6: loss did not improve from 0.49400\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5023 - accuracy: 0.8848 - dice_coef_loss: 0.5023\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - ETA: 0s - loss: 0.4539 - accuracy: 0.8965 - dice_coef_loss: 0.4539\n",
      "Epoch 7: loss improved from 0.49400 to 0.45386, saving model to ./temp/polar_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.4539 - accuracy: 0.8965 - dice_coef_loss: 0.4539\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4654 - accuracy: 0.9013 - dice_coef_loss: 0.4683\n",
      "Epoch 8: loss did not improve from 0.45386\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.4654 - accuracy: 0.9013 - dice_coef_loss: 0.4683\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4770 - accuracy: 0.8885 - dice_coef_loss: 0.4770\n",
      "Epoch 9: loss did not improve from 0.45386\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.4770 - accuracy: 0.8885 - dice_coef_loss: 0.4770\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4577 - accuracy: 0.8946 - dice_coef_loss: 0.4577\n",
      "Epoch 10: loss did not improve from 0.45386\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.4577 - accuracy: 0.8946 - dice_coef_loss: 0.4577\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4335 - accuracy: 0.9071 - dice_coef_loss: 0.4335\n",
      "Epoch 11: loss improved from 0.45386 to 0.43353, saving model to ./temp/polar_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.4335 - accuracy: 0.9071 - dice_coef_loss: 0.4335\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4761 - accuracy: 0.8916 - dice_coef_loss: 0.4761\n",
      "Epoch 12: loss did not improve from 0.43353\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4761 - accuracy: 0.8916 - dice_coef_loss: 0.4761\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4313 - accuracy: 0.9090 - dice_coef_loss: 0.4313\n",
      "Epoch 13: loss improved from 0.43353 to 0.43127, saving model to ./temp/polar_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.4313 - accuracy: 0.9090 - dice_coef_loss: 0.4313\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4335 - accuracy: 0.9054 - dice_coef_loss: 0.4335\n",
      "Epoch 14: loss did not improve from 0.43127\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4335 - accuracy: 0.9054 - dice_coef_loss: 0.4335\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4216 - accuracy: 0.9127 - dice_coef_loss: 0.4193\n",
      "Epoch 15: loss improved from 0.43127 to 0.42158, saving model to ./temp/polar_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 265ms/step - loss: 0.4216 - accuracy: 0.9127 - dice_coef_loss: 0.4193\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4154 - accuracy: 0.9144 - dice_coef_loss: 0.4154\n",
      "Epoch 16: loss improved from 0.42158 to 0.41544, saving model to ./temp/polar_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.4154 - accuracy: 0.9144 - dice_coef_loss: 0.4154\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4266 - accuracy: 0.9078 - dice_coef_loss: 0.4266\n",
      "Epoch 17: loss did not improve from 0.41544\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.4266 - accuracy: 0.9078 - dice_coef_loss: 0.4266\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4083 - accuracy: 0.9056 - dice_coef_loss: 0.4083\n",
      "Epoch 18: loss improved from 0.41544 to 0.40827, saving model to ./temp/polar_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.4083 - accuracy: 0.9056 - dice_coef_loss: 0.4083\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4082 - accuracy: 0.9101 - dice_coef_loss: 0.4082\n",
      "Epoch 19: loss improved from 0.40827 to 0.40818, saving model to ./temp/polar_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.4082 - accuracy: 0.9101 - dice_coef_loss: 0.4082\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4125 - accuracy: 0.8969 - dice_coef_loss: 0.4125\n",
      "Epoch 20: loss did not improve from 0.40818\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4125 - accuracy: 0.8969 - dice_coef_loss: 0.4125\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4274 - accuracy: 0.9044 - dice_coef_loss: 0.4274\n",
      "Epoch 21: loss did not improve from 0.40818\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4274 - accuracy: 0.9044 - dice_coef_loss: 0.4274\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3874 - accuracy: 0.9178 - dice_coef_loss: 0.3874\n",
      "Epoch 22: loss improved from 0.40818 to 0.38744, saving model to ./temp/polar_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 0.3874 - accuracy: 0.9178 - dice_coef_loss: 0.3874\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3972 - accuracy: 0.9052 - dice_coef_loss: 0.3960\n",
      "Epoch 23: loss did not improve from 0.38744\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.3972 - accuracy: 0.9052 - dice_coef_loss: 0.3960\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4112 - accuracy: 0.9102 - dice_coef_loss: 0.4112\n",
      "Epoch 24: loss did not improve from 0.38744\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4112 - accuracy: 0.9102 - dice_coef_loss: 0.4112\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3715 - accuracy: 0.9186 - dice_coef_loss: 0.3715\n",
      "Epoch 25: loss improved from 0.38744 to 0.37151, saving model to ./temp/polar_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.3715 - accuracy: 0.9186 - dice_coef_loss: 0.3715\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3786 - accuracy: 0.9185 - dice_coef_loss: 0.3786\n",
      "Epoch 26: loss did not improve from 0.37151\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3786 - accuracy: 0.9185 - dice_coef_loss: 0.3786\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3511 - accuracy: 0.9222 - dice_coef_loss: 0.3511\n",
      "Epoch 27: loss improved from 0.37151 to 0.35110, saving model to ./temp/polar_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.3511 - accuracy: 0.9222 - dice_coef_loss: 0.3511\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3776 - accuracy: 0.9196 - dice_coef_loss: 0.3776\n",
      "Epoch 28: loss did not improve from 0.35110\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3776 - accuracy: 0.9196 - dice_coef_loss: 0.3776\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3565 - accuracy: 0.9183 - dice_coef_loss: 0.3565\n",
      "Epoch 29: loss did not improve from 0.35110\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3565 - accuracy: 0.9183 - dice_coef_loss: 0.3565\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3784 - accuracy: 0.9148 - dice_coef_loss: 0.3776\n",
      "Epoch 30: loss did not improve from 0.35110\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.3784 - accuracy: 0.9148 - dice_coef_loss: 0.3776\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3464 - accuracy: 0.9273 - dice_coef_loss: 0.3464\n",
      "Epoch 31: loss improved from 0.35110 to 0.34641, saving model to ./temp/polar_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.3464 - accuracy: 0.9273 - dice_coef_loss: 0.3464\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4038 - accuracy: 0.9200 - dice_coef_loss: 0.4038\n",
      "Epoch 32: loss did not improve from 0.34641\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4038 - accuracy: 0.9200 - dice_coef_loss: 0.4038\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3839 - accuracy: 0.9100 - dice_coef_loss: 0.3839\n",
      "Epoch 33: loss did not improve from 0.34641\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3839 - accuracy: 0.9100 - dice_coef_loss: 0.3839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3841 - accuracy: 0.9115 - dice_coef_loss: 0.3841\n",
      "Epoch 34: loss did not improve from 0.34641\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3841 - accuracy: 0.9115 - dice_coef_loss: 0.3841\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3448 - accuracy: 0.9183 - dice_coef_loss: 0.3448\n",
      "Epoch 35: loss improved from 0.34641 to 0.34479, saving model to ./temp/polar_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.3448 - accuracy: 0.9183 - dice_coef_loss: 0.3448\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3414 - accuracy: 0.9209 - dice_coef_loss: 0.3414\n",
      "Epoch 36: loss improved from 0.34479 to 0.34143, saving model to ./temp/polar_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.3414 - accuracy: 0.9209 - dice_coef_loss: 0.3414\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3579 - accuracy: 0.9164 - dice_coef_loss: 0.3579\n",
      "Epoch 37: loss did not improve from 0.34143\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3579 - accuracy: 0.9164 - dice_coef_loss: 0.3579\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3680 - accuracy: 0.9170 - dice_coef_loss: 0.3669\n",
      "Epoch 38: loss did not improve from 0.34143\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.3680 - accuracy: 0.9170 - dice_coef_loss: 0.3669\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3657 - accuracy: 0.9214 - dice_coef_loss: 0.3657\n",
      "Epoch 39: loss did not improve from 0.34143\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3657 - accuracy: 0.9214 - dice_coef_loss: 0.3657\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3405 - accuracy: 0.9230 - dice_coef_loss: 0.3405\n",
      "Epoch 40: loss improved from 0.34143 to 0.34049, saving model to ./temp/polar_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.3405 - accuracy: 0.9230 - dice_coef_loss: 0.3405\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3595 - accuracy: 0.9195 - dice_coef_loss: 0.3595\n",
      "Epoch 41: loss did not improve from 0.34049\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3595 - accuracy: 0.9195 - dice_coef_loss: 0.3595\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3346 - accuracy: 0.9202 - dice_coef_loss: 0.3346\n",
      "Epoch 42: loss improved from 0.34049 to 0.33460, saving model to ./temp/polar_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.3346 - accuracy: 0.9202 - dice_coef_loss: 0.3346\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3398 - accuracy: 0.9294 - dice_coef_loss: 0.3398\n",
      "Epoch 43: loss did not improve from 0.33460\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3398 - accuracy: 0.9294 - dice_coef_loss: 0.3398\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3368 - accuracy: 0.9294 - dice_coef_loss: 0.3368\n",
      "Epoch 44: loss did not improve from 0.33460\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3368 - accuracy: 0.9294 - dice_coef_loss: 0.3368\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3703 - accuracy: 0.9210 - dice_coef_loss: 0.3708\n",
      "Epoch 45: loss did not improve from 0.33460\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.3703 - accuracy: 0.9210 - dice_coef_loss: 0.3708\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3503 - accuracy: 0.9240 - dice_coef_loss: 0.3503\n",
      "Epoch 46: loss did not improve from 0.33460\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3503 - accuracy: 0.9240 - dice_coef_loss: 0.3503\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3399 - accuracy: 0.9253 - dice_coef_loss: 0.3399\n",
      "Epoch 47: loss did not improve from 0.33460\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3399 - accuracy: 0.9253 - dice_coef_loss: 0.3399\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3284 - accuracy: 0.9314 - dice_coef_loss: 0.3284\n",
      "Epoch 48: loss improved from 0.33460 to 0.32838, saving model to ./temp/polar_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.3284 - accuracy: 0.9314 - dice_coef_loss: 0.3284\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3492 - accuracy: 0.9169 - dice_coef_loss: 0.3492\n",
      "Epoch 49: loss did not improve from 0.32838\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3492 - accuracy: 0.9169 - dice_coef_loss: 0.3492\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3653 - accuracy: 0.9242 - dice_coef_loss: 0.3653\n",
      "Epoch 50: loss did not improve from 0.32838\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3653 - accuracy: 0.9242 - dice_coef_loss: 0.3653\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3375 - accuracy: 0.9217 - dice_coef_loss: 0.3375\n",
      "Epoch 51: loss did not improve from 0.32838\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3375 - accuracy: 0.9217 - dice_coef_loss: 0.3375\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3347 - accuracy: 0.9261 - dice_coef_loss: 0.3343\n",
      "Epoch 52: loss did not improve from 0.32838\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.3347 - accuracy: 0.9261 - dice_coef_loss: 0.3343\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3323 - accuracy: 0.9249 - dice_coef_loss: 0.3323\n",
      "Epoch 53: loss did not improve from 0.32838\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3323 - accuracy: 0.9249 - dice_coef_loss: 0.3323\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3288 - accuracy: 0.9285 - dice_coef_loss: 0.3288\n",
      "Epoch 54: loss did not improve from 0.32838\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3288 - accuracy: 0.9285 - dice_coef_loss: 0.3288\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3350 - accuracy: 0.9309 - dice_coef_loss: 0.3350\n",
      "Epoch 55: loss did not improve from 0.32838\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3350 - accuracy: 0.9309 - dice_coef_loss: 0.3350\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3345 - accuracy: 0.9295 - dice_coef_loss: 0.3345\n",
      "Epoch 56: loss did not improve from 0.32838\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3345 - accuracy: 0.9295 - dice_coef_loss: 0.3345\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3091 - accuracy: 0.9276 - dice_coef_loss: 0.3091\n",
      "Epoch 57: loss improved from 0.32838 to 0.30915, saving model to ./temp/polar_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.3091 - accuracy: 0.9276 - dice_coef_loss: 0.3091\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3550 - accuracy: 0.9135 - dice_coef_loss: 0.3550\n",
      "Epoch 58: loss did not improve from 0.30915\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3550 - accuracy: 0.9135 - dice_coef_loss: 0.3550\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3136 - accuracy: 0.9325 - dice_coef_loss: 0.3136\n",
      "Epoch 59: loss did not improve from 0.30915\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3136 - accuracy: 0.9325 - dice_coef_loss: 0.3136\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3274 - accuracy: 0.9231 - dice_coef_loss: 0.3258\n",
      "Epoch 60: loss did not improve from 0.30915\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.3274 - accuracy: 0.9231 - dice_coef_loss: 0.3258\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3011 - accuracy: 0.9257 - dice_coef_loss: 0.3011\n",
      "Epoch 61: loss improved from 0.30915 to 0.30113, saving model to ./temp/polar_Dom/1/checkpoint.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 27s 268ms/step - loss: 0.3011 - accuracy: 0.9257 - dice_coef_loss: 0.3011\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2996 - accuracy: 0.9315 - dice_coef_loss: 0.2996\n",
      "Epoch 62: loss improved from 0.30113 to 0.29961, saving model to ./temp/polar_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.2996 - accuracy: 0.9315 - dice_coef_loss: 0.2996\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3157 - accuracy: 0.9342 - dice_coef_loss: 0.3157\n",
      "Epoch 63: loss did not improve from 0.29961\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3157 - accuracy: 0.9342 - dice_coef_loss: 0.3157\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3572 - accuracy: 0.9172 - dice_coef_loss: 0.3572\n",
      "Epoch 64: loss did not improve from 0.29961\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3572 - accuracy: 0.9172 - dice_coef_loss: 0.3572\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3116 - accuracy: 0.9328 - dice_coef_loss: 0.3116\n",
      "Epoch 65: loss did not improve from 0.29961\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3116 - accuracy: 0.9328 - dice_coef_loss: 0.3116\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3083 - accuracy: 0.9323 - dice_coef_loss: 0.3083\n",
      "Epoch 66: loss did not improve from 0.29961\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3083 - accuracy: 0.9323 - dice_coef_loss: 0.3083\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3133 - accuracy: 0.9271 - dice_coef_loss: 0.3185\n",
      "Epoch 67: loss did not improve from 0.29961\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.3133 - accuracy: 0.9271 - dice_coef_loss: 0.3185\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3288 - accuracy: 0.9263 - dice_coef_loss: 0.3288\n",
      "Epoch 68: loss did not improve from 0.29961\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3288 - accuracy: 0.9263 - dice_coef_loss: 0.3288\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3110 - accuracy: 0.9308 - dice_coef_loss: 0.3110\n",
      "Epoch 69: loss did not improve from 0.29961\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3110 - accuracy: 0.9308 - dice_coef_loss: 0.3110\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3055 - accuracy: 0.9305 - dice_coef_loss: 0.3055\n",
      "Epoch 70: loss did not improve from 0.29961\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3055 - accuracy: 0.9305 - dice_coef_loss: 0.3055\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3019 - accuracy: 0.9277 - dice_coef_loss: 0.3019\n",
      "Epoch 71: loss did not improve from 0.29961\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3019 - accuracy: 0.9277 - dice_coef_loss: 0.3019\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2980 - accuracy: 0.9339 - dice_coef_loss: 0.2980\n",
      "Epoch 72: loss improved from 0.29961 to 0.29795, saving model to ./temp/polar_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.2980 - accuracy: 0.9339 - dice_coef_loss: 0.2980\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3186 - accuracy: 0.9261 - dice_coef_loss: 0.3186\n",
      "Epoch 73: loss did not improve from 0.29795\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3186 - accuracy: 0.9261 - dice_coef_loss: 0.3186\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3128 - accuracy: 0.9263 - dice_coef_loss: 0.3128\n",
      "Epoch 74: loss did not improve from 0.29795\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3128 - accuracy: 0.9263 - dice_coef_loss: 0.3128\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3157 - accuracy: 0.9298 - dice_coef_loss: 0.3138\n",
      "Epoch 75: loss did not improve from 0.29795\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.3157 - accuracy: 0.9298 - dice_coef_loss: 0.3138\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3130 - accuracy: 0.9292 - dice_coef_loss: 0.3130\n",
      "Epoch 76: loss did not improve from 0.29795\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3130 - accuracy: 0.9292 - dice_coef_loss: 0.3130\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3166 - accuracy: 0.9309 - dice_coef_loss: 0.3166\n",
      "Epoch 77: loss did not improve from 0.29795\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3166 - accuracy: 0.9309 - dice_coef_loss: 0.3166\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3269 - accuracy: 0.9266 - dice_coef_loss: 0.3269\n",
      "Epoch 78: loss did not improve from 0.29795\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3269 - accuracy: 0.9266 - dice_coef_loss: 0.3269\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3042 - accuracy: 0.9337 - dice_coef_loss: 0.3042\n",
      "Epoch 79: loss did not improve from 0.29795\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3042 - accuracy: 0.9337 - dice_coef_loss: 0.3042\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2915 - accuracy: 0.9336 - dice_coef_loss: 0.2915\n",
      "Epoch 80: loss improved from 0.29795 to 0.29146, saving model to ./temp/polar_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.2915 - accuracy: 0.9336 - dice_coef_loss: 0.2915\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3161 - accuracy: 0.9301 - dice_coef_loss: 0.3161\n",
      "Epoch 81: loss did not improve from 0.29146\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3161 - accuracy: 0.9301 - dice_coef_loss: 0.3161\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3037 - accuracy: 0.9311 - dice_coef_loss: 0.3046\n",
      "Epoch 82: loss did not improve from 0.29146\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.3037 - accuracy: 0.9311 - dice_coef_loss: 0.3046\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2890 - accuracy: 0.9316 - dice_coef_loss: 0.2890\n",
      "Epoch 83: loss improved from 0.29146 to 0.28900, saving model to ./temp/polar_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.2890 - accuracy: 0.9316 - dice_coef_loss: 0.2890\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3085 - accuracy: 0.9336 - dice_coef_loss: 0.3085\n",
      "Epoch 84: loss did not improve from 0.28900\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3085 - accuracy: 0.9336 - dice_coef_loss: 0.3085\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2838 - accuracy: 0.9396 - dice_coef_loss: 0.2838\n",
      "Epoch 85: loss improved from 0.28900 to 0.28382, saving model to ./temp/polar_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.2838 - accuracy: 0.9396 - dice_coef_loss: 0.2838\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2983 - accuracy: 0.9304 - dice_coef_loss: 0.2983\n",
      "Epoch 86: loss did not improve from 0.28382\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.2983 - accuracy: 0.9304 - dice_coef_loss: 0.2983\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3020 - accuracy: 0.9295 - dice_coef_loss: 0.3020\n",
      "Epoch 87: loss did not improve from 0.28382\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3020 - accuracy: 0.9295 - dice_coef_loss: 0.3020\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2820 - accuracy: 0.9345 - dice_coef_loss: 0.2820\n",
      "Epoch 88: loss improved from 0.28382 to 0.28201, saving model to ./temp/polar_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.2820 - accuracy: 0.9345 - dice_coef_loss: 0.2820\n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - ETA: 0s - loss: 0.3028 - accuracy: 0.9327 - dice_coef_loss: 0.3015\n",
      "Epoch 89: loss did not improve from 0.28201\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.3028 - accuracy: 0.9327 - dice_coef_loss: 0.3015\n",
      "Epoch 90/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2620 - accuracy: 0.9389 - dice_coef_loss: 0.2620\n",
      "Epoch 90: loss improved from 0.28201 to 0.26200, saving model to ./temp/polar_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.2620 - accuracy: 0.9389 - dice_coef_loss: 0.2620\n",
      "Epoch 91/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3231 - accuracy: 0.9273 - dice_coef_loss: 0.3231\n",
      "Epoch 91: loss did not improve from 0.26200\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3231 - accuracy: 0.9273 - dice_coef_loss: 0.3231\n",
      "Epoch 92/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2976 - accuracy: 0.9285 - dice_coef_loss: 0.2976\n",
      "Epoch 92: loss did not improve from 0.26200\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.2976 - accuracy: 0.9285 - dice_coef_loss: 0.2976\n",
      "Epoch 93/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2704 - accuracy: 0.9354 - dice_coef_loss: 0.2704\n",
      "Epoch 93: loss did not improve from 0.26200\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.2704 - accuracy: 0.9354 - dice_coef_loss: 0.2704\n",
      "Epoch 94/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2763 - accuracy: 0.9341 - dice_coef_loss: 0.2763\n",
      "Epoch 94: loss did not improve from 0.26200\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.2763 - accuracy: 0.9341 - dice_coef_loss: 0.2763\n",
      "Epoch 95/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3106 - accuracy: 0.9300 - dice_coef_loss: 0.3106\n",
      "Epoch 95: loss did not improve from 0.26200\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3106 - accuracy: 0.9300 - dice_coef_loss: 0.3106\n",
      "Epoch 96/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2793 - accuracy: 0.9338 - dice_coef_loss: 0.2793\n",
      "Epoch 96: loss did not improve from 0.26200\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.2793 - accuracy: 0.9338 - dice_coef_loss: 0.2793\n",
      "Epoch 97/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2784 - accuracy: 0.9345 - dice_coef_loss: 0.2773\n",
      "Epoch 97: loss did not improve from 0.26200\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.2784 - accuracy: 0.9345 - dice_coef_loss: 0.2773\n",
      "Epoch 98/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2998 - accuracy: 0.9366 - dice_coef_loss: 0.2998\n",
      "Epoch 98: loss did not improve from 0.26200\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.2998 - accuracy: 0.9366 - dice_coef_loss: 0.2998\n",
      "Epoch 99/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3042 - accuracy: 0.9278 - dice_coef_loss: 0.3042\n",
      "Epoch 99: loss did not improve from 0.26200\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3042 - accuracy: 0.9278 - dice_coef_loss: 0.3042\n",
      "Epoch 100/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3036 - accuracy: 0.9296 - dice_coef_loss: 0.3036\n",
      "Epoch 100: loss did not improve from 0.26200\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3036 - accuracy: 0.9296 - dice_coef_loss: 0.3036\n",
      "Found 2962 images belonging to 1 classes.\n",
      "Found 2962 images belonging to 1 classes.\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcd5d23e5f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcd5d23e5f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.7769 - accuracy: 0.5396 - dice_coef_loss: 0.7769\n",
      "Epoch 1: loss improved from inf to 0.77688, saving model to ./temp/polar_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 263ms/step - loss: 0.7769 - accuracy: 0.5396 - dice_coef_loss: 0.7769\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6390 - accuracy: 0.7878 - dice_coef_loss: 0.6390\n",
      "Epoch 2: loss improved from 0.77688 to 0.63898, saving model to ./temp/polar_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.6390 - accuracy: 0.7878 - dice_coef_loss: 0.6390\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5939 - accuracy: 0.8685 - dice_coef_loss: 0.5939\n",
      "Epoch 3: loss improved from 0.63898 to 0.59395, saving model to ./temp/polar_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.5939 - accuracy: 0.8685 - dice_coef_loss: 0.5939\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5247 - accuracy: 0.8709 - dice_coef_loss: 0.5247\n",
      "Epoch 4: loss improved from 0.59395 to 0.52473, saving model to ./temp/polar_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.5247 - accuracy: 0.8709 - dice_coef_loss: 0.5247\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5100 - accuracy: 0.8811 - dice_coef_loss: 0.5100\n",
      "Epoch 5: loss improved from 0.52473 to 0.51004, saving model to ./temp/polar_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.5100 - accuracy: 0.8811 - dice_coef_loss: 0.5100\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4915 - accuracy: 0.8931 - dice_coef_loss: 0.4915\n",
      "Epoch 6: loss improved from 0.51004 to 0.49151, saving model to ./temp/polar_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.4915 - accuracy: 0.8931 - dice_coef_loss: 0.4915\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4887 - accuracy: 0.8784 - dice_coef_loss: 0.4887\n",
      "Epoch 7: loss improved from 0.49151 to 0.48868, saving model to ./temp/polar_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.4887 - accuracy: 0.8784 - dice_coef_loss: 0.4887\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4847 - accuracy: 0.8854 - dice_coef_loss: 0.4862\n",
      "Epoch 8: loss improved from 0.48868 to 0.48467, saving model to ./temp/polar_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 28s 283ms/step - loss: 0.4847 - accuracy: 0.8854 - dice_coef_loss: 0.4862\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4786 - accuracy: 0.8961 - dice_coef_loss: 0.4786\n",
      "Epoch 9: loss improved from 0.48467 to 0.47865, saving model to ./temp/polar_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 269ms/step - loss: 0.4786 - accuracy: 0.8961 - dice_coef_loss: 0.4786\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4651 - accuracy: 0.8993 - dice_coef_loss: 0.4651\n",
      "Epoch 10: loss improved from 0.47865 to 0.46505, saving model to ./temp/polar_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.4651 - accuracy: 0.8993 - dice_coef_loss: 0.4651\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4390 - accuracy: 0.8953 - dice_coef_loss: 0.4390\n",
      "Epoch 11: loss improved from 0.46505 to 0.43904, saving model to ./temp/polar_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 270ms/step - loss: 0.4390 - accuracy: 0.8953 - dice_coef_loss: 0.4390\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - ETA: 0s - loss: 0.4189 - accuracy: 0.9057 - dice_coef_loss: 0.4189\n",
      "Epoch 12: loss improved from 0.43904 to 0.41887, saving model to ./temp/polar_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 270ms/step - loss: 0.4189 - accuracy: 0.9057 - dice_coef_loss: 0.4189\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4085 - accuracy: 0.9095 - dice_coef_loss: 0.4085\n",
      "Epoch 13: loss improved from 0.41887 to 0.40852, saving model to ./temp/polar_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.4085 - accuracy: 0.9095 - dice_coef_loss: 0.4085\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4237 - accuracy: 0.9068 - dice_coef_loss: 0.4237\n",
      "Epoch 14: loss did not improve from 0.40852\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4237 - accuracy: 0.9068 - dice_coef_loss: 0.4237\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4690 - accuracy: 0.8901 - dice_coef_loss: 0.4693\n",
      "Epoch 15: loss did not improve from 0.40852\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.4690 - accuracy: 0.8901 - dice_coef_loss: 0.4693\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4259 - accuracy: 0.8975 - dice_coef_loss: 0.4259\n",
      "Epoch 16: loss did not improve from 0.40852\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4259 - accuracy: 0.8975 - dice_coef_loss: 0.4259\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4061 - accuracy: 0.9098 - dice_coef_loss: 0.4061\n",
      "Epoch 17: loss improved from 0.40852 to 0.40606, saving model to ./temp/polar_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.4061 - accuracy: 0.9098 - dice_coef_loss: 0.4061\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4314 - accuracy: 0.9124 - dice_coef_loss: 0.4314\n",
      "Epoch 18: loss did not improve from 0.40606\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4314 - accuracy: 0.9124 - dice_coef_loss: 0.4314\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4053 - accuracy: 0.9037 - dice_coef_loss: 0.4053\n",
      "Epoch 19: loss improved from 0.40606 to 0.40533, saving model to ./temp/polar_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.4053 - accuracy: 0.9037 - dice_coef_loss: 0.4053\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3880 - accuracy: 0.9221 - dice_coef_loss: 0.3880\n",
      "Epoch 20: loss improved from 0.40533 to 0.38804, saving model to ./temp/polar_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.3880 - accuracy: 0.9221 - dice_coef_loss: 0.3880\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4266 - accuracy: 0.9052 - dice_coef_loss: 0.4266\n",
      "Epoch 21: loss did not improve from 0.38804\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4266 - accuracy: 0.9052 - dice_coef_loss: 0.4266\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4002 - accuracy: 0.9054 - dice_coef_loss: 0.4002\n",
      "Epoch 22: loss did not improve from 0.38804\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4002 - accuracy: 0.9054 - dice_coef_loss: 0.4002\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3886 - accuracy: 0.9140 - dice_coef_loss: 0.3892\n",
      "Epoch 23: loss did not improve from 0.38804\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3886 - accuracy: 0.9140 - dice_coef_loss: 0.3892\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4108 - accuracy: 0.9120 - dice_coef_loss: 0.4108\n",
      "Epoch 24: loss did not improve from 0.38804\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4108 - accuracy: 0.9120 - dice_coef_loss: 0.4108\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3837 - accuracy: 0.9160 - dice_coef_loss: 0.3837\n",
      "Epoch 25: loss improved from 0.38804 to 0.38374, saving model to ./temp/polar_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.3837 - accuracy: 0.9160 - dice_coef_loss: 0.3837\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3870 - accuracy: 0.9265 - dice_coef_loss: 0.3870\n",
      "Epoch 26: loss did not improve from 0.38374\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3870 - accuracy: 0.9265 - dice_coef_loss: 0.3870\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3901 - accuracy: 0.9151 - dice_coef_loss: 0.3901\n",
      "Epoch 27: loss did not improve from 0.38374\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3901 - accuracy: 0.9151 - dice_coef_loss: 0.3901\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3954 - accuracy: 0.9087 - dice_coef_loss: 0.3954\n",
      "Epoch 28: loss did not improve from 0.38374\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3954 - accuracy: 0.9087 - dice_coef_loss: 0.3954\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3646 - accuracy: 0.9181 - dice_coef_loss: 0.3646\n",
      "Epoch 29: loss improved from 0.38374 to 0.36459, saving model to ./temp/polar_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.3646 - accuracy: 0.9181 - dice_coef_loss: 0.3646\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3941 - accuracy: 0.9173 - dice_coef_loss: 0.3970\n",
      "Epoch 30: loss did not improve from 0.36459\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3941 - accuracy: 0.9173 - dice_coef_loss: 0.3970\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3600 - accuracy: 0.9215 - dice_coef_loss: 0.3600\n",
      "Epoch 31: loss improved from 0.36459 to 0.36002, saving model to ./temp/polar_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.3600 - accuracy: 0.9215 - dice_coef_loss: 0.3600\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3944 - accuracy: 0.9085 - dice_coef_loss: 0.3944\n",
      "Epoch 32: loss did not improve from 0.36002\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3944 - accuracy: 0.9085 - dice_coef_loss: 0.3944\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3549 - accuracy: 0.9239 - dice_coef_loss: 0.3549\n",
      "Epoch 33: loss improved from 0.36002 to 0.35489, saving model to ./temp/polar_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.3549 - accuracy: 0.9239 - dice_coef_loss: 0.3549\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3868 - accuracy: 0.9115 - dice_coef_loss: 0.3868\n",
      "Epoch 34: loss did not improve from 0.35489\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3868 - accuracy: 0.9115 - dice_coef_loss: 0.3868\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3993 - accuracy: 0.9214 - dice_coef_loss: 0.3993\n",
      "Epoch 35: loss did not improve from 0.35489\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3993 - accuracy: 0.9214 - dice_coef_loss: 0.3993\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3655 - accuracy: 0.9165 - dice_coef_loss: 0.3655\n",
      "Epoch 36: loss did not improve from 0.35489\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3655 - accuracy: 0.9165 - dice_coef_loss: 0.3655\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3746 - accuracy: 0.9133 - dice_coef_loss: 0.3746\n",
      "Epoch 37: loss did not improve from 0.35489\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3746 - accuracy: 0.9133 - dice_coef_loss: 0.3746\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3476 - accuracy: 0.9264 - dice_coef_loss: 0.3476\n",
      "Epoch 38: loss improved from 0.35489 to 0.34765, saving model to ./temp/polar_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 0.3476 - accuracy: 0.9264 - dice_coef_loss: 0.3476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3707 - accuracy: 0.9211 - dice_coef_loss: 0.3707\n",
      "Epoch 39: loss did not improve from 0.34765\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3707 - accuracy: 0.9211 - dice_coef_loss: 0.3707\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3676 - accuracy: 0.9185 - dice_coef_loss: 0.3676\n",
      "Epoch 40: loss did not improve from 0.34765\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3676 - accuracy: 0.9185 - dice_coef_loss: 0.3676\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3354 - accuracy: 0.9275 - dice_coef_loss: 0.3354\n",
      "Epoch 41: loss improved from 0.34765 to 0.33536, saving model to ./temp/polar_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.3354 - accuracy: 0.9275 - dice_coef_loss: 0.3354\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3297 - accuracy: 0.9212 - dice_coef_loss: 0.3297\n",
      "Epoch 42: loss improved from 0.33536 to 0.32966, saving model to ./temp/polar_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.3297 - accuracy: 0.9212 - dice_coef_loss: 0.3297\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3315 - accuracy: 0.9232 - dice_coef_loss: 0.3315\n",
      "Epoch 43: loss did not improve from 0.32966\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3315 - accuracy: 0.9232 - dice_coef_loss: 0.3315\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3530 - accuracy: 0.9189 - dice_coef_loss: 0.3530\n",
      "Epoch 44: loss did not improve from 0.32966\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3530 - accuracy: 0.9189 - dice_coef_loss: 0.3530\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3783 - accuracy: 0.9268 - dice_coef_loss: 0.3782\n",
      "Epoch 45: loss did not improve from 0.32966\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3783 - accuracy: 0.9268 - dice_coef_loss: 0.3782\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3568 - accuracy: 0.9157 - dice_coef_loss: 0.3568\n",
      "Epoch 46: loss did not improve from 0.32966\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3568 - accuracy: 0.9157 - dice_coef_loss: 0.3568\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3586 - accuracy: 0.9153 - dice_coef_loss: 0.3586\n",
      "Epoch 47: loss did not improve from 0.32966\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3586 - accuracy: 0.9153 - dice_coef_loss: 0.3586\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3241 - accuracy: 0.9315 - dice_coef_loss: 0.3241\n",
      "Epoch 48: loss improved from 0.32966 to 0.32405, saving model to ./temp/polar_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.3241 - accuracy: 0.9315 - dice_coef_loss: 0.3241\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3195 - accuracy: 0.9308 - dice_coef_loss: 0.3195\n",
      "Epoch 49: loss improved from 0.32405 to 0.31953, saving model to ./temp/polar_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.3195 - accuracy: 0.9308 - dice_coef_loss: 0.3195\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3235 - accuracy: 0.9274 - dice_coef_loss: 0.3235\n",
      "Epoch 50: loss did not improve from 0.31953\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3235 - accuracy: 0.9274 - dice_coef_loss: 0.3235\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3204 - accuracy: 0.9273 - dice_coef_loss: 0.3204\n",
      "Epoch 51: loss did not improve from 0.31953\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3204 - accuracy: 0.9273 - dice_coef_loss: 0.3204\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3346 - accuracy: 0.9244 - dice_coef_loss: 0.3359\n",
      "Epoch 52: loss did not improve from 0.31953\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3346 - accuracy: 0.9244 - dice_coef_loss: 0.3359\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3174 - accuracy: 0.9325 - dice_coef_loss: 0.3174\n",
      "Epoch 53: loss improved from 0.31953 to 0.31736, saving model to ./temp/polar_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.3174 - accuracy: 0.9325 - dice_coef_loss: 0.3174\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3379 - accuracy: 0.9230 - dice_coef_loss: 0.3379\n",
      "Epoch 54: loss did not improve from 0.31736\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3379 - accuracy: 0.9230 - dice_coef_loss: 0.3379\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3515 - accuracy: 0.9190 - dice_coef_loss: 0.3515\n",
      "Epoch 55: loss did not improve from 0.31736\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3515 - accuracy: 0.9190 - dice_coef_loss: 0.3515\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3230 - accuracy: 0.9230 - dice_coef_loss: 0.3230\n",
      "Epoch 56: loss did not improve from 0.31736\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3230 - accuracy: 0.9230 - dice_coef_loss: 0.3230\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3211 - accuracy: 0.9309 - dice_coef_loss: 0.3211\n",
      "Epoch 57: loss did not improve from 0.31736\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3211 - accuracy: 0.9309 - dice_coef_loss: 0.3211\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3456 - accuracy: 0.9214 - dice_coef_loss: 0.3456\n",
      "Epoch 58: loss did not improve from 0.31736\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3456 - accuracy: 0.9214 - dice_coef_loss: 0.3456\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3462 - accuracy: 0.9220 - dice_coef_loss: 0.3462\n",
      "Epoch 59: loss did not improve from 0.31736\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3462 - accuracy: 0.9220 - dice_coef_loss: 0.3462\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3237 - accuracy: 0.9254 - dice_coef_loss: 0.3250\n",
      "Epoch 60: loss did not improve from 0.31736\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3237 - accuracy: 0.9254 - dice_coef_loss: 0.3250\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3136 - accuracy: 0.9343 - dice_coef_loss: 0.3136\n",
      "Epoch 61: loss improved from 0.31736 to 0.31363, saving model to ./temp/polar_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.3136 - accuracy: 0.9343 - dice_coef_loss: 0.3136\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3229 - accuracy: 0.9271 - dice_coef_loss: 0.3229\n",
      "Epoch 62: loss did not improve from 0.31363\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3229 - accuracy: 0.9271 - dice_coef_loss: 0.3229\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3228 - accuracy: 0.9311 - dice_coef_loss: 0.3228\n",
      "Epoch 63: loss did not improve from 0.31363\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3228 - accuracy: 0.9311 - dice_coef_loss: 0.3228\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3530 - accuracy: 0.9214 - dice_coef_loss: 0.3530\n",
      "Epoch 64: loss did not improve from 0.31363\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3530 - accuracy: 0.9214 - dice_coef_loss: 0.3530\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3113 - accuracy: 0.9274 - dice_coef_loss: 0.3113\n",
      "Epoch 65: loss improved from 0.31363 to 0.31134, saving model to ./temp/polar_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.3113 - accuracy: 0.9274 - dice_coef_loss: 0.3113\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3245 - accuracy: 0.9269 - dice_coef_loss: 0.3245\n",
      "Epoch 66: loss did not improve from 0.31134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3245 - accuracy: 0.9269 - dice_coef_loss: 0.3245\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3001 - accuracy: 0.9320 - dice_coef_loss: 0.2995\n",
      "Epoch 67: loss improved from 0.31134 to 0.30014, saving model to ./temp/polar_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.3001 - accuracy: 0.9320 - dice_coef_loss: 0.2995\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3226 - accuracy: 0.9218 - dice_coef_loss: 0.3226\n",
      "Epoch 68: loss did not improve from 0.30014\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3226 - accuracy: 0.9218 - dice_coef_loss: 0.3226\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3316 - accuracy: 0.9269 - dice_coef_loss: 0.3316\n",
      "Epoch 69: loss did not improve from 0.30014\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3316 - accuracy: 0.9269 - dice_coef_loss: 0.3316\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2962 - accuracy: 0.9301 - dice_coef_loss: 0.2962\n",
      "Epoch 70: loss improved from 0.30014 to 0.29619, saving model to ./temp/polar_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.2962 - accuracy: 0.9301 - dice_coef_loss: 0.2962\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2991 - accuracy: 0.9296 - dice_coef_loss: 0.2991\n",
      "Epoch 71: loss did not improve from 0.29619\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.2991 - accuracy: 0.9296 - dice_coef_loss: 0.2991\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3260 - accuracy: 0.9237 - dice_coef_loss: 0.3260\n",
      "Epoch 72: loss did not improve from 0.29619\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3260 - accuracy: 0.9237 - dice_coef_loss: 0.3260\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3000 - accuracy: 0.9358 - dice_coef_loss: 0.3000\n",
      "Epoch 73: loss did not improve from 0.29619\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3000 - accuracy: 0.9358 - dice_coef_loss: 0.3000\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3168 - accuracy: 0.9337 - dice_coef_loss: 0.3168\n",
      "Epoch 74: loss did not improve from 0.29619\n",
      "100/100 [==============================] - 26s 261ms/step - loss: 0.3168 - accuracy: 0.9337 - dice_coef_loss: 0.3168\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3217 - accuracy: 0.9294 - dice_coef_loss: 0.3212\n",
      "Epoch 75: loss did not improve from 0.29619\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3217 - accuracy: 0.9294 - dice_coef_loss: 0.3212\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2855 - accuracy: 0.9370 - dice_coef_loss: 0.2855\n",
      "Epoch 76: loss improved from 0.29619 to 0.28554, saving model to ./temp/polar_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.2855 - accuracy: 0.9370 - dice_coef_loss: 0.2855\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3214 - accuracy: 0.9228 - dice_coef_loss: 0.3214\n",
      "Epoch 77: loss did not improve from 0.28554\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3214 - accuracy: 0.9228 - dice_coef_loss: 0.3214\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3325 - accuracy: 0.9268 - dice_coef_loss: 0.3325\n",
      "Epoch 78: loss did not improve from 0.28554\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3325 - accuracy: 0.9268 - dice_coef_loss: 0.3325\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2999 - accuracy: 0.9359 - dice_coef_loss: 0.2999\n",
      "Epoch 79: loss did not improve from 0.28554\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.2999 - accuracy: 0.9359 - dice_coef_loss: 0.2999\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2971 - accuracy: 0.9327 - dice_coef_loss: 0.2971\n",
      "Epoch 80: loss did not improve from 0.28554\n",
      "100/100 [==============================] - 26s 261ms/step - loss: 0.2971 - accuracy: 0.9327 - dice_coef_loss: 0.2971\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3076 - accuracy: 0.9360 - dice_coef_loss: 0.3076\n",
      "Epoch 81: loss did not improve from 0.28554\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3076 - accuracy: 0.9360 - dice_coef_loss: 0.3076\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3114 - accuracy: 0.9325 - dice_coef_loss: 0.3133\n",
      "Epoch 82: loss did not improve from 0.28554\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3114 - accuracy: 0.9325 - dice_coef_loss: 0.3133\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3060 - accuracy: 0.9294 - dice_coef_loss: 0.3060\n",
      "Epoch 83: loss did not improve from 0.28554\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3060 - accuracy: 0.9294 - dice_coef_loss: 0.3060\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2921 - accuracy: 0.9359 - dice_coef_loss: 0.2921\n",
      "Epoch 84: loss did not improve from 0.28554\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.2921 - accuracy: 0.9359 - dice_coef_loss: 0.2921\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3111 - accuracy: 0.9291 - dice_coef_loss: 0.3111\n",
      "Epoch 85: loss did not improve from 0.28554\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3111 - accuracy: 0.9291 - dice_coef_loss: 0.3111\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3144 - accuracy: 0.9283 - dice_coef_loss: 0.3144\n",
      "Epoch 86: loss did not improve from 0.28554\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3144 - accuracy: 0.9283 - dice_coef_loss: 0.3144\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3177 - accuracy: 0.9337 - dice_coef_loss: 0.3177\n",
      "Epoch 87: loss did not improve from 0.28554\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3177 - accuracy: 0.9337 - dice_coef_loss: 0.3177\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2999 - accuracy: 0.9312 - dice_coef_loss: 0.2999\n",
      "Epoch 88: loss did not improve from 0.28554\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.2999 - accuracy: 0.9312 - dice_coef_loss: 0.2999\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3336 - accuracy: 0.9261 - dice_coef_loss: 0.3332\n",
      "Epoch 89: loss did not improve from 0.28554\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3336 - accuracy: 0.9261 - dice_coef_loss: 0.3332\n",
      "Epoch 90/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2994 - accuracy: 0.9271 - dice_coef_loss: 0.2994\n",
      "Epoch 90: loss did not improve from 0.28554\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.2994 - accuracy: 0.9271 - dice_coef_loss: 0.2994\n",
      "Epoch 91/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2966 - accuracy: 0.9285 - dice_coef_loss: 0.2966\n",
      "Epoch 91: loss did not improve from 0.28554\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.2966 - accuracy: 0.9285 - dice_coef_loss: 0.2966\n",
      "Epoch 92/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3364 - accuracy: 0.9288 - dice_coef_loss: 0.3364\n",
      "Epoch 92: loss did not improve from 0.28554\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3364 - accuracy: 0.9288 - dice_coef_loss: 0.3364\n",
      "Epoch 93/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3375 - accuracy: 0.9358 - dice_coef_loss: 0.3375\n",
      "Epoch 93: loss did not improve from 0.28554\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3375 - accuracy: 0.9358 - dice_coef_loss: 0.3375\n",
      "Epoch 94/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2966 - accuracy: 0.9329 - dice_coef_loss: 0.2966\n",
      "Epoch 94: loss did not improve from 0.28554\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.2966 - accuracy: 0.9329 - dice_coef_loss: 0.2966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2969 - accuracy: 0.9330 - dice_coef_loss: 0.2969\n",
      "Epoch 95: loss did not improve from 0.28554\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.2969 - accuracy: 0.9330 - dice_coef_loss: 0.2969\n",
      "Epoch 96/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2965 - accuracy: 0.9282 - dice_coef_loss: 0.2965\n",
      "Epoch 96: loss did not improve from 0.28554\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.2965 - accuracy: 0.9282 - dice_coef_loss: 0.2965\n",
      "Epoch 97/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2955 - accuracy: 0.9268 - dice_coef_loss: 0.2951\n",
      "Epoch 97: loss did not improve from 0.28554\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.2955 - accuracy: 0.9268 - dice_coef_loss: 0.2951\n",
      "Epoch 98/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2907 - accuracy: 0.9321 - dice_coef_loss: 0.2907\n",
      "Epoch 98: loss did not improve from 0.28554\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.2907 - accuracy: 0.9321 - dice_coef_loss: 0.2907\n",
      "Epoch 99/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2754 - accuracy: 0.9380 - dice_coef_loss: 0.2754\n",
      "Epoch 99: loss improved from 0.28554 to 0.27535, saving model to ./temp/polar_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.2754 - accuracy: 0.9380 - dice_coef_loss: 0.2754\n",
      "Epoch 100/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3157 - accuracy: 0.9262 - dice_coef_loss: 0.3157\n",
      "Epoch 100: loss did not improve from 0.27535\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3157 - accuracy: 0.9262 - dice_coef_loss: 0.3157\n",
      "Found 2962 images belonging to 1 classes.\n",
      "Found 2962 images belonging to 1 classes.\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcd5c54a710> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcd5c54a710> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.7827 - accuracy: 0.8197 - dice_coef_loss: 0.7827\n",
      "Epoch 1: loss improved from inf to 0.78272, saving model to ./temp/polar_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 263ms/step - loss: 0.7827 - accuracy: 0.8197 - dice_coef_loss: 0.7827\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.7186 - accuracy: 0.8195 - dice_coef_loss: 0.7186\n",
      "Epoch 2: loss improved from 0.78272 to 0.71855, saving model to ./temp/polar_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.7186 - accuracy: 0.8195 - dice_coef_loss: 0.7186\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6940 - accuracy: 0.8295 - dice_coef_loss: 0.6940\n",
      "Epoch 3: loss improved from 0.71855 to 0.69402, saving model to ./temp/polar_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.6940 - accuracy: 0.8295 - dice_coef_loss: 0.6940\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6700 - accuracy: 0.8501 - dice_coef_loss: 0.6700\n",
      "Epoch 4: loss improved from 0.69402 to 0.67000, saving model to ./temp/polar_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.6700 - accuracy: 0.8501 - dice_coef_loss: 0.6700\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6684 - accuracy: 0.8648 - dice_coef_loss: 0.6684\n",
      "Epoch 5: loss improved from 0.67000 to 0.66844, saving model to ./temp/polar_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.6684 - accuracy: 0.8648 - dice_coef_loss: 0.6684\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6568 - accuracy: 0.8671 - dice_coef_loss: 0.6568\n",
      "Epoch 6: loss improved from 0.66844 to 0.65685, saving model to ./temp/polar_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.6568 - accuracy: 0.8671 - dice_coef_loss: 0.6568\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6449 - accuracy: 0.8800 - dice_coef_loss: 0.6449\n",
      "Epoch 7: loss improved from 0.65685 to 0.64493, saving model to ./temp/polar_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.6449 - accuracy: 0.8800 - dice_coef_loss: 0.6449\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6314 - accuracy: 0.8888 - dice_coef_loss: 0.6320\n",
      "Epoch 8: loss improved from 0.64493 to 0.63142, saving model to ./temp/polar_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.6314 - accuracy: 0.8888 - dice_coef_loss: 0.6320\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6331 - accuracy: 0.8876 - dice_coef_loss: 0.6331\n",
      "Epoch 9: loss did not improve from 0.63142\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.6331 - accuracy: 0.8876 - dice_coef_loss: 0.6331\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6274 - accuracy: 0.9027 - dice_coef_loss: 0.6274\n",
      "Epoch 10: loss improved from 0.63142 to 0.62740, saving model to ./temp/polar_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 269ms/step - loss: 0.6274 - accuracy: 0.9027 - dice_coef_loss: 0.6274\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6281 - accuracy: 0.8848 - dice_coef_loss: 0.6281\n",
      "Epoch 11: loss did not improve from 0.62740\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.6281 - accuracy: 0.8848 - dice_coef_loss: 0.6281\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6144 - accuracy: 0.8916 - dice_coef_loss: 0.6144\n",
      "Epoch 12: loss improved from 0.62740 to 0.61435, saving model to ./temp/polar_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 270ms/step - loss: 0.6144 - accuracy: 0.8916 - dice_coef_loss: 0.6144\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6113 - accuracy: 0.9007 - dice_coef_loss: 0.6113\n",
      "Epoch 13: loss improved from 0.61435 to 0.61133, saving model to ./temp/polar_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 269ms/step - loss: 0.6113 - accuracy: 0.9007 - dice_coef_loss: 0.6113\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6265 - accuracy: 0.8941 - dice_coef_loss: 0.6265\n",
      "Epoch 14: loss did not improve from 0.61133\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.6265 - accuracy: 0.8941 - dice_coef_loss: 0.6265\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6257 - accuracy: 0.9012 - dice_coef_loss: 0.6259\n",
      "Epoch 15: loss did not improve from 0.61133\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6257 - accuracy: 0.9012 - dice_coef_loss: 0.6259\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5985 - accuracy: 0.9029 - dice_coef_loss: 0.5985\n",
      "Epoch 16: loss improved from 0.61133 to 0.59850, saving model to ./temp/polar_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5985 - accuracy: 0.9029 - dice_coef_loss: 0.5985\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5952 - accuracy: 0.9059 - dice_coef_loss: 0.5952\n",
      "Epoch 17: loss improved from 0.59850 to 0.59515, saving model to ./temp/polar_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5952 - accuracy: 0.9059 - dice_coef_loss: 0.5952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5987 - accuracy: 0.9125 - dice_coef_loss: 0.5987\n",
      "Epoch 18: loss did not improve from 0.59515\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5987 - accuracy: 0.9125 - dice_coef_loss: 0.5987\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5880 - accuracy: 0.9092 - dice_coef_loss: 0.5880\n",
      "Epoch 19: loss improved from 0.59515 to 0.58795, saving model to ./temp/polar_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5880 - accuracy: 0.9092 - dice_coef_loss: 0.5880\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5688 - accuracy: 0.9154 - dice_coef_loss: 0.5688\n",
      "Epoch 20: loss improved from 0.58795 to 0.56884, saving model to ./temp/polar_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.5688 - accuracy: 0.9154 - dice_coef_loss: 0.5688\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5984 - accuracy: 0.8897 - dice_coef_loss: 0.5984\n",
      "Epoch 21: loss did not improve from 0.56884\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5984 - accuracy: 0.8897 - dice_coef_loss: 0.5984\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5809 - accuracy: 0.9193 - dice_coef_loss: 0.5809\n",
      "Epoch 22: loss did not improve from 0.56884\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5809 - accuracy: 0.9193 - dice_coef_loss: 0.5809\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5980 - accuracy: 0.9072 - dice_coef_loss: 0.5977\n",
      "Epoch 23: loss did not improve from 0.56884\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5980 - accuracy: 0.9072 - dice_coef_loss: 0.5977\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5924 - accuracy: 0.9096 - dice_coef_loss: 0.5924\n",
      "Epoch 24: loss did not improve from 0.56884\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5924 - accuracy: 0.9096 - dice_coef_loss: 0.5924\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5797 - accuracy: 0.9096 - dice_coef_loss: 0.5797\n",
      "Epoch 25: loss did not improve from 0.56884\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5797 - accuracy: 0.9096 - dice_coef_loss: 0.5797\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5656 - accuracy: 0.9140 - dice_coef_loss: 0.5656\n",
      "Epoch 26: loss improved from 0.56884 to 0.56560, saving model to ./temp/polar_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5656 - accuracy: 0.9140 - dice_coef_loss: 0.5656\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5611 - accuracy: 0.9161 - dice_coef_loss: 0.5611\n",
      "Epoch 27: loss improved from 0.56560 to 0.56110, saving model to ./temp/polar_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5611 - accuracy: 0.9161 - dice_coef_loss: 0.5611\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5786 - accuracy: 0.9014 - dice_coef_loss: 0.5786\n",
      "Epoch 28: loss did not improve from 0.56110\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5786 - accuracy: 0.9014 - dice_coef_loss: 0.5786\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5587 - accuracy: 0.9188 - dice_coef_loss: 0.5587\n",
      "Epoch 29: loss improved from 0.56110 to 0.55866, saving model to ./temp/polar_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.5587 - accuracy: 0.9188 - dice_coef_loss: 0.5587\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5725 - accuracy: 0.9163 - dice_coef_loss: 0.5732\n",
      "Epoch 30: loss did not improve from 0.55866\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5725 - accuracy: 0.9163 - dice_coef_loss: 0.5732\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5698 - accuracy: 0.9120 - dice_coef_loss: 0.5698\n",
      "Epoch 31: loss did not improve from 0.55866\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5698 - accuracy: 0.9120 - dice_coef_loss: 0.5698\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5637 - accuracy: 0.9247 - dice_coef_loss: 0.5637\n",
      "Epoch 32: loss did not improve from 0.55866\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5637 - accuracy: 0.9247 - dice_coef_loss: 0.5637\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5642 - accuracy: 0.9106 - dice_coef_loss: 0.5642\n",
      "Epoch 33: loss did not improve from 0.55866\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5642 - accuracy: 0.9106 - dice_coef_loss: 0.5642\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5701 - accuracy: 0.9156 - dice_coef_loss: 0.5701\n",
      "Epoch 34: loss did not improve from 0.55866\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5701 - accuracy: 0.9156 - dice_coef_loss: 0.5701\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5938 - accuracy: 0.9019 - dice_coef_loss: 0.5938\n",
      "Epoch 35: loss did not improve from 0.55866\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5938 - accuracy: 0.9019 - dice_coef_loss: 0.5938\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5467 - accuracy: 0.9121 - dice_coef_loss: 0.5467\n",
      "Epoch 36: loss improved from 0.55866 to 0.54674, saving model to ./temp/polar_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.5467 - accuracy: 0.9121 - dice_coef_loss: 0.5467\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5552 - accuracy: 0.9167 - dice_coef_loss: 0.5552\n",
      "Epoch 37: loss did not improve from 0.54674\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5552 - accuracy: 0.9167 - dice_coef_loss: 0.5552\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5573 - accuracy: 0.9150 - dice_coef_loss: 0.5567\n",
      "Epoch 38: loss did not improve from 0.54674\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5573 - accuracy: 0.9150 - dice_coef_loss: 0.5567\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5430 - accuracy: 0.9163 - dice_coef_loss: 0.5430\n",
      "Epoch 39: loss improved from 0.54674 to 0.54298, saving model to ./temp/polar_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5430 - accuracy: 0.9163 - dice_coef_loss: 0.5430\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5572 - accuracy: 0.9149 - dice_coef_loss: 0.5572\n",
      "Epoch 40: loss did not improve from 0.54298\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5572 - accuracy: 0.9149 - dice_coef_loss: 0.5572\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5542 - accuracy: 0.9230 - dice_coef_loss: 0.5542\n",
      "Epoch 41: loss did not improve from 0.54298\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5542 - accuracy: 0.9230 - dice_coef_loss: 0.5542\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5348 - accuracy: 0.9261 - dice_coef_loss: 0.5348\n",
      "Epoch 42: loss improved from 0.54298 to 0.53476, saving model to ./temp/polar_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5348 - accuracy: 0.9261 - dice_coef_loss: 0.5348\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5331 - accuracy: 0.9221 - dice_coef_loss: 0.5331\n",
      "Epoch 43: loss improved from 0.53476 to 0.53310, saving model to ./temp/polar_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.5331 - accuracy: 0.9221 - dice_coef_loss: 0.5331\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5542 - accuracy: 0.9132 - dice_coef_loss: 0.5542\n",
      "Epoch 44: loss did not improve from 0.53310\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5542 - accuracy: 0.9132 - dice_coef_loss: 0.5542\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - ETA: 0s - loss: 0.5453 - accuracy: 0.9288 - dice_coef_loss: 0.5452\n",
      "Epoch 45: loss did not improve from 0.53310\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5453 - accuracy: 0.9288 - dice_coef_loss: 0.5452\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5312 - accuracy: 0.9247 - dice_coef_loss: 0.5312\n",
      "Epoch 46: loss improved from 0.53310 to 0.53121, saving model to ./temp/polar_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5312 - accuracy: 0.9247 - dice_coef_loss: 0.5312\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5378 - accuracy: 0.9170 - dice_coef_loss: 0.5378\n",
      "Epoch 47: loss did not improve from 0.53121\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5378 - accuracy: 0.9170 - dice_coef_loss: 0.5378\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5469 - accuracy: 0.9165 - dice_coef_loss: 0.5469\n",
      "Epoch 48: loss did not improve from 0.53121\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5469 - accuracy: 0.9165 - dice_coef_loss: 0.5469\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5550 - accuracy: 0.9159 - dice_coef_loss: 0.5550\n",
      "Epoch 49: loss did not improve from 0.53121\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5550 - accuracy: 0.9159 - dice_coef_loss: 0.5550\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5388 - accuracy: 0.9170 - dice_coef_loss: 0.5388\n",
      "Epoch 50: loss did not improve from 0.53121\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5388 - accuracy: 0.9170 - dice_coef_loss: 0.5388\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5419 - accuracy: 0.9212 - dice_coef_loss: 0.5419\n",
      "Epoch 51: loss did not improve from 0.53121\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5419 - accuracy: 0.9212 - dice_coef_loss: 0.5419\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5421 - accuracy: 0.9218 - dice_coef_loss: 0.5425\n",
      "Epoch 52: loss did not improve from 0.53121\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5421 - accuracy: 0.9218 - dice_coef_loss: 0.5425\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5404 - accuracy: 0.9198 - dice_coef_loss: 0.5404\n",
      "Epoch 53: loss did not improve from 0.53121\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5404 - accuracy: 0.9198 - dice_coef_loss: 0.5404\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5507 - accuracy: 0.9171 - dice_coef_loss: 0.5507\n",
      "Epoch 54: loss did not improve from 0.53121\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5507 - accuracy: 0.9171 - dice_coef_loss: 0.5507\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5277 - accuracy: 0.9267 - dice_coef_loss: 0.5277\n",
      "Epoch 55: loss improved from 0.53121 to 0.52774, saving model to ./temp/polar_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.5277 - accuracy: 0.9267 - dice_coef_loss: 0.5277\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5175 - accuracy: 0.9308 - dice_coef_loss: 0.5175\n",
      "Epoch 56: loss improved from 0.52774 to 0.51752, saving model to ./temp/polar_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.5175 - accuracy: 0.9308 - dice_coef_loss: 0.5175\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5516 - accuracy: 0.9220 - dice_coef_loss: 0.5516\n",
      "Epoch 57: loss did not improve from 0.51752\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5516 - accuracy: 0.9220 - dice_coef_loss: 0.5516\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5486 - accuracy: 0.9228 - dice_coef_loss: 0.5486\n",
      "Epoch 58: loss did not improve from 0.51752\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5486 - accuracy: 0.9228 - dice_coef_loss: 0.5486\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5211 - accuracy: 0.9275 - dice_coef_loss: 0.5211\n",
      "Epoch 59: loss did not improve from 0.51752\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5211 - accuracy: 0.9275 - dice_coef_loss: 0.5211\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5209 - accuracy: 0.9252 - dice_coef_loss: 0.5206\n",
      "Epoch 60: loss did not improve from 0.51752\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5209 - accuracy: 0.9252 - dice_coef_loss: 0.5206\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5162 - accuracy: 0.9291 - dice_coef_loss: 0.5162\n",
      "Epoch 61: loss improved from 0.51752 to 0.51619, saving model to ./temp/polar_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5162 - accuracy: 0.9291 - dice_coef_loss: 0.5162\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5457 - accuracy: 0.9200 - dice_coef_loss: 0.5457\n",
      "Epoch 62: loss did not improve from 0.51619\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5457 - accuracy: 0.9200 - dice_coef_loss: 0.5457\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5155 - accuracy: 0.9353 - dice_coef_loss: 0.5155\n",
      "Epoch 63: loss improved from 0.51619 to 0.51548, saving model to ./temp/polar_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.5155 - accuracy: 0.9353 - dice_coef_loss: 0.5155\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5283 - accuracy: 0.9261 - dice_coef_loss: 0.5283\n",
      "Epoch 64: loss did not improve from 0.51548\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5283 - accuracy: 0.9261 - dice_coef_loss: 0.5283\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5292 - accuracy: 0.9190 - dice_coef_loss: 0.5292\n",
      "Epoch 65: loss did not improve from 0.51548\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5292 - accuracy: 0.9190 - dice_coef_loss: 0.5292\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5322 - accuracy: 0.9228 - dice_coef_loss: 0.5322\n",
      "Epoch 66: loss did not improve from 0.51548\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5322 - accuracy: 0.9228 - dice_coef_loss: 0.5322\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5118 - accuracy: 0.9227 - dice_coef_loss: 0.5120\n",
      "Epoch 67: loss improved from 0.51548 to 0.51185, saving model to ./temp/polar_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 0.5118 - accuracy: 0.9227 - dice_coef_loss: 0.5120\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5348 - accuracy: 0.9214 - dice_coef_loss: 0.5348\n",
      "Epoch 68: loss did not improve from 0.51185\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5348 - accuracy: 0.9214 - dice_coef_loss: 0.5348\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5252 - accuracy: 0.9221 - dice_coef_loss: 0.5252\n",
      "Epoch 69: loss did not improve from 0.51185\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5252 - accuracy: 0.9221 - dice_coef_loss: 0.5252\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5212 - accuracy: 0.9199 - dice_coef_loss: 0.5212\n",
      "Epoch 70: loss did not improve from 0.51185\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5212 - accuracy: 0.9199 - dice_coef_loss: 0.5212\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5120 - accuracy: 0.9302 - dice_coef_loss: 0.5120\n",
      "Epoch 71: loss did not improve from 0.51185\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5120 - accuracy: 0.9302 - dice_coef_loss: 0.5120\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5201 - accuracy: 0.9298 - dice_coef_loss: 0.5201\n",
      "Epoch 72: loss did not improve from 0.51185\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5201 - accuracy: 0.9298 - dice_coef_loss: 0.5201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5266 - accuracy: 0.9279 - dice_coef_loss: 0.5266\n",
      "Epoch 73: loss did not improve from 0.51185\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5266 - accuracy: 0.9279 - dice_coef_loss: 0.5266\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5120 - accuracy: 0.9308 - dice_coef_loss: 0.5120\n",
      "Epoch 74: loss did not improve from 0.51185\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5120 - accuracy: 0.9308 - dice_coef_loss: 0.5120\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5103 - accuracy: 0.9280 - dice_coef_loss: 0.5108\n",
      "Epoch 75: loss improved from 0.51185 to 0.51029, saving model to ./temp/polar_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 0.5103 - accuracy: 0.9280 - dice_coef_loss: 0.5108\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5151 - accuracy: 0.9295 - dice_coef_loss: 0.5151\n",
      "Epoch 76: loss did not improve from 0.51029\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5151 - accuracy: 0.9295 - dice_coef_loss: 0.5151\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5150 - accuracy: 0.9306 - dice_coef_loss: 0.5150\n",
      "Epoch 77: loss did not improve from 0.51029\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5150 - accuracy: 0.9306 - dice_coef_loss: 0.5150\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5175 - accuracy: 0.9322 - dice_coef_loss: 0.5175\n",
      "Epoch 78: loss did not improve from 0.51029\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5175 - accuracy: 0.9322 - dice_coef_loss: 0.5175\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5107 - accuracy: 0.9208 - dice_coef_loss: 0.5107\n",
      "Epoch 79: loss did not improve from 0.51029\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5107 - accuracy: 0.9208 - dice_coef_loss: 0.5107\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5132 - accuracy: 0.9271 - dice_coef_loss: 0.5132\n",
      "Epoch 80: loss did not improve from 0.51029\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5132 - accuracy: 0.9271 - dice_coef_loss: 0.5132\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5263 - accuracy: 0.9304 - dice_coef_loss: 0.5263\n",
      "Epoch 81: loss did not improve from 0.51029\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5263 - accuracy: 0.9304 - dice_coef_loss: 0.5263\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5222 - accuracy: 0.9238 - dice_coef_loss: 0.5234\n",
      "Epoch 82: loss did not improve from 0.51029\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5222 - accuracy: 0.9238 - dice_coef_loss: 0.5234\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5110 - accuracy: 0.9336 - dice_coef_loss: 0.5110\n",
      "Epoch 83: loss did not improve from 0.51029\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5110 - accuracy: 0.9336 - dice_coef_loss: 0.5110\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4992 - accuracy: 0.9271 - dice_coef_loss: 0.4992\n",
      "Epoch 84: loss improved from 0.51029 to 0.49916, saving model to ./temp/polar_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.4992 - accuracy: 0.9271 - dice_coef_loss: 0.4992\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5199 - accuracy: 0.9244 - dice_coef_loss: 0.5199\n",
      "Epoch 85: loss did not improve from 0.49916\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5199 - accuracy: 0.9244 - dice_coef_loss: 0.5199\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5039 - accuracy: 0.9299 - dice_coef_loss: 0.5039\n",
      "Epoch 86: loss did not improve from 0.49916\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5039 - accuracy: 0.9299 - dice_coef_loss: 0.5039\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5102 - accuracy: 0.9250 - dice_coef_loss: 0.5102\n",
      "Epoch 87: loss did not improve from 0.49916\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5102 - accuracy: 0.9250 - dice_coef_loss: 0.5102\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5119 - accuracy: 0.9269 - dice_coef_loss: 0.5119\n",
      "Epoch 88: loss did not improve from 0.49916\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5119 - accuracy: 0.9269 - dice_coef_loss: 0.5119\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5215 - accuracy: 0.9308 - dice_coef_loss: 0.5208\n",
      "Epoch 89: loss did not improve from 0.49916\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5215 - accuracy: 0.9308 - dice_coef_loss: 0.5208\n",
      "Epoch 90/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4993 - accuracy: 0.9377 - dice_coef_loss: 0.4993\n",
      "Epoch 90: loss did not improve from 0.49916\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4993 - accuracy: 0.9377 - dice_coef_loss: 0.4993\n",
      "Epoch 91/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4970 - accuracy: 0.9369 - dice_coef_loss: 0.4970\n",
      "Epoch 91: loss improved from 0.49916 to 0.49700, saving model to ./temp/polar_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.4970 - accuracy: 0.9369 - dice_coef_loss: 0.4970\n",
      "Epoch 92/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5246 - accuracy: 0.9240 - dice_coef_loss: 0.5246\n",
      "Epoch 92: loss did not improve from 0.49700\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5246 - accuracy: 0.9240 - dice_coef_loss: 0.5246\n",
      "Epoch 93/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5092 - accuracy: 0.9331 - dice_coef_loss: 0.5092\n",
      "Epoch 93: loss did not improve from 0.49700\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5092 - accuracy: 0.9331 - dice_coef_loss: 0.5092\n",
      "Epoch 94/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5076 - accuracy: 0.9264 - dice_coef_loss: 0.5076\n",
      "Epoch 94: loss did not improve from 0.49700\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5076 - accuracy: 0.9264 - dice_coef_loss: 0.5076\n",
      "Epoch 95/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5096 - accuracy: 0.9265 - dice_coef_loss: 0.5096\n",
      "Epoch 95: loss did not improve from 0.49700\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5096 - accuracy: 0.9265 - dice_coef_loss: 0.5096\n",
      "Epoch 96/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5010 - accuracy: 0.9336 - dice_coef_loss: 0.5010\n",
      "Epoch 96: loss did not improve from 0.49700\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5010 - accuracy: 0.9336 - dice_coef_loss: 0.5010\n",
      "Epoch 97/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5100 - accuracy: 0.9351 - dice_coef_loss: 0.5092\n",
      "Epoch 97: loss did not improve from 0.49700\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5100 - accuracy: 0.9351 - dice_coef_loss: 0.5092\n",
      "Epoch 98/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4960 - accuracy: 0.9341 - dice_coef_loss: 0.4960\n",
      "Epoch 98: loss improved from 0.49700 to 0.49601, saving model to ./temp/polar_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.4960 - accuracy: 0.9341 - dice_coef_loss: 0.4960\n",
      "Epoch 99/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5328 - accuracy: 0.9251 - dice_coef_loss: 0.5328\n",
      "Epoch 99: loss did not improve from 0.49601\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5328 - accuracy: 0.9251 - dice_coef_loss: 0.5328\n",
      "Epoch 100/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4945 - accuracy: 0.9295 - dice_coef_loss: 0.4945\n",
      "Epoch 100: loss improved from 0.49601 to 0.49452, saving model to ./temp/polar_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.4945 - accuracy: 0.9295 - dice_coef_loss: 0.4945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2962 images belonging to 1 classes.\n",
      "Found 2962 images belonging to 1 classes.\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcd5c0d7f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcd5c0d7f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.7937 - accuracy: 0.4470 - dice_coef_loss: 0.7937\n",
      "Epoch 1: loss improved from inf to 0.79369, saving model to ./temp/polar_Dom/4/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 263ms/step - loss: 0.7937 - accuracy: 0.4470 - dice_coef_loss: 0.7937\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6566 - accuracy: 0.7884 - dice_coef_loss: 0.6566\n",
      "Epoch 2: loss improved from 0.79369 to 0.65661, saving model to ./temp/polar_Dom/4/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.6566 - accuracy: 0.7884 - dice_coef_loss: 0.6566\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5432 - accuracy: 0.8673 - dice_coef_loss: 0.5432\n",
      "Epoch 3: loss improved from 0.65661 to 0.54321, saving model to ./temp/polar_Dom/4/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.5432 - accuracy: 0.8673 - dice_coef_loss: 0.5432\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5516 - accuracy: 0.8588 - dice_coef_loss: 0.5516\n",
      "Epoch 4: loss did not improve from 0.54321\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5516 - accuracy: 0.8588 - dice_coef_loss: 0.5516\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5081 - accuracy: 0.8632 - dice_coef_loss: 0.5081\n",
      "Epoch 5: loss improved from 0.54321 to 0.50810, saving model to ./temp/polar_Dom/4/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.5081 - accuracy: 0.8632 - dice_coef_loss: 0.5081\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4988 - accuracy: 0.8848 - dice_coef_loss: 0.4988\n",
      "Epoch 6: loss improved from 0.50810 to 0.49879, saving model to ./temp/polar_Dom/4/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.4988 - accuracy: 0.8848 - dice_coef_loss: 0.4988\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4986 - accuracy: 0.8862 - dice_coef_loss: 0.4986\n",
      "Epoch 7: loss improved from 0.49879 to 0.49864, saving model to ./temp/polar_Dom/4/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.4986 - accuracy: 0.8862 - dice_coef_loss: 0.4986\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5028 - accuracy: 0.8737 - dice_coef_loss: 0.5020\n",
      "Epoch 8: loss did not improve from 0.49864\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5028 - accuracy: 0.8737 - dice_coef_loss: 0.5020\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4806 - accuracy: 0.8947 - dice_coef_loss: 0.4806\n",
      "Epoch 9: loss improved from 0.49864 to 0.48063, saving model to ./temp/polar_Dom/4/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 270ms/step - loss: 0.4806 - accuracy: 0.8947 - dice_coef_loss: 0.4806\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4638 - accuracy: 0.8951 - dice_coef_loss: 0.4638\n",
      "Epoch 10: loss improved from 0.48063 to 0.46383, saving model to ./temp/polar_Dom/4/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.4638 - accuracy: 0.8951 - dice_coef_loss: 0.4638\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4877 - accuracy: 0.8853 - dice_coef_loss: 0.4877\n",
      "Epoch 11: loss did not improve from 0.46383\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4877 - accuracy: 0.8853 - dice_coef_loss: 0.4877\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4461 - accuracy: 0.8958 - dice_coef_loss: 0.4461\n",
      "Epoch 12: loss improved from 0.46383 to 0.44614, saving model to ./temp/polar_Dom/4/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 270ms/step - loss: 0.4461 - accuracy: 0.8958 - dice_coef_loss: 0.4461\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4537 - accuracy: 0.9005 - dice_coef_loss: 0.4537\n",
      "Epoch 13: loss did not improve from 0.44614\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4537 - accuracy: 0.9005 - dice_coef_loss: 0.4537\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4337 - accuracy: 0.9106 - dice_coef_loss: 0.4337\n",
      "Epoch 14: loss improved from 0.44614 to 0.43371, saving model to ./temp/polar_Dom/4/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.4337 - accuracy: 0.9106 - dice_coef_loss: 0.4337\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4365 - accuracy: 0.9099 - dice_coef_loss: 0.4362\n",
      "Epoch 15: loss did not improve from 0.43371\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.4365 - accuracy: 0.9099 - dice_coef_loss: 0.4362\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4208 - accuracy: 0.9005 - dice_coef_loss: 0.4208\n",
      "Epoch 16: loss improved from 0.43371 to 0.42078, saving model to ./temp/polar_Dom/4/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.4208 - accuracy: 0.9005 - dice_coef_loss: 0.4208\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4495 - accuracy: 0.8968 - dice_coef_loss: 0.4495\n",
      "Epoch 17: loss did not improve from 0.42078\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4495 - accuracy: 0.8968 - dice_coef_loss: 0.4495\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4345 - accuracy: 0.9148 - dice_coef_loss: 0.4345\n",
      "Epoch 18: loss did not improve from 0.42078\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4345 - accuracy: 0.9148 - dice_coef_loss: 0.4345\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4100 - accuracy: 0.9080 - dice_coef_loss: 0.4100\n",
      "Epoch 19: loss improved from 0.42078 to 0.41000, saving model to ./temp/polar_Dom/4/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.4100 - accuracy: 0.9080 - dice_coef_loss: 0.4100\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3940 - accuracy: 0.9162 - dice_coef_loss: 0.3940\n",
      "Epoch 20: loss improved from 0.41000 to 0.39402, saving model to ./temp/polar_Dom/4/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.3940 - accuracy: 0.9162 - dice_coef_loss: 0.3940\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4459 - accuracy: 0.9032 - dice_coef_loss: 0.4459\n",
      "Epoch 21: loss did not improve from 0.39402\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4459 - accuracy: 0.9032 - dice_coef_loss: 0.4459\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3801 - accuracy: 0.9194 - dice_coef_loss: 0.3801\n",
      "Epoch 22: loss improved from 0.39402 to 0.38010, saving model to ./temp/polar_Dom/4/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.3801 - accuracy: 0.9194 - dice_coef_loss: 0.3801\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4184 - accuracy: 0.9184 - dice_coef_loss: 0.4191\n",
      "Epoch 23: loss did not improve from 0.38010\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.4184 - accuracy: 0.9184 - dice_coef_loss: 0.4191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4095 - accuracy: 0.9063 - dice_coef_loss: 0.4095\n",
      "Epoch 24: loss did not improve from 0.38010\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4095 - accuracy: 0.9063 - dice_coef_loss: 0.4095\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3845 - accuracy: 0.9127 - dice_coef_loss: 0.3845\n",
      "Epoch 25: loss did not improve from 0.38010\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3845 - accuracy: 0.9127 - dice_coef_loss: 0.3845\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3924 - accuracy: 0.9143 - dice_coef_loss: 0.3924\n",
      "Epoch 26: loss did not improve from 0.38010\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3924 - accuracy: 0.9143 - dice_coef_loss: 0.3924\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4069 - accuracy: 0.9087 - dice_coef_loss: 0.4069\n",
      "Epoch 27: loss did not improve from 0.38010\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4069 - accuracy: 0.9087 - dice_coef_loss: 0.4069\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3727 - accuracy: 0.9163 - dice_coef_loss: 0.3727\n",
      "Epoch 28: loss improved from 0.38010 to 0.37267, saving model to ./temp/polar_Dom/4/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.3727 - accuracy: 0.9163 - dice_coef_loss: 0.3727\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3829 - accuracy: 0.9233 - dice_coef_loss: 0.3829\n",
      "Epoch 29: loss did not improve from 0.37267\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3829 - accuracy: 0.9233 - dice_coef_loss: 0.3829\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3862 - accuracy: 0.9114 - dice_coef_loss: 0.3848\n",
      "Epoch 30: loss did not improve from 0.37267\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3862 - accuracy: 0.9114 - dice_coef_loss: 0.3848\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4272 - accuracy: 0.9094 - dice_coef_loss: 0.4272\n",
      "Epoch 31: loss did not improve from 0.37267\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4272 - accuracy: 0.9094 - dice_coef_loss: 0.4272\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3846 - accuracy: 0.9175 - dice_coef_loss: 0.3846\n",
      "Epoch 32: loss did not improve from 0.37267\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3846 - accuracy: 0.9175 - dice_coef_loss: 0.3846\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3793 - accuracy: 0.9068 - dice_coef_loss: 0.3793\n",
      "Epoch 33: loss did not improve from 0.37267\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3793 - accuracy: 0.9068 - dice_coef_loss: 0.3793\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3621 - accuracy: 0.9214 - dice_coef_loss: 0.3621\n",
      "Epoch 34: loss improved from 0.37267 to 0.36213, saving model to ./temp/polar_Dom/4/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.3621 - accuracy: 0.9214 - dice_coef_loss: 0.3621\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3710 - accuracy: 0.9241 - dice_coef_loss: 0.3710\n",
      "Epoch 35: loss did not improve from 0.36213\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3710 - accuracy: 0.9241 - dice_coef_loss: 0.3710\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3330 - accuracy: 0.9266 - dice_coef_loss: 0.3330\n",
      "Epoch 36: loss improved from 0.36213 to 0.33297, saving model to ./temp/polar_Dom/4/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.3330 - accuracy: 0.9266 - dice_coef_loss: 0.3330\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3301 - accuracy: 0.9251 - dice_coef_loss: 0.3301\n",
      "Epoch 37: loss improved from 0.33297 to 0.33005, saving model to ./temp/polar_Dom/4/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.3301 - accuracy: 0.9251 - dice_coef_loss: 0.3301\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3765 - accuracy: 0.9170 - dice_coef_loss: 0.3782\n",
      "Epoch 38: loss did not improve from 0.33005\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3765 - accuracy: 0.9170 - dice_coef_loss: 0.3782\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3296 - accuracy: 0.9299 - dice_coef_loss: 0.3296\n",
      "Epoch 39: loss improved from 0.33005 to 0.32959, saving model to ./temp/polar_Dom/4/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.3296 - accuracy: 0.9299 - dice_coef_loss: 0.3296\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3995 - accuracy: 0.9135 - dice_coef_loss: 0.3995\n",
      "Epoch 40: loss did not improve from 0.32959\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3995 - accuracy: 0.9135 - dice_coef_loss: 0.3995\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3627 - accuracy: 0.9187 - dice_coef_loss: 0.3627\n",
      "Epoch 41: loss did not improve from 0.32959\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3627 - accuracy: 0.9187 - dice_coef_loss: 0.3627\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3395 - accuracy: 0.9222 - dice_coef_loss: 0.3395\n",
      "Epoch 42: loss did not improve from 0.32959\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3395 - accuracy: 0.9222 - dice_coef_loss: 0.3395\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3363 - accuracy: 0.9207 - dice_coef_loss: 0.3363\n",
      "Epoch 43: loss did not improve from 0.32959\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3363 - accuracy: 0.9207 - dice_coef_loss: 0.3363\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3578 - accuracy: 0.9249 - dice_coef_loss: 0.3578\n",
      "Epoch 44: loss did not improve from 0.32959\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3578 - accuracy: 0.9249 - dice_coef_loss: 0.3578\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4169 - accuracy: 0.8995 - dice_coef_loss: 0.4157\n",
      "Epoch 45: loss did not improve from 0.32959\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.4169 - accuracy: 0.8995 - dice_coef_loss: 0.4157\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3599 - accuracy: 0.9120 - dice_coef_loss: 0.3599\n",
      "Epoch 46: loss did not improve from 0.32959\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3599 - accuracy: 0.9120 - dice_coef_loss: 0.3599\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3669 - accuracy: 0.9199 - dice_coef_loss: 0.3669\n",
      "Epoch 47: loss did not improve from 0.32959\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3669 - accuracy: 0.9199 - dice_coef_loss: 0.3669\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3134 - accuracy: 0.9282 - dice_coef_loss: 0.3134\n",
      "Epoch 48: loss improved from 0.32959 to 0.31342, saving model to ./temp/polar_Dom/4/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.3134 - accuracy: 0.9282 - dice_coef_loss: 0.3134\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3313 - accuracy: 0.9265 - dice_coef_loss: 0.3313\n",
      "Epoch 49: loss did not improve from 0.31342\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3313 - accuracy: 0.9265 - dice_coef_loss: 0.3313\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3735 - accuracy: 0.9167 - dice_coef_loss: 0.3735\n",
      "Epoch 50: loss did not improve from 0.31342\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3735 - accuracy: 0.9167 - dice_coef_loss: 0.3735\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3368 - accuracy: 0.9279 - dice_coef_loss: 0.3368\n",
      "Epoch 51: loss did not improve from 0.31342\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3368 - accuracy: 0.9279 - dice_coef_loss: 0.3368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3429 - accuracy: 0.9276 - dice_coef_loss: 0.3440\n",
      "Epoch 52: loss did not improve from 0.31342\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3429 - accuracy: 0.9276 - dice_coef_loss: 0.3440\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3282 - accuracy: 0.9225 - dice_coef_loss: 0.3282\n",
      "Epoch 53: loss did not improve from 0.31342\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3282 - accuracy: 0.9225 - dice_coef_loss: 0.3282\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3419 - accuracy: 0.9227 - dice_coef_loss: 0.3419\n",
      "Epoch 54: loss did not improve from 0.31342\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3419 - accuracy: 0.9227 - dice_coef_loss: 0.3419\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3555 - accuracy: 0.9242 - dice_coef_loss: 0.3555\n",
      "Epoch 55: loss did not improve from 0.31342\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3555 - accuracy: 0.9242 - dice_coef_loss: 0.3555\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3172 - accuracy: 0.9278 - dice_coef_loss: 0.3172\n",
      "Epoch 56: loss did not improve from 0.31342\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3172 - accuracy: 0.9278 - dice_coef_loss: 0.3172\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3379 - accuracy: 0.9293 - dice_coef_loss: 0.3379\n",
      "Epoch 57: loss did not improve from 0.31342\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3379 - accuracy: 0.9293 - dice_coef_loss: 0.3379\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3556 - accuracy: 0.9229 - dice_coef_loss: 0.3556\n",
      "Epoch 58: loss did not improve from 0.31342\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3556 - accuracy: 0.9229 - dice_coef_loss: 0.3556\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3389 - accuracy: 0.9289 - dice_coef_loss: 0.3389\n",
      "Epoch 59: loss did not improve from 0.31342\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3389 - accuracy: 0.9289 - dice_coef_loss: 0.3389\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3512 - accuracy: 0.9279 - dice_coef_loss: 0.3511\n",
      "Epoch 60: loss did not improve from 0.31342\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3512 - accuracy: 0.9279 - dice_coef_loss: 0.3511\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3059 - accuracy: 0.9265 - dice_coef_loss: 0.3059\n",
      "Epoch 61: loss improved from 0.31342 to 0.30587, saving model to ./temp/polar_Dom/4/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.3059 - accuracy: 0.9265 - dice_coef_loss: 0.3059\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3115 - accuracy: 0.9254 - dice_coef_loss: 0.3115\n",
      "Epoch 62: loss did not improve from 0.30587\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3115 - accuracy: 0.9254 - dice_coef_loss: 0.3115\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3298 - accuracy: 0.9294 - dice_coef_loss: 0.3298\n",
      "Epoch 63: loss did not improve from 0.30587\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3298 - accuracy: 0.9294 - dice_coef_loss: 0.3298\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3310 - accuracy: 0.9286 - dice_coef_loss: 0.3310\n",
      "Epoch 64: loss did not improve from 0.30587\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3310 - accuracy: 0.9286 - dice_coef_loss: 0.3310\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3459 - accuracy: 0.9235 - dice_coef_loss: 0.3459\n",
      "Epoch 65: loss did not improve from 0.30587\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3459 - accuracy: 0.9235 - dice_coef_loss: 0.3459\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3262 - accuracy: 0.9261 - dice_coef_loss: 0.3262\n",
      "Epoch 66: loss did not improve from 0.30587\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3262 - accuracy: 0.9261 - dice_coef_loss: 0.3262\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3421 - accuracy: 0.9306 - dice_coef_loss: 0.3416\n",
      "Epoch 67: loss did not improve from 0.30587\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3421 - accuracy: 0.9306 - dice_coef_loss: 0.3416\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3468 - accuracy: 0.9178 - dice_coef_loss: 0.3468\n",
      "Epoch 68: loss did not improve from 0.30587\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3468 - accuracy: 0.9178 - dice_coef_loss: 0.3468\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3140 - accuracy: 0.9292 - dice_coef_loss: 0.3140\n",
      "Epoch 69: loss did not improve from 0.30587\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3140 - accuracy: 0.9292 - dice_coef_loss: 0.3140\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3222 - accuracy: 0.9316 - dice_coef_loss: 0.3222\n",
      "Epoch 70: loss did not improve from 0.30587\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3222 - accuracy: 0.9316 - dice_coef_loss: 0.3222\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3058 - accuracy: 0.9284 - dice_coef_loss: 0.3058\n",
      "Epoch 71: loss improved from 0.30587 to 0.30575, saving model to ./temp/polar_Dom/4/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.3058 - accuracy: 0.9284 - dice_coef_loss: 0.3058\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2927 - accuracy: 0.9330 - dice_coef_loss: 0.2927\n",
      "Epoch 72: loss improved from 0.30575 to 0.29273, saving model to ./temp/polar_Dom/4/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.2927 - accuracy: 0.9330 - dice_coef_loss: 0.2927\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3109 - accuracy: 0.9341 - dice_coef_loss: 0.3109\n",
      "Epoch 73: loss did not improve from 0.29273\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3109 - accuracy: 0.9341 - dice_coef_loss: 0.3109\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3285 - accuracy: 0.9243 - dice_coef_loss: 0.3285\n",
      "Epoch 74: loss did not improve from 0.29273\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3285 - accuracy: 0.9243 - dice_coef_loss: 0.3285\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3125 - accuracy: 0.9308 - dice_coef_loss: 0.3129\n",
      "Epoch 75: loss did not improve from 0.29273\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3125 - accuracy: 0.9308 - dice_coef_loss: 0.3129\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3167 - accuracy: 0.9239 - dice_coef_loss: 0.3167\n",
      "Epoch 76: loss did not improve from 0.29273\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3167 - accuracy: 0.9239 - dice_coef_loss: 0.3167\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2865 - accuracy: 0.9389 - dice_coef_loss: 0.2865\n",
      "Epoch 77: loss improved from 0.29273 to 0.28653, saving model to ./temp/polar_Dom/4/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.2865 - accuracy: 0.9389 - dice_coef_loss: 0.2865\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3311 - accuracy: 0.9304 - dice_coef_loss: 0.3311\n",
      "Epoch 78: loss did not improve from 0.28653\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3311 - accuracy: 0.9304 - dice_coef_loss: 0.3311\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3035 - accuracy: 0.9328 - dice_coef_loss: 0.3035\n",
      "Epoch 79: loss did not improve from 0.28653\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3035 - accuracy: 0.9328 - dice_coef_loss: 0.3035\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - ETA: 0s - loss: 0.2994 - accuracy: 0.9348 - dice_coef_loss: 0.2994\n",
      "Epoch 80: loss did not improve from 0.28653\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.2994 - accuracy: 0.9348 - dice_coef_loss: 0.2994\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3624 - accuracy: 0.9257 - dice_coef_loss: 0.3624\n",
      "Epoch 81: loss did not improve from 0.28653\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3624 - accuracy: 0.9257 - dice_coef_loss: 0.3624\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3088 - accuracy: 0.9288 - dice_coef_loss: 0.3086\n",
      "Epoch 82: loss did not improve from 0.28653\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3088 - accuracy: 0.9288 - dice_coef_loss: 0.3086\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3035 - accuracy: 0.9296 - dice_coef_loss: 0.3035\n",
      "Epoch 83: loss did not improve from 0.28653\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3035 - accuracy: 0.9296 - dice_coef_loss: 0.3035\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3086 - accuracy: 0.9361 - dice_coef_loss: 0.3086\n",
      "Epoch 84: loss did not improve from 0.28653\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3086 - accuracy: 0.9361 - dice_coef_loss: 0.3086\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3241 - accuracy: 0.9291 - dice_coef_loss: 0.3241\n",
      "Epoch 85: loss did not improve from 0.28653\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3241 - accuracy: 0.9291 - dice_coef_loss: 0.3241\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3071 - accuracy: 0.9263 - dice_coef_loss: 0.3071\n",
      "Epoch 86: loss did not improve from 0.28653\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3071 - accuracy: 0.9263 - dice_coef_loss: 0.3071\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3065 - accuracy: 0.9256 - dice_coef_loss: 0.3065\n",
      "Epoch 87: loss did not improve from 0.28653\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3065 - accuracy: 0.9256 - dice_coef_loss: 0.3065\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2740 - accuracy: 0.9370 - dice_coef_loss: 0.2740\n",
      "Epoch 88: loss improved from 0.28653 to 0.27402, saving model to ./temp/polar_Dom/4/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.2740 - accuracy: 0.9370 - dice_coef_loss: 0.2740\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3240 - accuracy: 0.9358 - dice_coef_loss: 0.3232\n",
      "Epoch 89: loss did not improve from 0.27402\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.3240 - accuracy: 0.9358 - dice_coef_loss: 0.3232\n",
      "Epoch 90/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3156 - accuracy: 0.9351 - dice_coef_loss: 0.3156\n",
      "Epoch 90: loss did not improve from 0.27402\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3156 - accuracy: 0.9351 - dice_coef_loss: 0.3156\n",
      "Epoch 91/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3097 - accuracy: 0.9277 - dice_coef_loss: 0.3097\n",
      "Epoch 91: loss did not improve from 0.27402\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3097 - accuracy: 0.9277 - dice_coef_loss: 0.3097\n",
      "Epoch 92/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2833 - accuracy: 0.9334 - dice_coef_loss: 0.2833\n",
      "Epoch 92: loss did not improve from 0.27402\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.2833 - accuracy: 0.9334 - dice_coef_loss: 0.2833\n",
      "Epoch 93/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2879 - accuracy: 0.9342 - dice_coef_loss: 0.2879\n",
      "Epoch 93: loss did not improve from 0.27402\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.2879 - accuracy: 0.9342 - dice_coef_loss: 0.2879\n",
      "Epoch 94/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2895 - accuracy: 0.9372 - dice_coef_loss: 0.2895\n",
      "Epoch 94: loss did not improve from 0.27402\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.2895 - accuracy: 0.9372 - dice_coef_loss: 0.2895\n",
      "Epoch 95/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3178 - accuracy: 0.9242 - dice_coef_loss: 0.3178\n",
      "Epoch 95: loss did not improve from 0.27402\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3178 - accuracy: 0.9242 - dice_coef_loss: 0.3178\n",
      "Epoch 96/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3012 - accuracy: 0.9361 - dice_coef_loss: 0.3012\n",
      "Epoch 96: loss did not improve from 0.27402\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3012 - accuracy: 0.9361 - dice_coef_loss: 0.3012\n",
      "Epoch 97/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2857 - accuracy: 0.9363 - dice_coef_loss: 0.2858\n",
      "Epoch 97: loss did not improve from 0.27402\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.2857 - accuracy: 0.9363 - dice_coef_loss: 0.2858\n",
      "Epoch 98/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3054 - accuracy: 0.9330 - dice_coef_loss: 0.3054\n",
      "Epoch 98: loss did not improve from 0.27402\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3054 - accuracy: 0.9330 - dice_coef_loss: 0.3054\n",
      "Epoch 99/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2798 - accuracy: 0.9377 - dice_coef_loss: 0.2798\n",
      "Epoch 99: loss did not improve from 0.27402\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.2798 - accuracy: 0.9377 - dice_coef_loss: 0.2798\n",
      "Epoch 100/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2968 - accuracy: 0.9283 - dice_coef_loss: 0.2968\n",
      "Epoch 100: loss did not improve from 0.27402\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.2968 - accuracy: 0.9283 - dice_coef_loss: 0.2968\n",
      "Found 2961 images belonging to 1 classes.\n",
      "Found 2961 images belonging to 1 classes.\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcd5433db00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcd5433db00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.7030 - accuracy: 0.7509 - dice_coef_loss: 0.7030\n",
      "Epoch 1: loss improved from inf to 0.70301, saving model to ./temp/cartesian_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 262ms/step - loss: 0.7030 - accuracy: 0.7509 - dice_coef_loss: 0.7030\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6339 - accuracy: 0.7981 - dice_coef_loss: 0.6339\n",
      "Epoch 2: loss improved from 0.70301 to 0.63392, saving model to ./temp/cartesian_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.6339 - accuracy: 0.7981 - dice_coef_loss: 0.6339\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6124 - accuracy: 0.8312 - dice_coef_loss: 0.6124\n",
      "Epoch 3: loss improved from 0.63392 to 0.61240, saving model to ./temp/cartesian_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.6124 - accuracy: 0.8312 - dice_coef_loss: 0.6124\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5911 - accuracy: 0.8453 - dice_coef_loss: 0.5911\n",
      "Epoch 4: loss improved from 0.61240 to 0.59108, saving model to ./temp/cartesian_Dom/0/checkpoint.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5911 - accuracy: 0.8453 - dice_coef_loss: 0.5911\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5805 - accuracy: 0.8465 - dice_coef_loss: 0.5805\n",
      "Epoch 5: loss improved from 0.59108 to 0.58054, saving model to ./temp/cartesian_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5805 - accuracy: 0.8465 - dice_coef_loss: 0.5805\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5804 - accuracy: 0.8502 - dice_coef_loss: 0.5804\n",
      "Epoch 6: loss improved from 0.58054 to 0.58040, saving model to ./temp/cartesian_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5804 - accuracy: 0.8502 - dice_coef_loss: 0.5804\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5728 - accuracy: 0.8551 - dice_coef_loss: 0.5728\n",
      "Epoch 7: loss improved from 0.58040 to 0.57283, saving model to ./temp/cartesian_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5728 - accuracy: 0.8551 - dice_coef_loss: 0.5728\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5712 - accuracy: 0.8590 - dice_coef_loss: 0.5745\n",
      "Epoch 8: loss improved from 0.57283 to 0.57124, saving model to ./temp/cartesian_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5712 - accuracy: 0.8590 - dice_coef_loss: 0.5745\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5481 - accuracy: 0.8745 - dice_coef_loss: 0.5481\n",
      "Epoch 9: loss improved from 0.57124 to 0.54806, saving model to ./temp/cartesian_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 269ms/step - loss: 0.5481 - accuracy: 0.8745 - dice_coef_loss: 0.5481\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5670 - accuracy: 0.8648 - dice_coef_loss: 0.5670\n",
      "Epoch 10: loss did not improve from 0.54806\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5670 - accuracy: 0.8648 - dice_coef_loss: 0.5670\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5486 - accuracy: 0.8783 - dice_coef_loss: 0.5486\n",
      "Epoch 11: loss did not improve from 0.54806\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5486 - accuracy: 0.8783 - dice_coef_loss: 0.5486\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5391 - accuracy: 0.8727 - dice_coef_loss: 0.5391\n",
      "Epoch 12: loss improved from 0.54806 to 0.53914, saving model to ./temp/cartesian_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5391 - accuracy: 0.8727 - dice_coef_loss: 0.5391\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5416 - accuracy: 0.8768 - dice_coef_loss: 0.5416\n",
      "Epoch 13: loss did not improve from 0.53914\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5416 - accuracy: 0.8768 - dice_coef_loss: 0.5416\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5627 - accuracy: 0.8702 - dice_coef_loss: 0.5627\n",
      "Epoch 14: loss did not improve from 0.53914\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5627 - accuracy: 0.8702 - dice_coef_loss: 0.5627\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5501 - accuracy: 0.8765 - dice_coef_loss: 0.5496\n",
      "Epoch 15: loss did not improve from 0.53914\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.5501 - accuracy: 0.8765 - dice_coef_loss: 0.5496\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5360 - accuracy: 0.8872 - dice_coef_loss: 0.5360\n",
      "Epoch 16: loss improved from 0.53914 to 0.53600, saving model to ./temp/cartesian_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5360 - accuracy: 0.8872 - dice_coef_loss: 0.5360\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5316 - accuracy: 0.8816 - dice_coef_loss: 0.5316\n",
      "Epoch 17: loss improved from 0.53600 to 0.53162, saving model to ./temp/cartesian_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5316 - accuracy: 0.8816 - dice_coef_loss: 0.5316\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5313 - accuracy: 0.8874 - dice_coef_loss: 0.5313\n",
      "Epoch 18: loss improved from 0.53162 to 0.53129, saving model to ./temp/cartesian_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5313 - accuracy: 0.8874 - dice_coef_loss: 0.5313\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5321 - accuracy: 0.8859 - dice_coef_loss: 0.5321\n",
      "Epoch 19: loss did not improve from 0.53129\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5321 - accuracy: 0.8859 - dice_coef_loss: 0.5321\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5280 - accuracy: 0.8893 - dice_coef_loss: 0.5280\n",
      "Epoch 20: loss improved from 0.53129 to 0.52802, saving model to ./temp/cartesian_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5280 - accuracy: 0.8893 - dice_coef_loss: 0.5280\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5245 - accuracy: 0.8945 - dice_coef_loss: 0.5245\n",
      "Epoch 21: loss improved from 0.52802 to 0.52455, saving model to ./temp/cartesian_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5245 - accuracy: 0.8945 - dice_coef_loss: 0.5245\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5156 - accuracy: 0.8909 - dice_coef_loss: 0.5156\n",
      "Epoch 22: loss improved from 0.52455 to 0.51556, saving model to ./temp/cartesian_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5156 - accuracy: 0.8909 - dice_coef_loss: 0.5156\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5182 - accuracy: 0.8982 - dice_coef_loss: 0.5218\n",
      "Epoch 23: loss did not improve from 0.51556\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.5182 - accuracy: 0.8982 - dice_coef_loss: 0.5218\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5072 - accuracy: 0.8990 - dice_coef_loss: 0.5072\n",
      "Epoch 24: loss improved from 0.51556 to 0.50719, saving model to ./temp/cartesian_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5072 - accuracy: 0.8990 - dice_coef_loss: 0.5072\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5166 - accuracy: 0.9012 - dice_coef_loss: 0.5166\n",
      "Epoch 25: loss did not improve from 0.50719\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5166 - accuracy: 0.9012 - dice_coef_loss: 0.5166\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5341 - accuracy: 0.8902 - dice_coef_loss: 0.5341\n",
      "Epoch 26: loss did not improve from 0.50719\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5341 - accuracy: 0.8902 - dice_coef_loss: 0.5341\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5046 - accuracy: 0.8995 - dice_coef_loss: 0.5046\n",
      "Epoch 27: loss improved from 0.50719 to 0.50459, saving model to ./temp/cartesian_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5046 - accuracy: 0.8995 - dice_coef_loss: 0.5046\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5001 - accuracy: 0.9077 - dice_coef_loss: 0.5001\n",
      "Epoch 28: loss improved from 0.50459 to 0.50008, saving model to ./temp/cartesian_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5001 - accuracy: 0.9077 - dice_coef_loss: 0.5001\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5166 - accuracy: 0.8934 - dice_coef_loss: 0.5166\n",
      "Epoch 29: loss did not improve from 0.50008\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5166 - accuracy: 0.8934 - dice_coef_loss: 0.5166\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - ETA: 0s - loss: 0.4977 - accuracy: 0.9037 - dice_coef_loss: 0.4980\n",
      "Epoch 30: loss improved from 0.50008 to 0.49775, saving model to ./temp/cartesian_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 0.4977 - accuracy: 0.9037 - dice_coef_loss: 0.4980\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5144 - accuracy: 0.8979 - dice_coef_loss: 0.5144\n",
      "Epoch 31: loss did not improve from 0.49775\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5144 - accuracy: 0.8979 - dice_coef_loss: 0.5144\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4965 - accuracy: 0.9028 - dice_coef_loss: 0.4965\n",
      "Epoch 32: loss improved from 0.49775 to 0.49650, saving model to ./temp/cartesian_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.4965 - accuracy: 0.9028 - dice_coef_loss: 0.4965\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5012 - accuracy: 0.9072 - dice_coef_loss: 0.5012\n",
      "Epoch 33: loss did not improve from 0.49650\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5012 - accuracy: 0.9072 - dice_coef_loss: 0.5012\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5002 - accuracy: 0.9013 - dice_coef_loss: 0.5002\n",
      "Epoch 34: loss did not improve from 0.49650\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5002 - accuracy: 0.9013 - dice_coef_loss: 0.5002\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4973 - accuracy: 0.9112 - dice_coef_loss: 0.4973\n",
      "Epoch 35: loss did not improve from 0.49650\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.4973 - accuracy: 0.9112 - dice_coef_loss: 0.4973\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5019 - accuracy: 0.9055 - dice_coef_loss: 0.5019\n",
      "Epoch 36: loss did not improve from 0.49650\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.5019 - accuracy: 0.9055 - dice_coef_loss: 0.5019\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4804 - accuracy: 0.9096 - dice_coef_loss: 0.4804\n",
      "Epoch 37: loss improved from 0.49650 to 0.48038, saving model to ./temp/cartesian_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.4804 - accuracy: 0.9096 - dice_coef_loss: 0.4804\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4766 - accuracy: 0.9204 - dice_coef_loss: 0.4805\n",
      "Epoch 38: loss improved from 0.48038 to 0.47659, saving model to ./temp/cartesian_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 265ms/step - loss: 0.4766 - accuracy: 0.9204 - dice_coef_loss: 0.4805\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4869 - accuracy: 0.9086 - dice_coef_loss: 0.4869\n",
      "Epoch 39: loss did not improve from 0.47659\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.4869 - accuracy: 0.9086 - dice_coef_loss: 0.4869\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4754 - accuracy: 0.9131 - dice_coef_loss: 0.4754\n",
      "Epoch 40: loss improved from 0.47659 to 0.47539, saving model to ./temp/cartesian_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.4754 - accuracy: 0.9131 - dice_coef_loss: 0.4754\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4853 - accuracy: 0.9082 - dice_coef_loss: 0.4853\n",
      "Epoch 41: loss did not improve from 0.47539\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.4853 - accuracy: 0.9082 - dice_coef_loss: 0.4853\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4807 - accuracy: 0.9094 - dice_coef_loss: 0.4807\n",
      "Epoch 42: loss did not improve from 0.47539\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.4807 - accuracy: 0.9094 - dice_coef_loss: 0.4807\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4712 - accuracy: 0.9186 - dice_coef_loss: 0.4712\n",
      "Epoch 43: loss improved from 0.47539 to 0.47116, saving model to ./temp/cartesian_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.4712 - accuracy: 0.9186 - dice_coef_loss: 0.4712\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4886 - accuracy: 0.9148 - dice_coef_loss: 0.4886\n",
      "Epoch 44: loss did not improve from 0.47116\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.4886 - accuracy: 0.9148 - dice_coef_loss: 0.4886\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4813 - accuracy: 0.9171 - dice_coef_loss: 0.4804\n",
      "Epoch 45: loss did not improve from 0.47116\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.4813 - accuracy: 0.9171 - dice_coef_loss: 0.4804\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4677 - accuracy: 0.9205 - dice_coef_loss: 0.4677\n",
      "Epoch 46: loss improved from 0.47116 to 0.46770, saving model to ./temp/cartesian_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.4677 - accuracy: 0.9205 - dice_coef_loss: 0.4677\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4714 - accuracy: 0.9155 - dice_coef_loss: 0.4714\n",
      "Epoch 47: loss did not improve from 0.46770\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4714 - accuracy: 0.9155 - dice_coef_loss: 0.4714\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4728 - accuracy: 0.9191 - dice_coef_loss: 0.4728\n",
      "Epoch 48: loss did not improve from 0.46770\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.4728 - accuracy: 0.9191 - dice_coef_loss: 0.4728\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4806 - accuracy: 0.9173 - dice_coef_loss: 0.4806\n",
      "Epoch 49: loss did not improve from 0.46770\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4806 - accuracy: 0.9173 - dice_coef_loss: 0.4806\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4543 - accuracy: 0.9236 - dice_coef_loss: 0.4543\n",
      "Epoch 50: loss improved from 0.46770 to 0.45431, saving model to ./temp/cartesian_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.4543 - accuracy: 0.9236 - dice_coef_loss: 0.4543\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4653 - accuracy: 0.9188 - dice_coef_loss: 0.4653\n",
      "Epoch 51: loss did not improve from 0.45431\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.4653 - accuracy: 0.9188 - dice_coef_loss: 0.4653\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4854 - accuracy: 0.9205 - dice_coef_loss: 0.4856\n",
      "Epoch 52: loss did not improve from 0.45431\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.4854 - accuracy: 0.9205 - dice_coef_loss: 0.4856\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4631 - accuracy: 0.9222 - dice_coef_loss: 0.4631\n",
      "Epoch 53: loss did not improve from 0.45431\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4631 - accuracy: 0.9222 - dice_coef_loss: 0.4631\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4657 - accuracy: 0.9219 - dice_coef_loss: 0.4657\n",
      "Epoch 54: loss did not improve from 0.45431\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.4657 - accuracy: 0.9219 - dice_coef_loss: 0.4657\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4682 - accuracy: 0.9209 - dice_coef_loss: 0.4682\n",
      "Epoch 55: loss did not improve from 0.45431\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4682 - accuracy: 0.9209 - dice_coef_loss: 0.4682\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4574 - accuracy: 0.9221 - dice_coef_loss: 0.4574\n",
      "Epoch 56: loss did not improve from 0.45431\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4574 - accuracy: 0.9221 - dice_coef_loss: 0.4574\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - ETA: 0s - loss: 0.4589 - accuracy: 0.9243 - dice_coef_loss: 0.4589\n",
      "Epoch 57: loss did not improve from 0.45431\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.4589 - accuracy: 0.9243 - dice_coef_loss: 0.4589\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4814 - accuracy: 0.9123 - dice_coef_loss: 0.4814\n",
      "Epoch 58: loss did not improve from 0.45431\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.4814 - accuracy: 0.9123 - dice_coef_loss: 0.4814\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4561 - accuracy: 0.9221 - dice_coef_loss: 0.4561\n",
      "Epoch 59: loss did not improve from 0.45431\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4561 - accuracy: 0.9221 - dice_coef_loss: 0.4561\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4439 - accuracy: 0.9277 - dice_coef_loss: 0.4444\n",
      "Epoch 60: loss improved from 0.45431 to 0.44391, saving model to ./temp/cartesian_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 265ms/step - loss: 0.4439 - accuracy: 0.9277 - dice_coef_loss: 0.4444\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4623 - accuracy: 0.9254 - dice_coef_loss: 0.4623\n",
      "Epoch 61: loss did not improve from 0.44391\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4623 - accuracy: 0.9254 - dice_coef_loss: 0.4623\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4729 - accuracy: 0.9088 - dice_coef_loss: 0.4729\n",
      "Epoch 62: loss did not improve from 0.44391\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.4729 - accuracy: 0.9088 - dice_coef_loss: 0.4729\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4705 - accuracy: 0.9205 - dice_coef_loss: 0.4705\n",
      "Epoch 63: loss did not improve from 0.44391\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4705 - accuracy: 0.9205 - dice_coef_loss: 0.4705\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4598 - accuracy: 0.9250 - dice_coef_loss: 0.4598\n",
      "Epoch 64: loss did not improve from 0.44391\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.4598 - accuracy: 0.9250 - dice_coef_loss: 0.4598\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4475 - accuracy: 0.9323 - dice_coef_loss: 0.4475\n",
      "Epoch 65: loss did not improve from 0.44391\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.4475 - accuracy: 0.9323 - dice_coef_loss: 0.4475\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4473 - accuracy: 0.9290 - dice_coef_loss: 0.4473\n",
      "Epoch 66: loss did not improve from 0.44391\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4473 - accuracy: 0.9290 - dice_coef_loss: 0.4473\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4447 - accuracy: 0.9293 - dice_coef_loss: 0.4442\n",
      "Epoch 67: loss did not improve from 0.44391\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.4447 - accuracy: 0.9293 - dice_coef_loss: 0.4442\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4583 - accuracy: 0.9269 - dice_coef_loss: 0.4583\n",
      "Epoch 68: loss did not improve from 0.44391\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4583 - accuracy: 0.9269 - dice_coef_loss: 0.4583\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4659 - accuracy: 0.9207 - dice_coef_loss: 0.4659\n",
      "Epoch 69: loss did not improve from 0.44391\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.4659 - accuracy: 0.9207 - dice_coef_loss: 0.4659\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4591 - accuracy: 0.9270 - dice_coef_loss: 0.4591\n",
      "Epoch 70: loss did not improve from 0.44391\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4591 - accuracy: 0.9270 - dice_coef_loss: 0.4591\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4514 - accuracy: 0.9342 - dice_coef_loss: 0.4514\n",
      "Epoch 71: loss did not improve from 0.44391\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.4514 - accuracy: 0.9342 - dice_coef_loss: 0.4514\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4499 - accuracy: 0.9202 - dice_coef_loss: 0.4499\n",
      "Epoch 72: loss did not improve from 0.44391\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4499 - accuracy: 0.9202 - dice_coef_loss: 0.4499\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4478 - accuracy: 0.9291 - dice_coef_loss: 0.4478\n",
      "Epoch 73: loss did not improve from 0.44391\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.4478 - accuracy: 0.9291 - dice_coef_loss: 0.4478\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4349 - accuracy: 0.9313 - dice_coef_loss: 0.4349\n",
      "Epoch 74: loss improved from 0.44391 to 0.43486, saving model to ./temp/cartesian_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.4349 - accuracy: 0.9313 - dice_coef_loss: 0.4349\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4430 - accuracy: 0.9326 - dice_coef_loss: 0.4436\n",
      "Epoch 75: loss did not improve from 0.43486\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.4430 - accuracy: 0.9326 - dice_coef_loss: 0.4436\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4516 - accuracy: 0.9227 - dice_coef_loss: 0.4516\n",
      "Epoch 76: loss did not improve from 0.43486\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.4516 - accuracy: 0.9227 - dice_coef_loss: 0.4516\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4298 - accuracy: 0.9352 - dice_coef_loss: 0.4298\n",
      "Epoch 77: loss improved from 0.43486 to 0.42977, saving model to ./temp/cartesian_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.4298 - accuracy: 0.9352 - dice_coef_loss: 0.4298\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4386 - accuracy: 0.9312 - dice_coef_loss: 0.4386\n",
      "Epoch 78: loss did not improve from 0.42977\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.4386 - accuracy: 0.9312 - dice_coef_loss: 0.4386\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4495 - accuracy: 0.9308 - dice_coef_loss: 0.4495\n",
      "Epoch 79: loss did not improve from 0.42977\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.4495 - accuracy: 0.9308 - dice_coef_loss: 0.4495\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4529 - accuracy: 0.9238 - dice_coef_loss: 0.4529\n",
      "Epoch 80: loss did not improve from 0.42977\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4529 - accuracy: 0.9238 - dice_coef_loss: 0.4529\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4453 - accuracy: 0.9316 - dice_coef_loss: 0.4453\n",
      "Epoch 81: loss did not improve from 0.42977\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.4453 - accuracy: 0.9316 - dice_coef_loss: 0.4453\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4362 - accuracy: 0.9342 - dice_coef_loss: 0.4368\n",
      "Epoch 82: loss did not improve from 0.42977\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.4362 - accuracy: 0.9342 - dice_coef_loss: 0.4368\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4375 - accuracy: 0.9355 - dice_coef_loss: 0.4375\n",
      "Epoch 83: loss did not improve from 0.42977\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.4375 - accuracy: 0.9355 - dice_coef_loss: 0.4375\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4478 - accuracy: 0.9354 - dice_coef_loss: 0.4478\n",
      "Epoch 84: loss did not improve from 0.42977\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4478 - accuracy: 0.9354 - dice_coef_loss: 0.4478\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - ETA: 0s - loss: 0.4328 - accuracy: 0.9336 - dice_coef_loss: 0.4328\n",
      "Epoch 85: loss did not improve from 0.42977\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4328 - accuracy: 0.9336 - dice_coef_loss: 0.4328\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4266 - accuracy: 0.9367 - dice_coef_loss: 0.4266\n",
      "Epoch 86: loss improved from 0.42977 to 0.42658, saving model to ./temp/cartesian_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.4266 - accuracy: 0.9367 - dice_coef_loss: 0.4266\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4205 - accuracy: 0.9372 - dice_coef_loss: 0.4205\n",
      "Epoch 87: loss improved from 0.42658 to 0.42046, saving model to ./temp/cartesian_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.4205 - accuracy: 0.9372 - dice_coef_loss: 0.4205\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4471 - accuracy: 0.9300 - dice_coef_loss: 0.4471\n",
      "Epoch 88: loss did not improve from 0.42046\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4471 - accuracy: 0.9300 - dice_coef_loss: 0.4471\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4472 - accuracy: 0.9228 - dice_coef_loss: 0.4473\n",
      "Epoch 89: loss did not improve from 0.42046\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.4472 - accuracy: 0.9228 - dice_coef_loss: 0.4473\n",
      "Epoch 90/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4471 - accuracy: 0.9325 - dice_coef_loss: 0.4471\n",
      "Epoch 90: loss did not improve from 0.42046\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4471 - accuracy: 0.9325 - dice_coef_loss: 0.4471\n",
      "Epoch 91/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4219 - accuracy: 0.9428 - dice_coef_loss: 0.4219\n",
      "Epoch 91: loss did not improve from 0.42046\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.4219 - accuracy: 0.9428 - dice_coef_loss: 0.4219\n",
      "Epoch 92/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4435 - accuracy: 0.9374 - dice_coef_loss: 0.4435\n",
      "Epoch 92: loss did not improve from 0.42046\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4435 - accuracy: 0.9374 - dice_coef_loss: 0.4435\n",
      "Epoch 93/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4128 - accuracy: 0.9399 - dice_coef_loss: 0.4128\n",
      "Epoch 93: loss improved from 0.42046 to 0.41280, saving model to ./temp/cartesian_Dom/0/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.4128 - accuracy: 0.9399 - dice_coef_loss: 0.4128\n",
      "Epoch 94/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4202 - accuracy: 0.9396 - dice_coef_loss: 0.4202\n",
      "Epoch 94: loss did not improve from 0.41280\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.4202 - accuracy: 0.9396 - dice_coef_loss: 0.4202\n",
      "Epoch 95/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4383 - accuracy: 0.9348 - dice_coef_loss: 0.4383\n",
      "Epoch 95: loss did not improve from 0.41280\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.4383 - accuracy: 0.9348 - dice_coef_loss: 0.4383\n",
      "Epoch 96/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4216 - accuracy: 0.9384 - dice_coef_loss: 0.4216\n",
      "Epoch 96: loss did not improve from 0.41280\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.4216 - accuracy: 0.9384 - dice_coef_loss: 0.4216\n",
      "Epoch 97/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4225 - accuracy: 0.9405 - dice_coef_loss: 0.4218\n",
      "Epoch 97: loss did not improve from 0.41280\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.4225 - accuracy: 0.9405 - dice_coef_loss: 0.4218\n",
      "Epoch 98/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4246 - accuracy: 0.9423 - dice_coef_loss: 0.4246\n",
      "Epoch 98: loss did not improve from 0.41280\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.4246 - accuracy: 0.9423 - dice_coef_loss: 0.4246\n",
      "Epoch 99/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4277 - accuracy: 0.9350 - dice_coef_loss: 0.4277\n",
      "Epoch 99: loss did not improve from 0.41280\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4277 - accuracy: 0.9350 - dice_coef_loss: 0.4277\n",
      "Epoch 100/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4245 - accuracy: 0.9372 - dice_coef_loss: 0.4245\n",
      "Epoch 100: loss did not improve from 0.41280\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.4245 - accuracy: 0.9372 - dice_coef_loss: 0.4245\n",
      "Found 2961 images belonging to 1 classes.\n",
      "Found 2961 images belonging to 1 classes.\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcd30336d40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcd30336d40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.7091 - accuracy: 0.7367 - dice_coef_loss: 0.7091\n",
      "Epoch 1: loss improved from inf to 0.70907, saving model to ./temp/cartesian_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 263ms/step - loss: 0.7091 - accuracy: 0.7367 - dice_coef_loss: 0.7091\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6367 - accuracy: 0.7889 - dice_coef_loss: 0.6367\n",
      "Epoch 2: loss improved from 0.70907 to 0.63667, saving model to ./temp/cartesian_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.6367 - accuracy: 0.7889 - dice_coef_loss: 0.6367\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6020 - accuracy: 0.8242 - dice_coef_loss: 0.6020\n",
      "Epoch 3: loss improved from 0.63667 to 0.60197, saving model to ./temp/cartesian_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.6020 - accuracy: 0.8242 - dice_coef_loss: 0.6020\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5880 - accuracy: 0.8459 - dice_coef_loss: 0.5880\n",
      "Epoch 4: loss improved from 0.60197 to 0.58797, saving model to ./temp/cartesian_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5880 - accuracy: 0.8459 - dice_coef_loss: 0.5880\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5950 - accuracy: 0.8318 - dice_coef_loss: 0.5950\n",
      "Epoch 5: loss did not improve from 0.58797\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5950 - accuracy: 0.8318 - dice_coef_loss: 0.5950\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6045 - accuracy: 0.8477 - dice_coef_loss: 0.6045\n",
      "Epoch 6: loss did not improve from 0.58797\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.6045 - accuracy: 0.8477 - dice_coef_loss: 0.6045\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5533 - accuracy: 0.8592 - dice_coef_loss: 0.5533\n",
      "Epoch 7: loss improved from 0.58797 to 0.55329, saving model to ./temp/cartesian_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5533 - accuracy: 0.8592 - dice_coef_loss: 0.5533\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5756 - accuracy: 0.8612 - dice_coef_loss: 0.5788\n",
      "Epoch 8: loss did not improve from 0.55329\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.5756 - accuracy: 0.8612 - dice_coef_loss: 0.5788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5569 - accuracy: 0.8677 - dice_coef_loss: 0.5569\n",
      "Epoch 9: loss did not improve from 0.55329\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5569 - accuracy: 0.8677 - dice_coef_loss: 0.5569\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5563 - accuracy: 0.8778 - dice_coef_loss: 0.5563\n",
      "Epoch 10: loss did not improve from 0.55329\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5563 - accuracy: 0.8778 - dice_coef_loss: 0.5563\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5397 - accuracy: 0.8700 - dice_coef_loss: 0.5397\n",
      "Epoch 11: loss improved from 0.55329 to 0.53966, saving model to ./temp/cartesian_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5397 - accuracy: 0.8700 - dice_coef_loss: 0.5397\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5816 - accuracy: 0.8662 - dice_coef_loss: 0.5816\n",
      "Epoch 12: loss did not improve from 0.53966\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5816 - accuracy: 0.8662 - dice_coef_loss: 0.5816\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5502 - accuracy: 0.8744 - dice_coef_loss: 0.5502\n",
      "Epoch 13: loss did not improve from 0.53966\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5502 - accuracy: 0.8744 - dice_coef_loss: 0.5502\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5480 - accuracy: 0.8732 - dice_coef_loss: 0.5480\n",
      "Epoch 14: loss did not improve from 0.53966\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5480 - accuracy: 0.8732 - dice_coef_loss: 0.5480\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5625 - accuracy: 0.8682 - dice_coef_loss: 0.5625\n",
      "Epoch 15: loss did not improve from 0.53966\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.5625 - accuracy: 0.8682 - dice_coef_loss: 0.5625\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5299 - accuracy: 0.8845 - dice_coef_loss: 0.5299\n",
      "Epoch 16: loss improved from 0.53966 to 0.52988, saving model to ./temp/cartesian_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.5299 - accuracy: 0.8845 - dice_coef_loss: 0.5299\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5399 - accuracy: 0.8854 - dice_coef_loss: 0.5399\n",
      "Epoch 17: loss did not improve from 0.52988\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5399 - accuracy: 0.8854 - dice_coef_loss: 0.5399\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5302 - accuracy: 0.8881 - dice_coef_loss: 0.5302\n",
      "Epoch 18: loss did not improve from 0.52988\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5302 - accuracy: 0.8881 - dice_coef_loss: 0.5302\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5271 - accuracy: 0.8860 - dice_coef_loss: 0.5271\n",
      "Epoch 19: loss improved from 0.52988 to 0.52710, saving model to ./temp/cartesian_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.5271 - accuracy: 0.8860 - dice_coef_loss: 0.5271\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5340 - accuracy: 0.8794 - dice_coef_loss: 0.5340\n",
      "Epoch 20: loss did not improve from 0.52710\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5340 - accuracy: 0.8794 - dice_coef_loss: 0.5340\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5288 - accuracy: 0.8881 - dice_coef_loss: 0.5288\n",
      "Epoch 21: loss did not improve from 0.52710\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5288 - accuracy: 0.8881 - dice_coef_loss: 0.5288\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5145 - accuracy: 0.8963 - dice_coef_loss: 0.5145\n",
      "Epoch 22: loss improved from 0.52710 to 0.51449, saving model to ./temp/cartesian_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.5145 - accuracy: 0.8963 - dice_coef_loss: 0.5145\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5106 - accuracy: 0.8922 - dice_coef_loss: 0.5096\n",
      "Epoch 23: loss improved from 0.51449 to 0.51060, saving model to ./temp/cartesian_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 0.5106 - accuracy: 0.8922 - dice_coef_loss: 0.5096\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5203 - accuracy: 0.9002 - dice_coef_loss: 0.5203\n",
      "Epoch 24: loss did not improve from 0.51060\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5203 - accuracy: 0.9002 - dice_coef_loss: 0.5203\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5206 - accuracy: 0.8912 - dice_coef_loss: 0.5206\n",
      "Epoch 25: loss did not improve from 0.51060\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5206 - accuracy: 0.8912 - dice_coef_loss: 0.5206\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5260 - accuracy: 0.8973 - dice_coef_loss: 0.5260\n",
      "Epoch 26: loss did not improve from 0.51060\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5260 - accuracy: 0.8973 - dice_coef_loss: 0.5260\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5223 - accuracy: 0.8908 - dice_coef_loss: 0.5223\n",
      "Epoch 27: loss did not improve from 0.51060\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5223 - accuracy: 0.8908 - dice_coef_loss: 0.5223\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5138 - accuracy: 0.9048 - dice_coef_loss: 0.5138\n",
      "Epoch 28: loss did not improve from 0.51060\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5138 - accuracy: 0.9048 - dice_coef_loss: 0.5138\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4927 - accuracy: 0.9083 - dice_coef_loss: 0.4927\n",
      "Epoch 29: loss improved from 0.51060 to 0.49271, saving model to ./temp/cartesian_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.4927 - accuracy: 0.9083 - dice_coef_loss: 0.4927\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5197 - accuracy: 0.8914 - dice_coef_loss: 0.5185\n",
      "Epoch 30: loss did not improve from 0.49271\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.5197 - accuracy: 0.8914 - dice_coef_loss: 0.5185\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5075 - accuracy: 0.9079 - dice_coef_loss: 0.5075\n",
      "Epoch 31: loss did not improve from 0.49271\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5075 - accuracy: 0.9079 - dice_coef_loss: 0.5075\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5078 - accuracy: 0.9018 - dice_coef_loss: 0.5078\n",
      "Epoch 32: loss did not improve from 0.49271\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5078 - accuracy: 0.9018 - dice_coef_loss: 0.5078\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4923 - accuracy: 0.9086 - dice_coef_loss: 0.4923\n",
      "Epoch 33: loss improved from 0.49271 to 0.49228, saving model to ./temp/cartesian_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.4923 - accuracy: 0.9086 - dice_coef_loss: 0.4923\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5122 - accuracy: 0.9031 - dice_coef_loss: 0.5122\n",
      "Epoch 34: loss did not improve from 0.49228\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5122 - accuracy: 0.9031 - dice_coef_loss: 0.5122\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4998 - accuracy: 0.9017 - dice_coef_loss: 0.4998\n",
      "Epoch 35: loss did not improve from 0.49228\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4998 - accuracy: 0.9017 - dice_coef_loss: 0.4998\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - ETA: 0s - loss: 0.4898 - accuracy: 0.9022 - dice_coef_loss: 0.4898\n",
      "Epoch 36: loss improved from 0.49228 to 0.48982, saving model to ./temp/cartesian_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.4898 - accuracy: 0.9022 - dice_coef_loss: 0.4898\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4919 - accuracy: 0.9092 - dice_coef_loss: 0.4919\n",
      "Epoch 37: loss did not improve from 0.48982\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4919 - accuracy: 0.9092 - dice_coef_loss: 0.4919\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4869 - accuracy: 0.9096 - dice_coef_loss: 0.4883\n",
      "Epoch 38: loss improved from 0.48982 to 0.48686, saving model to ./temp/cartesian_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 0.4869 - accuracy: 0.9096 - dice_coef_loss: 0.4883\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4999 - accuracy: 0.9111 - dice_coef_loss: 0.4999\n",
      "Epoch 39: loss did not improve from 0.48686\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4999 - accuracy: 0.9111 - dice_coef_loss: 0.4999\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5079 - accuracy: 0.8990 - dice_coef_loss: 0.5079\n",
      "Epoch 40: loss did not improve from 0.48686\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5079 - accuracy: 0.8990 - dice_coef_loss: 0.5079\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4776 - accuracy: 0.9132 - dice_coef_loss: 0.4776\n",
      "Epoch 41: loss improved from 0.48686 to 0.47765, saving model to ./temp/cartesian_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.4776 - accuracy: 0.9132 - dice_coef_loss: 0.4776\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4749 - accuracy: 0.9143 - dice_coef_loss: 0.4749\n",
      "Epoch 42: loss improved from 0.47765 to 0.47494, saving model to ./temp/cartesian_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.4749 - accuracy: 0.9143 - dice_coef_loss: 0.4749\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4956 - accuracy: 0.9049 - dice_coef_loss: 0.4956\n",
      "Epoch 43: loss did not improve from 0.47494\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4956 - accuracy: 0.9049 - dice_coef_loss: 0.4956\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4899 - accuracy: 0.9123 - dice_coef_loss: 0.4899\n",
      "Epoch 44: loss did not improve from 0.47494\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4899 - accuracy: 0.9123 - dice_coef_loss: 0.4899\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4797 - accuracy: 0.9099 - dice_coef_loss: 0.4791\n",
      "Epoch 45: loss did not improve from 0.47494\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.4797 - accuracy: 0.9099 - dice_coef_loss: 0.4791\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4832 - accuracy: 0.9173 - dice_coef_loss: 0.4832\n",
      "Epoch 46: loss did not improve from 0.47494\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4832 - accuracy: 0.9173 - dice_coef_loss: 0.4832\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4739 - accuracy: 0.9163 - dice_coef_loss: 0.4739\n",
      "Epoch 47: loss improved from 0.47494 to 0.47386, saving model to ./temp/cartesian_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.4739 - accuracy: 0.9163 - dice_coef_loss: 0.4739\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4808 - accuracy: 0.9084 - dice_coef_loss: 0.4808\n",
      "Epoch 48: loss did not improve from 0.47386\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4808 - accuracy: 0.9084 - dice_coef_loss: 0.4808\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4756 - accuracy: 0.9202 - dice_coef_loss: 0.4756\n",
      "Epoch 49: loss did not improve from 0.47386\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4756 - accuracy: 0.9202 - dice_coef_loss: 0.4756\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4887 - accuracy: 0.9088 - dice_coef_loss: 0.4887\n",
      "Epoch 50: loss did not improve from 0.47386\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4887 - accuracy: 0.9088 - dice_coef_loss: 0.4887\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4751 - accuracy: 0.9154 - dice_coef_loss: 0.4751\n",
      "Epoch 51: loss did not improve from 0.47386\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4751 - accuracy: 0.9154 - dice_coef_loss: 0.4751\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4702 - accuracy: 0.9185 - dice_coef_loss: 0.4698\n",
      "Epoch 52: loss improved from 0.47386 to 0.47020, saving model to ./temp/cartesian_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 0.4702 - accuracy: 0.9185 - dice_coef_loss: 0.4698\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4686 - accuracy: 0.9149 - dice_coef_loss: 0.4686\n",
      "Epoch 53: loss improved from 0.47020 to 0.46863, saving model to ./temp/cartesian_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.4686 - accuracy: 0.9149 - dice_coef_loss: 0.4686\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4599 - accuracy: 0.9238 - dice_coef_loss: 0.4599\n",
      "Epoch 54: loss improved from 0.46863 to 0.45988, saving model to ./temp/cartesian_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.4599 - accuracy: 0.9238 - dice_coef_loss: 0.4599\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4843 - accuracy: 0.9161 - dice_coef_loss: 0.4843\n",
      "Epoch 55: loss did not improve from 0.45988\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4843 - accuracy: 0.9161 - dice_coef_loss: 0.4843\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4655 - accuracy: 0.9168 - dice_coef_loss: 0.4655\n",
      "Epoch 56: loss did not improve from 0.45988\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4655 - accuracy: 0.9168 - dice_coef_loss: 0.4655\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4796 - accuracy: 0.9170 - dice_coef_loss: 0.4796\n",
      "Epoch 57: loss did not improve from 0.45988\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4796 - accuracy: 0.9170 - dice_coef_loss: 0.4796\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4588 - accuracy: 0.9236 - dice_coef_loss: 0.4588\n",
      "Epoch 58: loss improved from 0.45988 to 0.45882, saving model to ./temp/cartesian_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.4588 - accuracy: 0.9236 - dice_coef_loss: 0.4588\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4711 - accuracy: 0.9165 - dice_coef_loss: 0.4711\n",
      "Epoch 59: loss did not improve from 0.45882\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4711 - accuracy: 0.9165 - dice_coef_loss: 0.4711\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4583 - accuracy: 0.9251 - dice_coef_loss: 0.4586\n",
      "Epoch 60: loss improved from 0.45882 to 0.45834, saving model to ./temp/cartesian_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 0.4583 - accuracy: 0.9251 - dice_coef_loss: 0.4586\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4567 - accuracy: 0.9227 - dice_coef_loss: 0.4567\n",
      "Epoch 61: loss improved from 0.45834 to 0.45673, saving model to ./temp/cartesian_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.4567 - accuracy: 0.9227 - dice_coef_loss: 0.4567\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4578 - accuracy: 0.9269 - dice_coef_loss: 0.4578\n",
      "Epoch 62: loss did not improve from 0.45673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4578 - accuracy: 0.9269 - dice_coef_loss: 0.4578\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4726 - accuracy: 0.9139 - dice_coef_loss: 0.4726\n",
      "Epoch 63: loss did not improve from 0.45673\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4726 - accuracy: 0.9139 - dice_coef_loss: 0.4726\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4477 - accuracy: 0.9332 - dice_coef_loss: 0.4477\n",
      "Epoch 64: loss improved from 0.45673 to 0.44768, saving model to ./temp/cartesian_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.4477 - accuracy: 0.9332 - dice_coef_loss: 0.4477\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4674 - accuracy: 0.9289 - dice_coef_loss: 0.4674\n",
      "Epoch 65: loss did not improve from 0.44768\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4674 - accuracy: 0.9289 - dice_coef_loss: 0.4674\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4701 - accuracy: 0.9223 - dice_coef_loss: 0.4701\n",
      "Epoch 66: loss did not improve from 0.44768\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4701 - accuracy: 0.9223 - dice_coef_loss: 0.4701\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4463 - accuracy: 0.9268 - dice_coef_loss: 0.4457\n",
      "Epoch 67: loss improved from 0.44768 to 0.44633, saving model to ./temp/cartesian_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 0.4463 - accuracy: 0.9268 - dice_coef_loss: 0.4457\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4504 - accuracy: 0.9245 - dice_coef_loss: 0.4504\n",
      "Epoch 68: loss did not improve from 0.44633\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4504 - accuracy: 0.9245 - dice_coef_loss: 0.4504\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4631 - accuracy: 0.9234 - dice_coef_loss: 0.4631\n",
      "Epoch 69: loss did not improve from 0.44633\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4631 - accuracy: 0.9234 - dice_coef_loss: 0.4631\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4542 - accuracy: 0.9282 - dice_coef_loss: 0.4542\n",
      "Epoch 70: loss did not improve from 0.44633\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4542 - accuracy: 0.9282 - dice_coef_loss: 0.4542\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4542 - accuracy: 0.9242 - dice_coef_loss: 0.4542\n",
      "Epoch 71: loss did not improve from 0.44633\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4542 - accuracy: 0.9242 - dice_coef_loss: 0.4542\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4413 - accuracy: 0.9308 - dice_coef_loss: 0.4413\n",
      "Epoch 72: loss improved from 0.44633 to 0.44129, saving model to ./temp/cartesian_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.4413 - accuracy: 0.9308 - dice_coef_loss: 0.4413\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4608 - accuracy: 0.9254 - dice_coef_loss: 0.4608\n",
      "Epoch 73: loss did not improve from 0.44129\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4608 - accuracy: 0.9254 - dice_coef_loss: 0.4608\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4416 - accuracy: 0.9311 - dice_coef_loss: 0.4416\n",
      "Epoch 74: loss did not improve from 0.44129\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4416 - accuracy: 0.9311 - dice_coef_loss: 0.4416\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4535 - accuracy: 0.9333 - dice_coef_loss: 0.4552\n",
      "Epoch 75: loss did not improve from 0.44129\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.4535 - accuracy: 0.9333 - dice_coef_loss: 0.4552\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4539 - accuracy: 0.9203 - dice_coef_loss: 0.4539\n",
      "Epoch 76: loss did not improve from 0.44129\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4539 - accuracy: 0.9203 - dice_coef_loss: 0.4539\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4444 - accuracy: 0.9270 - dice_coef_loss: 0.4444\n",
      "Epoch 77: loss did not improve from 0.44129\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4444 - accuracy: 0.9270 - dice_coef_loss: 0.4444\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4547 - accuracy: 0.9259 - dice_coef_loss: 0.4547\n",
      "Epoch 78: loss did not improve from 0.44129\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4547 - accuracy: 0.9259 - dice_coef_loss: 0.4547\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4366 - accuracy: 0.9342 - dice_coef_loss: 0.4366\n",
      "Epoch 79: loss improved from 0.44129 to 0.43656, saving model to ./temp/cartesian_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.4366 - accuracy: 0.9342 - dice_coef_loss: 0.4366\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4437 - accuracy: 0.9335 - dice_coef_loss: 0.4437\n",
      "Epoch 80: loss did not improve from 0.43656\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4437 - accuracy: 0.9335 - dice_coef_loss: 0.4437\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4379 - accuracy: 0.9309 - dice_coef_loss: 0.4379\n",
      "Epoch 81: loss did not improve from 0.43656\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4379 - accuracy: 0.9309 - dice_coef_loss: 0.4379\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4482 - accuracy: 0.9331 - dice_coef_loss: 0.4474\n",
      "Epoch 82: loss did not improve from 0.43656\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.4482 - accuracy: 0.9331 - dice_coef_loss: 0.4474\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4516 - accuracy: 0.9288 - dice_coef_loss: 0.4516\n",
      "Epoch 83: loss did not improve from 0.43656\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4516 - accuracy: 0.9288 - dice_coef_loss: 0.4516\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4417 - accuracy: 0.9310 - dice_coef_loss: 0.4417\n",
      "Epoch 84: loss did not improve from 0.43656\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.4417 - accuracy: 0.9310 - dice_coef_loss: 0.4417\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4284 - accuracy: 0.9370 - dice_coef_loss: 0.4284\n",
      "Epoch 85: loss improved from 0.43656 to 0.42844, saving model to ./temp/cartesian_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.4284 - accuracy: 0.9370 - dice_coef_loss: 0.4284\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4356 - accuracy: 0.9354 - dice_coef_loss: 0.4356\n",
      "Epoch 86: loss did not improve from 0.42844\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4356 - accuracy: 0.9354 - dice_coef_loss: 0.4356\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4381 - accuracy: 0.9312 - dice_coef_loss: 0.4381\n",
      "Epoch 87: loss did not improve from 0.42844\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4381 - accuracy: 0.9312 - dice_coef_loss: 0.4381\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4319 - accuracy: 0.9398 - dice_coef_loss: 0.4319\n",
      "Epoch 88: loss did not improve from 0.42844\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4319 - accuracy: 0.9398 - dice_coef_loss: 0.4319\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4579 - accuracy: 0.9058 - dice_coef_loss: 0.4595\n",
      "Epoch 89: loss did not improve from 0.42844\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.4579 - accuracy: 0.9058 - dice_coef_loss: 0.4595\n",
      "Epoch 90/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - ETA: 0s - loss: 0.4948 - accuracy: 0.8945 - dice_coef_loss: 0.4948\n",
      "Epoch 90: loss did not improve from 0.42844\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4948 - accuracy: 0.8945 - dice_coef_loss: 0.4948\n",
      "Epoch 91/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4576 - accuracy: 0.9219 - dice_coef_loss: 0.4576\n",
      "Epoch 91: loss did not improve from 0.42844\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4576 - accuracy: 0.9219 - dice_coef_loss: 0.4576\n",
      "Epoch 92/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4224 - accuracy: 0.9379 - dice_coef_loss: 0.4224\n",
      "Epoch 92: loss improved from 0.42844 to 0.42237, saving model to ./temp/cartesian_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.4224 - accuracy: 0.9379 - dice_coef_loss: 0.4224\n",
      "Epoch 93/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4314 - accuracy: 0.9381 - dice_coef_loss: 0.4314\n",
      "Epoch 93: loss did not improve from 0.42237\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4314 - accuracy: 0.9381 - dice_coef_loss: 0.4314\n",
      "Epoch 94/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4357 - accuracy: 0.9337 - dice_coef_loss: 0.4357\n",
      "Epoch 94: loss did not improve from 0.42237\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4357 - accuracy: 0.9337 - dice_coef_loss: 0.4357\n",
      "Epoch 95/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4318 - accuracy: 0.9356 - dice_coef_loss: 0.4318\n",
      "Epoch 95: loss did not improve from 0.42237\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4318 - accuracy: 0.9356 - dice_coef_loss: 0.4318\n",
      "Epoch 96/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4294 - accuracy: 0.9372 - dice_coef_loss: 0.4294\n",
      "Epoch 96: loss did not improve from 0.42237\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4294 - accuracy: 0.9372 - dice_coef_loss: 0.4294\n",
      "Epoch 97/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4276 - accuracy: 0.9378 - dice_coef_loss: 0.4279\n",
      "Epoch 97: loss did not improve from 0.42237\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.4276 - accuracy: 0.9378 - dice_coef_loss: 0.4279\n",
      "Epoch 98/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4307 - accuracy: 0.9339 - dice_coef_loss: 0.4307\n",
      "Epoch 98: loss did not improve from 0.42237\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4307 - accuracy: 0.9339 - dice_coef_loss: 0.4307\n",
      "Epoch 99/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4137 - accuracy: 0.9413 - dice_coef_loss: 0.4137\n",
      "Epoch 99: loss improved from 0.42237 to 0.41373, saving model to ./temp/cartesian_Dom/1/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.4137 - accuracy: 0.9413 - dice_coef_loss: 0.4137\n",
      "Epoch 100/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4270 - accuracy: 0.9373 - dice_coef_loss: 0.4270\n",
      "Epoch 100: loss did not improve from 0.41373\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4270 - accuracy: 0.9373 - dice_coef_loss: 0.4270\n",
      "Found 2962 images belonging to 1 classes.\n",
      "Found 2962 images belonging to 1 classes.\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcd206385f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcd206385f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.7178 - accuracy: 0.3398 - dice_coef_loss: 0.7178\n",
      "Epoch 1: loss improved from inf to 0.71780, saving model to ./temp/cartesian_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 262ms/step - loss: 0.7178 - accuracy: 0.3398 - dice_coef_loss: 0.7178\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6904 - accuracy: 0.6899 - dice_coef_loss: 0.6904\n",
      "Epoch 2: loss improved from 0.71780 to 0.69039, saving model to ./temp/cartesian_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 0.6904 - accuracy: 0.6899 - dice_coef_loss: 0.6904\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6992 - accuracy: 0.6923 - dice_coef_loss: 0.6992\n",
      "Epoch 3: loss did not improve from 0.69039\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6992 - accuracy: 0.6923 - dice_coef_loss: 0.6992\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6647 - accuracy: 0.7691 - dice_coef_loss: 0.6647\n",
      "Epoch 4: loss improved from 0.69039 to 0.66474, saving model to ./temp/cartesian_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.6647 - accuracy: 0.7691 - dice_coef_loss: 0.6647\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6723 - accuracy: 0.7470 - dice_coef_loss: 0.6723\n",
      "Epoch 5: loss did not improve from 0.66474\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6723 - accuracy: 0.7470 - dice_coef_loss: 0.6723\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6686 - accuracy: 0.7816 - dice_coef_loss: 0.6686\n",
      "Epoch 6: loss did not improve from 0.66474\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6686 - accuracy: 0.7816 - dice_coef_loss: 0.6686\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6663 - accuracy: 0.7842 - dice_coef_loss: 0.6663\n",
      "Epoch 7: loss did not improve from 0.66474\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6663 - accuracy: 0.7842 - dice_coef_loss: 0.6663\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6786 - accuracy: 0.8015 - dice_coef_loss: 0.6795\n",
      "Epoch 8: loss did not improve from 0.66474\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6786 - accuracy: 0.8015 - dice_coef_loss: 0.6795\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6629 - accuracy: 0.8124 - dice_coef_loss: 0.6629\n",
      "Epoch 9: loss improved from 0.66474 to 0.66288, saving model to ./temp/cartesian_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.6629 - accuracy: 0.8124 - dice_coef_loss: 0.6629\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6693 - accuracy: 0.7876 - dice_coef_loss: 0.6693\n",
      "Epoch 10: loss did not improve from 0.66288\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6693 - accuracy: 0.7876 - dice_coef_loss: 0.6693\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6523 - accuracy: 0.8183 - dice_coef_loss: 0.6523\n",
      "Epoch 11: loss improved from 0.66288 to 0.65233, saving model to ./temp/cartesian_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 0.6523 - accuracy: 0.8183 - dice_coef_loss: 0.6523\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6519 - accuracy: 0.8203 - dice_coef_loss: 0.6519\n",
      "Epoch 12: loss improved from 0.65233 to 0.65190, saving model to ./temp/cartesian_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 0.6519 - accuracy: 0.8203 - dice_coef_loss: 0.6519\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6526 - accuracy: 0.8269 - dice_coef_loss: 0.6526\n",
      "Epoch 13: loss did not improve from 0.65190\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6526 - accuracy: 0.8269 - dice_coef_loss: 0.6526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6650 - accuracy: 0.8325 - dice_coef_loss: 0.6650\n",
      "Epoch 14: loss did not improve from 0.65190\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6650 - accuracy: 0.8325 - dice_coef_loss: 0.6650\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6538 - accuracy: 0.8299 - dice_coef_loss: 0.6545\n",
      "Epoch 15: loss did not improve from 0.65190\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6538 - accuracy: 0.8299 - dice_coef_loss: 0.6545\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6432 - accuracy: 0.8453 - dice_coef_loss: 0.6432\n",
      "Epoch 16: loss improved from 0.65190 to 0.64317, saving model to ./temp/cartesian_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.6432 - accuracy: 0.8453 - dice_coef_loss: 0.6432\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6488 - accuracy: 0.8355 - dice_coef_loss: 0.6488\n",
      "Epoch 17: loss did not improve from 0.64317\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6488 - accuracy: 0.8355 - dice_coef_loss: 0.6488\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6490 - accuracy: 0.8353 - dice_coef_loss: 0.6490\n",
      "Epoch 18: loss did not improve from 0.64317\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6490 - accuracy: 0.8353 - dice_coef_loss: 0.6490\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6537 - accuracy: 0.8465 - dice_coef_loss: 0.6537\n",
      "Epoch 19: loss did not improve from 0.64317\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6537 - accuracy: 0.8465 - dice_coef_loss: 0.6537\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6463 - accuracy: 0.8539 - dice_coef_loss: 0.6463\n",
      "Epoch 20: loss did not improve from 0.64317\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6463 - accuracy: 0.8539 - dice_coef_loss: 0.6463\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6562 - accuracy: 0.8519 - dice_coef_loss: 0.6562\n",
      "Epoch 21: loss did not improve from 0.64317\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6562 - accuracy: 0.8519 - dice_coef_loss: 0.6562\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6567 - accuracy: 0.8536 - dice_coef_loss: 0.6567\n",
      "Epoch 22: loss did not improve from 0.64317\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6567 - accuracy: 0.8536 - dice_coef_loss: 0.6567\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6500 - accuracy: 0.8497 - dice_coef_loss: 0.6495\n",
      "Epoch 23: loss did not improve from 0.64317\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6500 - accuracy: 0.8497 - dice_coef_loss: 0.6495\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6592 - accuracy: 0.8430 - dice_coef_loss: 0.6592\n",
      "Epoch 24: loss did not improve from 0.64317\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6592 - accuracy: 0.8430 - dice_coef_loss: 0.6592\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6591 - accuracy: 0.8588 - dice_coef_loss: 0.6591\n",
      "Epoch 25: loss did not improve from 0.64317\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6591 - accuracy: 0.8588 - dice_coef_loss: 0.6591\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6463 - accuracy: 0.8485 - dice_coef_loss: 0.6463\n",
      "Epoch 26: loss did not improve from 0.64317\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6463 - accuracy: 0.8485 - dice_coef_loss: 0.6463\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6255 - accuracy: 0.8584 - dice_coef_loss: 0.6255\n",
      "Epoch 27: loss improved from 0.64317 to 0.62554, saving model to ./temp/cartesian_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 0.6255 - accuracy: 0.8584 - dice_coef_loss: 0.6255\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6696 - accuracy: 0.8507 - dice_coef_loss: 0.6696\n",
      "Epoch 28: loss did not improve from 0.62554\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6696 - accuracy: 0.8507 - dice_coef_loss: 0.6696\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6328 - accuracy: 0.8611 - dice_coef_loss: 0.6328\n",
      "Epoch 29: loss did not improve from 0.62554\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6328 - accuracy: 0.8611 - dice_coef_loss: 0.6328\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6396 - accuracy: 0.8620 - dice_coef_loss: 0.6391\n",
      "Epoch 30: loss did not improve from 0.62554\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6396 - accuracy: 0.8620 - dice_coef_loss: 0.6391\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6565 - accuracy: 0.8566 - dice_coef_loss: 0.6565\n",
      "Epoch 31: loss did not improve from 0.62554\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6565 - accuracy: 0.8566 - dice_coef_loss: 0.6565\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6427 - accuracy: 0.8695 - dice_coef_loss: 0.6427\n",
      "Epoch 32: loss did not improve from 0.62554\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6427 - accuracy: 0.8695 - dice_coef_loss: 0.6427\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6398 - accuracy: 0.8650 - dice_coef_loss: 0.6398\n",
      "Epoch 33: loss did not improve from 0.62554\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6398 - accuracy: 0.8650 - dice_coef_loss: 0.6398\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6377 - accuracy: 0.8630 - dice_coef_loss: 0.6377\n",
      "Epoch 34: loss did not improve from 0.62554\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6377 - accuracy: 0.8630 - dice_coef_loss: 0.6377\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6483 - accuracy: 0.8822 - dice_coef_loss: 0.6483\n",
      "Epoch 35: loss did not improve from 0.62554\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6483 - accuracy: 0.8822 - dice_coef_loss: 0.6483\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6257 - accuracy: 0.8731 - dice_coef_loss: 0.6257\n",
      "Epoch 36: loss did not improve from 0.62554\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6257 - accuracy: 0.8731 - dice_coef_loss: 0.6257\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6464 - accuracy: 0.8701 - dice_coef_loss: 0.6464\n",
      "Epoch 37: loss did not improve from 0.62554\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6464 - accuracy: 0.8701 - dice_coef_loss: 0.6464\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6396 - accuracy: 0.8859 - dice_coef_loss: 0.6387\n",
      "Epoch 38: loss did not improve from 0.62554\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6396 - accuracy: 0.8859 - dice_coef_loss: 0.6387\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6541 - accuracy: 0.8616 - dice_coef_loss: 0.6541\n",
      "Epoch 39: loss did not improve from 0.62554\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6541 - accuracy: 0.8616 - dice_coef_loss: 0.6541\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6511 - accuracy: 0.8707 - dice_coef_loss: 0.6511\n",
      "Epoch 40: loss did not improve from 0.62554\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6511 - accuracy: 0.8707 - dice_coef_loss: 0.6511\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6350 - accuracy: 0.8843 - dice_coef_loss: 0.6350\n",
      "Epoch 41: loss did not improve from 0.62554\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6350 - accuracy: 0.8843 - dice_coef_loss: 0.6350\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - ETA: 0s - loss: 0.6489 - accuracy: 0.8738 - dice_coef_loss: 0.6489\n",
      "Epoch 42: loss did not improve from 0.62554\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6489 - accuracy: 0.8738 - dice_coef_loss: 0.6489\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6319 - accuracy: 0.8755 - dice_coef_loss: 0.6319\n",
      "Epoch 43: loss did not improve from 0.62554\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6319 - accuracy: 0.8755 - dice_coef_loss: 0.6319\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6246 - accuracy: 0.8730 - dice_coef_loss: 0.6246\n",
      "Epoch 44: loss improved from 0.62554 to 0.62459, saving model to ./temp/cartesian_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 0.6246 - accuracy: 0.8730 - dice_coef_loss: 0.6246\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6312 - accuracy: 0.8970 - dice_coef_loss: 0.6315\n",
      "Epoch 45: loss did not improve from 0.62459\n",
      "100/100 [==============================] - 26s 257ms/step - loss: 0.6312 - accuracy: 0.8970 - dice_coef_loss: 0.6315\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6434 - accuracy: 0.8847 - dice_coef_loss: 0.6434\n",
      "Epoch 46: loss did not improve from 0.62459\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6434 - accuracy: 0.8847 - dice_coef_loss: 0.6434\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6297 - accuracy: 0.8836 - dice_coef_loss: 0.6297\n",
      "Epoch 47: loss did not improve from 0.62459\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6297 - accuracy: 0.8836 - dice_coef_loss: 0.6297\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6313 - accuracy: 0.8847 - dice_coef_loss: 0.6313\n",
      "Epoch 48: loss did not improve from 0.62459\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6313 - accuracy: 0.8847 - dice_coef_loss: 0.6313\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6474 - accuracy: 0.8816 - dice_coef_loss: 0.6474\n",
      "Epoch 49: loss did not improve from 0.62459\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6474 - accuracy: 0.8816 - dice_coef_loss: 0.6474\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6358 - accuracy: 0.8866 - dice_coef_loss: 0.6358\n",
      "Epoch 50: loss did not improve from 0.62459\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6358 - accuracy: 0.8866 - dice_coef_loss: 0.6358\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6334 - accuracy: 0.8913 - dice_coef_loss: 0.6334\n",
      "Epoch 51: loss did not improve from 0.62459\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6334 - accuracy: 0.8913 - dice_coef_loss: 0.6334\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6374 - accuracy: 0.8879 - dice_coef_loss: 0.6365\n",
      "Epoch 52: loss did not improve from 0.62459\n",
      "100/100 [==============================] - 26s 257ms/step - loss: 0.6374 - accuracy: 0.8879 - dice_coef_loss: 0.6365\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6367 - accuracy: 0.8824 - dice_coef_loss: 0.6367\n",
      "Epoch 53: loss did not improve from 0.62459\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6367 - accuracy: 0.8824 - dice_coef_loss: 0.6367\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6192 - accuracy: 0.8976 - dice_coef_loss: 0.6192\n",
      "Epoch 54: loss improved from 0.62459 to 0.61917, saving model to ./temp/cartesian_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 265ms/step - loss: 0.6192 - accuracy: 0.8976 - dice_coef_loss: 0.6192\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6367 - accuracy: 0.8980 - dice_coef_loss: 0.6367\n",
      "Epoch 55: loss did not improve from 0.61917\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6367 - accuracy: 0.8980 - dice_coef_loss: 0.6367\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6508 - accuracy: 0.8975 - dice_coef_loss: 0.6508\n",
      "Epoch 56: loss did not improve from 0.61917\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6508 - accuracy: 0.8975 - dice_coef_loss: 0.6508\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6219 - accuracy: 0.8913 - dice_coef_loss: 0.6219\n",
      "Epoch 57: loss did not improve from 0.61917\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6219 - accuracy: 0.8913 - dice_coef_loss: 0.6219\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6372 - accuracy: 0.8939 - dice_coef_loss: 0.6372\n",
      "Epoch 58: loss did not improve from 0.61917\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6372 - accuracy: 0.8939 - dice_coef_loss: 0.6372\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6344 - accuracy: 0.8904 - dice_coef_loss: 0.6344\n",
      "Epoch 59: loss did not improve from 0.61917\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6344 - accuracy: 0.8904 - dice_coef_loss: 0.6344\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6159 - accuracy: 0.8999 - dice_coef_loss: 0.6152\n",
      "Epoch 60: loss improved from 0.61917 to 0.61592, saving model to ./temp/cartesian_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 0.6159 - accuracy: 0.8999 - dice_coef_loss: 0.6152\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6251 - accuracy: 0.9073 - dice_coef_loss: 0.6251\n",
      "Epoch 61: loss did not improve from 0.61592\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6251 - accuracy: 0.9073 - dice_coef_loss: 0.6251\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6089 - accuracy: 0.9017 - dice_coef_loss: 0.6089\n",
      "Epoch 62: loss improved from 0.61592 to 0.60893, saving model to ./temp/cartesian_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.6089 - accuracy: 0.9017 - dice_coef_loss: 0.6089\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6356 - accuracy: 0.8908 - dice_coef_loss: 0.6356\n",
      "Epoch 63: loss did not improve from 0.60893\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6356 - accuracy: 0.8908 - dice_coef_loss: 0.6356\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6433 - accuracy: 0.8936 - dice_coef_loss: 0.6433\n",
      "Epoch 64: loss did not improve from 0.60893\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6433 - accuracy: 0.8936 - dice_coef_loss: 0.6433\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6319 - accuracy: 0.9019 - dice_coef_loss: 0.6319\n",
      "Epoch 65: loss did not improve from 0.60893\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6319 - accuracy: 0.9019 - dice_coef_loss: 0.6319\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6350 - accuracy: 0.8985 - dice_coef_loss: 0.6350\n",
      "Epoch 66: loss did not improve from 0.60893\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6350 - accuracy: 0.8985 - dice_coef_loss: 0.6350\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6309 - accuracy: 0.9026 - dice_coef_loss: 0.6312\n",
      "Epoch 67: loss did not improve from 0.60893\n",
      "100/100 [==============================] - 26s 257ms/step - loss: 0.6309 - accuracy: 0.9026 - dice_coef_loss: 0.6312\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6261 - accuracy: 0.8913 - dice_coef_loss: 0.6261\n",
      "Epoch 68: loss did not improve from 0.60893\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6261 - accuracy: 0.8913 - dice_coef_loss: 0.6261\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6337 - accuracy: 0.8996 - dice_coef_loss: 0.6337\n",
      "Epoch 69: loss did not improve from 0.60893\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6337 - accuracy: 0.8996 - dice_coef_loss: 0.6337\n",
      "Epoch 70/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - ETA: 0s - loss: 0.6380 - accuracy: 0.8914 - dice_coef_loss: 0.6380\n",
      "Epoch 70: loss did not improve from 0.60893\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6380 - accuracy: 0.8914 - dice_coef_loss: 0.6380\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6296 - accuracy: 0.9036 - dice_coef_loss: 0.6296\n",
      "Epoch 71: loss did not improve from 0.60893\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6296 - accuracy: 0.9036 - dice_coef_loss: 0.6296\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6209 - accuracy: 0.9037 - dice_coef_loss: 0.6209\n",
      "Epoch 72: loss did not improve from 0.60893\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6209 - accuracy: 0.9037 - dice_coef_loss: 0.6209\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6194 - accuracy: 0.8998 - dice_coef_loss: 0.6194\n",
      "Epoch 73: loss did not improve from 0.60893\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6194 - accuracy: 0.8998 - dice_coef_loss: 0.6194\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6366 - accuracy: 0.9010 - dice_coef_loss: 0.6366\n",
      "Epoch 74: loss did not improve from 0.60893\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6366 - accuracy: 0.9010 - dice_coef_loss: 0.6366\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6455 - accuracy: 0.8975 - dice_coef_loss: 0.6454\n",
      "Epoch 75: loss did not improve from 0.60893\n",
      "100/100 [==============================] - 26s 257ms/step - loss: 0.6455 - accuracy: 0.8975 - dice_coef_loss: 0.6454\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6266 - accuracy: 0.9059 - dice_coef_loss: 0.6266\n",
      "Epoch 76: loss did not improve from 0.60893\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6266 - accuracy: 0.9059 - dice_coef_loss: 0.6266\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6177 - accuracy: 0.9091 - dice_coef_loss: 0.6177\n",
      "Epoch 77: loss did not improve from 0.60893\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6177 - accuracy: 0.9091 - dice_coef_loss: 0.6177\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6209 - accuracy: 0.9082 - dice_coef_loss: 0.6209\n",
      "Epoch 78: loss did not improve from 0.60893\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6209 - accuracy: 0.9082 - dice_coef_loss: 0.6209\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6105 - accuracy: 0.9094 - dice_coef_loss: 0.6105\n",
      "Epoch 79: loss did not improve from 0.60893\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6105 - accuracy: 0.9094 - dice_coef_loss: 0.6105\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6192 - accuracy: 0.9040 - dice_coef_loss: 0.6192\n",
      "Epoch 80: loss did not improve from 0.60893\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6192 - accuracy: 0.9040 - dice_coef_loss: 0.6192\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6404 - accuracy: 0.9081 - dice_coef_loss: 0.6404\n",
      "Epoch 81: loss did not improve from 0.60893\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6404 - accuracy: 0.9081 - dice_coef_loss: 0.6404\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6219 - accuracy: 0.9051 - dice_coef_loss: 0.6219\n",
      "Epoch 82: loss did not improve from 0.60893\n",
      "100/100 [==============================] - 26s 257ms/step - loss: 0.6219 - accuracy: 0.9051 - dice_coef_loss: 0.6219\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6297 - accuracy: 0.9147 - dice_coef_loss: 0.6297\n",
      "Epoch 83: loss did not improve from 0.60893\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6297 - accuracy: 0.9147 - dice_coef_loss: 0.6297\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6177 - accuracy: 0.9137 - dice_coef_loss: 0.6177\n",
      "Epoch 84: loss did not improve from 0.60893\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6177 - accuracy: 0.9137 - dice_coef_loss: 0.6177\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6142 - accuracy: 0.9075 - dice_coef_loss: 0.6142\n",
      "Epoch 85: loss did not improve from 0.60893\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6142 - accuracy: 0.9075 - dice_coef_loss: 0.6142\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6239 - accuracy: 0.9105 - dice_coef_loss: 0.6239\n",
      "Epoch 86: loss did not improve from 0.60893\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6239 - accuracy: 0.9105 - dice_coef_loss: 0.6239\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6103 - accuracy: 0.9133 - dice_coef_loss: 0.6103\n",
      "Epoch 87: loss did not improve from 0.60893\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6103 - accuracy: 0.9133 - dice_coef_loss: 0.6103\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6342 - accuracy: 0.9201 - dice_coef_loss: 0.6342\n",
      "Epoch 88: loss did not improve from 0.60893\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6342 - accuracy: 0.9201 - dice_coef_loss: 0.6342\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6287 - accuracy: 0.9101 - dice_coef_loss: 0.6293\n",
      "Epoch 89: loss did not improve from 0.60893\n",
      "100/100 [==============================] - 26s 257ms/step - loss: 0.6287 - accuracy: 0.9101 - dice_coef_loss: 0.6293\n",
      "Epoch 90/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6256 - accuracy: 0.9101 - dice_coef_loss: 0.6256\n",
      "Epoch 90: loss did not improve from 0.60893\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6256 - accuracy: 0.9101 - dice_coef_loss: 0.6256\n",
      "Epoch 91/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6074 - accuracy: 0.9151 - dice_coef_loss: 0.6074\n",
      "Epoch 91: loss improved from 0.60893 to 0.60744, saving model to ./temp/cartesian_Dom/2/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 265ms/step - loss: 0.6074 - accuracy: 0.9151 - dice_coef_loss: 0.6074\n",
      "Epoch 92/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6298 - accuracy: 0.9131 - dice_coef_loss: 0.6298\n",
      "Epoch 92: loss did not improve from 0.60744\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6298 - accuracy: 0.9131 - dice_coef_loss: 0.6298\n",
      "Epoch 93/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6411 - accuracy: 0.9156 - dice_coef_loss: 0.6411\n",
      "Epoch 93: loss did not improve from 0.60744\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6411 - accuracy: 0.9156 - dice_coef_loss: 0.6411\n",
      "Epoch 94/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6227 - accuracy: 0.9007 - dice_coef_loss: 0.6227\n",
      "Epoch 94: loss did not improve from 0.60744\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6227 - accuracy: 0.9007 - dice_coef_loss: 0.6227\n",
      "Epoch 95/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6194 - accuracy: 0.9175 - dice_coef_loss: 0.6194\n",
      "Epoch 95: loss did not improve from 0.60744\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6194 - accuracy: 0.9175 - dice_coef_loss: 0.6194\n",
      "Epoch 96/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6196 - accuracy: 0.9119 - dice_coef_loss: 0.6196\n",
      "Epoch 96: loss did not improve from 0.60744\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6196 - accuracy: 0.9119 - dice_coef_loss: 0.6196\n",
      "Epoch 97/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6135 - accuracy: 0.9202 - dice_coef_loss: 0.6133\n",
      "Epoch 97: loss did not improve from 0.60744\n",
      "100/100 [==============================] - 26s 257ms/step - loss: 0.6135 - accuracy: 0.9202 - dice_coef_loss: 0.6133\n",
      "Epoch 98/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6157 - accuracy: 0.9121 - dice_coef_loss: 0.6157\n",
      "Epoch 98: loss did not improve from 0.60744\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6157 - accuracy: 0.9121 - dice_coef_loss: 0.6157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6142 - accuracy: 0.9098 - dice_coef_loss: 0.6142\n",
      "Epoch 99: loss did not improve from 0.60744\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6142 - accuracy: 0.9098 - dice_coef_loss: 0.6142\n",
      "Epoch 100/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6238 - accuracy: 0.9177 - dice_coef_loss: 0.6238\n",
      "Epoch 100: loss did not improve from 0.60744\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6238 - accuracy: 0.9177 - dice_coef_loss: 0.6238\n",
      "Found 2962 images belonging to 1 classes.\n",
      "Found 2962 images belonging to 1 classes.\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcd20123f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcd20123f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6918 - accuracy: 0.4407 - dice_coef_loss: 0.6918\n",
      "Epoch 1: loss improved from inf to 0.69183, saving model to ./temp/cartesian_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 262ms/step - loss: 0.6918 - accuracy: 0.4407 - dice_coef_loss: 0.6918\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6711 - accuracy: 0.7203 - dice_coef_loss: 0.6711\n",
      "Epoch 2: loss improved from 0.69183 to 0.67115, saving model to ./temp/cartesian_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.6711 - accuracy: 0.7203 - dice_coef_loss: 0.6711\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6827 - accuracy: 0.7328 - dice_coef_loss: 0.6827\n",
      "Epoch 3: loss did not improve from 0.67115\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6827 - accuracy: 0.7328 - dice_coef_loss: 0.6827\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6646 - accuracy: 0.7802 - dice_coef_loss: 0.6646\n",
      "Epoch 4: loss improved from 0.67115 to 0.66458, saving model to ./temp/cartesian_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.6646 - accuracy: 0.7802 - dice_coef_loss: 0.6646\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6771 - accuracy: 0.7873 - dice_coef_loss: 0.6771\n",
      "Epoch 5: loss did not improve from 0.66458\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6771 - accuracy: 0.7873 - dice_coef_loss: 0.6771\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6715 - accuracy: 0.8071 - dice_coef_loss: 0.6715\n",
      "Epoch 6: loss did not improve from 0.66458\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6715 - accuracy: 0.8071 - dice_coef_loss: 0.6715\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6694 - accuracy: 0.8066 - dice_coef_loss: 0.6694\n",
      "Epoch 7: loss did not improve from 0.66458\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.6694 - accuracy: 0.8066 - dice_coef_loss: 0.6694\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6462 - accuracy: 0.8224 - dice_coef_loss: 0.6471\n",
      "Epoch 8: loss improved from 0.66458 to 0.64624, saving model to ./temp/cartesian_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.6462 - accuracy: 0.8224 - dice_coef_loss: 0.6471\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6523 - accuracy: 0.8141 - dice_coef_loss: 0.6523\n",
      "Epoch 9: loss did not improve from 0.64624\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.6523 - accuracy: 0.8141 - dice_coef_loss: 0.6523\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6504 - accuracy: 0.8335 - dice_coef_loss: 0.6504\n",
      "Epoch 10: loss did not improve from 0.64624\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6504 - accuracy: 0.8335 - dice_coef_loss: 0.6504\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6568 - accuracy: 0.8323 - dice_coef_loss: 0.6568\n",
      "Epoch 11: loss did not improve from 0.64624\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6568 - accuracy: 0.8323 - dice_coef_loss: 0.6568\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6558 - accuracy: 0.8373 - dice_coef_loss: 0.6558\n",
      "Epoch 12: loss did not improve from 0.64624\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6558 - accuracy: 0.8373 - dice_coef_loss: 0.6558\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6505 - accuracy: 0.8515 - dice_coef_loss: 0.6505\n",
      "Epoch 13: loss did not improve from 0.64624\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6505 - accuracy: 0.8515 - dice_coef_loss: 0.6505\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6663 - accuracy: 0.8368 - dice_coef_loss: 0.6663\n",
      "Epoch 14: loss did not improve from 0.64624\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6663 - accuracy: 0.8368 - dice_coef_loss: 0.6663\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6701 - accuracy: 0.8467 - dice_coef_loss: 0.6703\n",
      "Epoch 15: loss did not improve from 0.64624\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6701 - accuracy: 0.8467 - dice_coef_loss: 0.6703\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6613 - accuracy: 0.8448 - dice_coef_loss: 0.6613\n",
      "Epoch 16: loss did not improve from 0.64624\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6613 - accuracy: 0.8448 - dice_coef_loss: 0.6613\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6485 - accuracy: 0.8414 - dice_coef_loss: 0.6485\n",
      "Epoch 17: loss did not improve from 0.64624\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6485 - accuracy: 0.8414 - dice_coef_loss: 0.6485\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6433 - accuracy: 0.8505 - dice_coef_loss: 0.6433\n",
      "Epoch 18: loss improved from 0.64624 to 0.64326, saving model to ./temp/cartesian_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.6433 - accuracy: 0.8505 - dice_coef_loss: 0.6433\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6358 - accuracy: 0.8551 - dice_coef_loss: 0.6358\n",
      "Epoch 19: loss improved from 0.64326 to 0.63583, saving model to ./temp/cartesian_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.6358 - accuracy: 0.8551 - dice_coef_loss: 0.6358\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6420 - accuracy: 0.8651 - dice_coef_loss: 0.6420\n",
      "Epoch 20: loss did not improve from 0.63583\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6420 - accuracy: 0.8651 - dice_coef_loss: 0.6420\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6705 - accuracy: 0.8523 - dice_coef_loss: 0.6705\n",
      "Epoch 21: loss did not improve from 0.63583\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6705 - accuracy: 0.8523 - dice_coef_loss: 0.6705\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6489 - accuracy: 0.8523 - dice_coef_loss: 0.6489\n",
      "Epoch 22: loss did not improve from 0.63583\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6489 - accuracy: 0.8523 - dice_coef_loss: 0.6489\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - ETA: 0s - loss: 0.6484 - accuracy: 0.8602 - dice_coef_loss: 0.6486\n",
      "Epoch 23: loss did not improve from 0.63583\n",
      "100/100 [==============================] - 26s 257ms/step - loss: 0.6484 - accuracy: 0.8602 - dice_coef_loss: 0.6486\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6653 - accuracy: 0.8669 - dice_coef_loss: 0.6653\n",
      "Epoch 24: loss did not improve from 0.63583\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6653 - accuracy: 0.8669 - dice_coef_loss: 0.6653\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6483 - accuracy: 0.8476 - dice_coef_loss: 0.6483\n",
      "Epoch 25: loss did not improve from 0.63583\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6483 - accuracy: 0.8476 - dice_coef_loss: 0.6483\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6363 - accuracy: 0.8496 - dice_coef_loss: 0.6363\n",
      "Epoch 26: loss did not improve from 0.63583\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6363 - accuracy: 0.8496 - dice_coef_loss: 0.6363\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6447 - accuracy: 0.8683 - dice_coef_loss: 0.6447\n",
      "Epoch 27: loss did not improve from 0.63583\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6447 - accuracy: 0.8683 - dice_coef_loss: 0.6447\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6481 - accuracy: 0.8514 - dice_coef_loss: 0.6481\n",
      "Epoch 28: loss did not improve from 0.63583\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6481 - accuracy: 0.8514 - dice_coef_loss: 0.6481\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6388 - accuracy: 0.8710 - dice_coef_loss: 0.6388\n",
      "Epoch 29: loss did not improve from 0.63583\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6388 - accuracy: 0.8710 - dice_coef_loss: 0.6388\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6403 - accuracy: 0.8691 - dice_coef_loss: 0.6393\n",
      "Epoch 30: loss did not improve from 0.63583\n",
      "100/100 [==============================] - 26s 257ms/step - loss: 0.6403 - accuracy: 0.8691 - dice_coef_loss: 0.6393\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6695 - accuracy: 0.8608 - dice_coef_loss: 0.6695\n",
      "Epoch 31: loss did not improve from 0.63583\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6695 - accuracy: 0.8608 - dice_coef_loss: 0.6695\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6452 - accuracy: 0.8684 - dice_coef_loss: 0.6452\n",
      "Epoch 32: loss did not improve from 0.63583\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6452 - accuracy: 0.8684 - dice_coef_loss: 0.6452\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6454 - accuracy: 0.8735 - dice_coef_loss: 0.6454\n",
      "Epoch 33: loss did not improve from 0.63583\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6454 - accuracy: 0.8735 - dice_coef_loss: 0.6454\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6236 - accuracy: 0.8812 - dice_coef_loss: 0.6236\n",
      "Epoch 34: loss improved from 0.63583 to 0.62357, saving model to ./temp/cartesian_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 0.6236 - accuracy: 0.8812 - dice_coef_loss: 0.6236\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6305 - accuracy: 0.8836 - dice_coef_loss: 0.6305\n",
      "Epoch 35: loss did not improve from 0.62357\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6305 - accuracy: 0.8836 - dice_coef_loss: 0.6305\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6313 - accuracy: 0.8829 - dice_coef_loss: 0.6313\n",
      "Epoch 36: loss did not improve from 0.62357\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6313 - accuracy: 0.8829 - dice_coef_loss: 0.6313\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6595 - accuracy: 0.8648 - dice_coef_loss: 0.6595\n",
      "Epoch 37: loss did not improve from 0.62357\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6595 - accuracy: 0.8648 - dice_coef_loss: 0.6595\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6415 - accuracy: 0.8809 - dice_coef_loss: 0.6416\n",
      "Epoch 38: loss did not improve from 0.62357\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6415 - accuracy: 0.8809 - dice_coef_loss: 0.6416\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6422 - accuracy: 0.8696 - dice_coef_loss: 0.6422\n",
      "Epoch 39: loss did not improve from 0.62357\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6422 - accuracy: 0.8696 - dice_coef_loss: 0.6422\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6447 - accuracy: 0.8839 - dice_coef_loss: 0.6447\n",
      "Epoch 40: loss did not improve from 0.62357\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6447 - accuracy: 0.8839 - dice_coef_loss: 0.6447\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6282 - accuracy: 0.8918 - dice_coef_loss: 0.6282\n",
      "Epoch 41: loss did not improve from 0.62357\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6282 - accuracy: 0.8918 - dice_coef_loss: 0.6282\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6360 - accuracy: 0.8850 - dice_coef_loss: 0.6360\n",
      "Epoch 42: loss did not improve from 0.62357\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6360 - accuracy: 0.8850 - dice_coef_loss: 0.6360\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6337 - accuracy: 0.8764 - dice_coef_loss: 0.6337\n",
      "Epoch 43: loss did not improve from 0.62357\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6337 - accuracy: 0.8764 - dice_coef_loss: 0.6337\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6478 - accuracy: 0.8881 - dice_coef_loss: 0.6478\n",
      "Epoch 44: loss did not improve from 0.62357\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6478 - accuracy: 0.8881 - dice_coef_loss: 0.6478\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6573 - accuracy: 0.8706 - dice_coef_loss: 0.6568\n",
      "Epoch 45: loss did not improve from 0.62357\n",
      "100/100 [==============================] - 26s 257ms/step - loss: 0.6573 - accuracy: 0.8706 - dice_coef_loss: 0.6568\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6269 - accuracy: 0.8941 - dice_coef_loss: 0.6269\n",
      "Epoch 46: loss did not improve from 0.62357\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6269 - accuracy: 0.8941 - dice_coef_loss: 0.6269\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6424 - accuracy: 0.8823 - dice_coef_loss: 0.6424\n",
      "Epoch 47: loss did not improve from 0.62357\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6424 - accuracy: 0.8823 - dice_coef_loss: 0.6424\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6285 - accuracy: 0.8863 - dice_coef_loss: 0.6285\n",
      "Epoch 48: loss did not improve from 0.62357\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6285 - accuracy: 0.8863 - dice_coef_loss: 0.6285\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6240 - accuracy: 0.8872 - dice_coef_loss: 0.6240\n",
      "Epoch 49: loss did not improve from 0.62357\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6240 - accuracy: 0.8872 - dice_coef_loss: 0.6240\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6430 - accuracy: 0.8983 - dice_coef_loss: 0.6430\n",
      "Epoch 50: loss did not improve from 0.62357\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6430 - accuracy: 0.8983 - dice_coef_loss: 0.6430\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6224 - accuracy: 0.8814 - dice_coef_loss: 0.6224\n",
      "Epoch 51: loss improved from 0.62357 to 0.62243, saving model to ./temp/cartesian_Dom/3/checkpoint.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 27s 266ms/step - loss: 0.6224 - accuracy: 0.8814 - dice_coef_loss: 0.6224\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6531 - accuracy: 0.8959 - dice_coef_loss: 0.6531\n",
      "Epoch 52: loss did not improve from 0.62243\n",
      "100/100 [==============================] - 26s 257ms/step - loss: 0.6531 - accuracy: 0.8959 - dice_coef_loss: 0.6531\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6383 - accuracy: 0.8917 - dice_coef_loss: 0.6383\n",
      "Epoch 53: loss did not improve from 0.62243\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6383 - accuracy: 0.8917 - dice_coef_loss: 0.6383\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6330 - accuracy: 0.9040 - dice_coef_loss: 0.6330\n",
      "Epoch 54: loss did not improve from 0.62243\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6330 - accuracy: 0.9040 - dice_coef_loss: 0.6330\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6376 - accuracy: 0.8946 - dice_coef_loss: 0.6376\n",
      "Epoch 55: loss did not improve from 0.62243\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6376 - accuracy: 0.8946 - dice_coef_loss: 0.6376\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6270 - accuracy: 0.8930 - dice_coef_loss: 0.6270\n",
      "Epoch 56: loss did not improve from 0.62243\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6270 - accuracy: 0.8930 - dice_coef_loss: 0.6270\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6292 - accuracy: 0.8938 - dice_coef_loss: 0.6292\n",
      "Epoch 57: loss did not improve from 0.62243\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6292 - accuracy: 0.8938 - dice_coef_loss: 0.6292\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6517 - accuracy: 0.8866 - dice_coef_loss: 0.6517\n",
      "Epoch 58: loss did not improve from 0.62243\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6517 - accuracy: 0.8866 - dice_coef_loss: 0.6517\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6216 - accuracy: 0.8955 - dice_coef_loss: 0.6216\n",
      "Epoch 59: loss improved from 0.62243 to 0.62156, saving model to ./temp/cartesian_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.6216 - accuracy: 0.8955 - dice_coef_loss: 0.6216\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6205 - accuracy: 0.9007 - dice_coef_loss: 0.6203\n",
      "Epoch 60: loss improved from 0.62156 to 0.62048, saving model to ./temp/cartesian_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 26s 265ms/step - loss: 0.6205 - accuracy: 0.9007 - dice_coef_loss: 0.6203\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6469 - accuracy: 0.9020 - dice_coef_loss: 0.6469\n",
      "Epoch 61: loss did not improve from 0.62048\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6469 - accuracy: 0.9020 - dice_coef_loss: 0.6469\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6304 - accuracy: 0.8869 - dice_coef_loss: 0.6304\n",
      "Epoch 62: loss did not improve from 0.62048\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6304 - accuracy: 0.8869 - dice_coef_loss: 0.6304\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6252 - accuracy: 0.8988 - dice_coef_loss: 0.6252\n",
      "Epoch 63: loss did not improve from 0.62048\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6252 - accuracy: 0.8988 - dice_coef_loss: 0.6252\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6265 - accuracy: 0.8920 - dice_coef_loss: 0.6265\n",
      "Epoch 64: loss did not improve from 0.62048\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6265 - accuracy: 0.8920 - dice_coef_loss: 0.6265\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6296 - accuracy: 0.8944 - dice_coef_loss: 0.6296\n",
      "Epoch 65: loss did not improve from 0.62048\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6296 - accuracy: 0.8944 - dice_coef_loss: 0.6296\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6284 - accuracy: 0.9067 - dice_coef_loss: 0.6284\n",
      "Epoch 66: loss did not improve from 0.62048\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6284 - accuracy: 0.9067 - dice_coef_loss: 0.6284\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6533 - accuracy: 0.8947 - dice_coef_loss: 0.6528\n",
      "Epoch 67: loss did not improve from 0.62048\n",
      "100/100 [==============================] - 26s 257ms/step - loss: 0.6533 - accuracy: 0.8947 - dice_coef_loss: 0.6528\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6352 - accuracy: 0.8969 - dice_coef_loss: 0.6352\n",
      "Epoch 68: loss did not improve from 0.62048\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6352 - accuracy: 0.8969 - dice_coef_loss: 0.6352\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6266 - accuracy: 0.9041 - dice_coef_loss: 0.6266\n",
      "Epoch 69: loss did not improve from 0.62048\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6266 - accuracy: 0.9041 - dice_coef_loss: 0.6266\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6269 - accuracy: 0.9046 - dice_coef_loss: 0.6269\n",
      "Epoch 70: loss did not improve from 0.62048\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6269 - accuracy: 0.9046 - dice_coef_loss: 0.6269\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6312 - accuracy: 0.8974 - dice_coef_loss: 0.6312\n",
      "Epoch 71: loss did not improve from 0.62048\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6312 - accuracy: 0.8974 - dice_coef_loss: 0.6312\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6178 - accuracy: 0.9000 - dice_coef_loss: 0.6178\n",
      "Epoch 72: loss improved from 0.62048 to 0.61778, saving model to ./temp/cartesian_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 0.6178 - accuracy: 0.9000 - dice_coef_loss: 0.6178\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6233 - accuracy: 0.9150 - dice_coef_loss: 0.6233\n",
      "Epoch 73: loss did not improve from 0.61778\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6233 - accuracy: 0.9150 - dice_coef_loss: 0.6233\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6315 - accuracy: 0.9030 - dice_coef_loss: 0.6315\n",
      "Epoch 74: loss did not improve from 0.61778\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.6315 - accuracy: 0.9030 - dice_coef_loss: 0.6315\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6437 - accuracy: 0.9072 - dice_coef_loss: 0.6434\n",
      "Epoch 75: loss did not improve from 0.61778\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6437 - accuracy: 0.9072 - dice_coef_loss: 0.6434\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6266 - accuracy: 0.9064 - dice_coef_loss: 0.6266\n",
      "Epoch 76: loss did not improve from 0.61778\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6266 - accuracy: 0.9064 - dice_coef_loss: 0.6266\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6076 - accuracy: 0.9194 - dice_coef_loss: 0.6076\n",
      "Epoch 77: loss improved from 0.61778 to 0.60761, saving model to ./temp/cartesian_Dom/3/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.6076 - accuracy: 0.9194 - dice_coef_loss: 0.6076\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6319 - accuracy: 0.9062 - dice_coef_loss: 0.6319\n",
      "Epoch 78: loss did not improve from 0.60761\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6319 - accuracy: 0.9062 - dice_coef_loss: 0.6319\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6127 - accuracy: 0.9153 - dice_coef_loss: 0.6127\n",
      "Epoch 79: loss did not improve from 0.60761\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6127 - accuracy: 0.9153 - dice_coef_loss: 0.6127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6217 - accuracy: 0.9110 - dice_coef_loss: 0.6217\n",
      "Epoch 80: loss did not improve from 0.60761\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6217 - accuracy: 0.9110 - dice_coef_loss: 0.6217\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6256 - accuracy: 0.9080 - dice_coef_loss: 0.6256\n",
      "Epoch 81: loss did not improve from 0.60761\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6256 - accuracy: 0.9080 - dice_coef_loss: 0.6256\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6306 - accuracy: 0.9079 - dice_coef_loss: 0.6313\n",
      "Epoch 82: loss did not improve from 0.60761\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6306 - accuracy: 0.9079 - dice_coef_loss: 0.6313\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6139 - accuracy: 0.9187 - dice_coef_loss: 0.6139\n",
      "Epoch 83: loss did not improve from 0.60761\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.6139 - accuracy: 0.9187 - dice_coef_loss: 0.6139\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6416 - accuracy: 0.9160 - dice_coef_loss: 0.6416\n",
      "Epoch 84: loss did not improve from 0.60761\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6416 - accuracy: 0.9160 - dice_coef_loss: 0.6416\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6309 - accuracy: 0.9116 - dice_coef_loss: 0.6309\n",
      "Epoch 85: loss did not improve from 0.60761\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.6309 - accuracy: 0.9116 - dice_coef_loss: 0.6309\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6134 - accuracy: 0.9075 - dice_coef_loss: 0.6134\n",
      "Epoch 86: loss did not improve from 0.60761\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6134 - accuracy: 0.9075 - dice_coef_loss: 0.6134\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6219 - accuracy: 0.9164 - dice_coef_loss: 0.6219\n",
      "Epoch 87: loss did not improve from 0.60761\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6219 - accuracy: 0.9164 - dice_coef_loss: 0.6219\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6262 - accuracy: 0.9063 - dice_coef_loss: 0.6262\n",
      "Epoch 88: loss did not improve from 0.60761\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.6262 - accuracy: 0.9063 - dice_coef_loss: 0.6262\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6191 - accuracy: 0.9053 - dice_coef_loss: 0.6187\n",
      "Epoch 89: loss did not improve from 0.60761\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6191 - accuracy: 0.9053 - dice_coef_loss: 0.6187\n",
      "Epoch 90/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6292 - accuracy: 0.9085 - dice_coef_loss: 0.6292\n",
      "Epoch 90: loss did not improve from 0.60761\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6292 - accuracy: 0.9085 - dice_coef_loss: 0.6292\n",
      "Epoch 91/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6340 - accuracy: 0.9161 - dice_coef_loss: 0.6340\n",
      "Epoch 91: loss did not improve from 0.60761\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6340 - accuracy: 0.9161 - dice_coef_loss: 0.6340\n",
      "Epoch 92/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6206 - accuracy: 0.9125 - dice_coef_loss: 0.6206\n",
      "Epoch 92: loss did not improve from 0.60761\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6206 - accuracy: 0.9125 - dice_coef_loss: 0.6206\n",
      "Epoch 93/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6234 - accuracy: 0.9158 - dice_coef_loss: 0.6234\n",
      "Epoch 93: loss did not improve from 0.60761\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6234 - accuracy: 0.9158 - dice_coef_loss: 0.6234\n",
      "Epoch 94/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6289 - accuracy: 0.9100 - dice_coef_loss: 0.6289\n",
      "Epoch 94: loss did not improve from 0.60761\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6289 - accuracy: 0.9100 - dice_coef_loss: 0.6289\n",
      "Epoch 95/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6184 - accuracy: 0.9200 - dice_coef_loss: 0.6184\n",
      "Epoch 95: loss did not improve from 0.60761\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.6184 - accuracy: 0.9200 - dice_coef_loss: 0.6184\n",
      "Epoch 96/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6148 - accuracy: 0.9185 - dice_coef_loss: 0.6148\n",
      "Epoch 96: loss did not improve from 0.60761\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6148 - accuracy: 0.9185 - dice_coef_loss: 0.6148\n",
      "Epoch 97/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6202 - accuracy: 0.9227 - dice_coef_loss: 0.6199\n",
      "Epoch 97: loss did not improve from 0.60761\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6202 - accuracy: 0.9227 - dice_coef_loss: 0.6199\n",
      "Epoch 98/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6224 - accuracy: 0.9097 - dice_coef_loss: 0.6224\n",
      "Epoch 98: loss did not improve from 0.60761\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6224 - accuracy: 0.9097 - dice_coef_loss: 0.6224\n",
      "Epoch 99/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6232 - accuracy: 0.9141 - dice_coef_loss: 0.6232\n",
      "Epoch 99: loss did not improve from 0.60761\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6232 - accuracy: 0.9141 - dice_coef_loss: 0.6232\n",
      "Epoch 100/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6213 - accuracy: 0.9152 - dice_coef_loss: 0.6213\n",
      "Epoch 100: loss did not improve from 0.60761\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6213 - accuracy: 0.9152 - dice_coef_loss: 0.6213\n",
      "Found 2962 images belonging to 1 classes.\n",
      "Found 2962 images belonging to 1 classes.\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcd5dfb1e60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcd5dfb1e60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.7002 - accuracy: 0.5243 - dice_coef_loss: 0.7002\n",
      "Epoch 1: loss improved from inf to 0.70024, saving model to ./temp/cartesian_Dom/4/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 261ms/step - loss: 0.7002 - accuracy: 0.5243 - dice_coef_loss: 0.7002\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6803 - accuracy: 0.7077 - dice_coef_loss: 0.6803\n",
      "Epoch 2: loss improved from 0.70024 to 0.68029, saving model to ./temp/cartesian_Dom/4/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 0.6803 - accuracy: 0.7077 - dice_coef_loss: 0.6803\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6740 - accuracy: 0.7363 - dice_coef_loss: 0.6740\n",
      "Epoch 3: loss improved from 0.68029 to 0.67397, saving model to ./temp/cartesian_Dom/4/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 0.6740 - accuracy: 0.7363 - dice_coef_loss: 0.6740\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6771 - accuracy: 0.7665 - dice_coef_loss: 0.6771\n",
      "Epoch 4: loss did not improve from 0.67397\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6771 - accuracy: 0.7665 - dice_coef_loss: 0.6771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6751 - accuracy: 0.7905 - dice_coef_loss: 0.6751\n",
      "Epoch 5: loss did not improve from 0.67397\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6751 - accuracy: 0.7905 - dice_coef_loss: 0.6751\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6571 - accuracy: 0.7874 - dice_coef_loss: 0.6571\n",
      "Epoch 6: loss improved from 0.67397 to 0.65709, saving model to ./temp/cartesian_Dom/4/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 0.6571 - accuracy: 0.7874 - dice_coef_loss: 0.6571\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6681 - accuracy: 0.8097 - dice_coef_loss: 0.6681\n",
      "Epoch 7: loss did not improve from 0.65709\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6681 - accuracy: 0.8097 - dice_coef_loss: 0.6681\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6525 - accuracy: 0.8122 - dice_coef_loss: 0.6535\n",
      "Epoch 8: loss improved from 0.65709 to 0.65251, saving model to ./temp/cartesian_Dom/4/checkpoint.hdf5\n",
      "100/100 [==============================] - 26s 264ms/step - loss: 0.6525 - accuracy: 0.8122 - dice_coef_loss: 0.6535\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6564 - accuracy: 0.8074 - dice_coef_loss: 0.6564\n",
      "Epoch 9: loss did not improve from 0.65251\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6564 - accuracy: 0.8074 - dice_coef_loss: 0.6564\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6752 - accuracy: 0.8156 - dice_coef_loss: 0.6752\n",
      "Epoch 10: loss did not improve from 0.65251\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6752 - accuracy: 0.8156 - dice_coef_loss: 0.6752\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6650 - accuracy: 0.8259 - dice_coef_loss: 0.6650\n",
      "Epoch 11: loss did not improve from 0.65251\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6650 - accuracy: 0.8259 - dice_coef_loss: 0.6650\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6533 - accuracy: 0.8253 - dice_coef_loss: 0.6533\n",
      "Epoch 12: loss did not improve from 0.65251\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6533 - accuracy: 0.8253 - dice_coef_loss: 0.6533\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6502 - accuracy: 0.8270 - dice_coef_loss: 0.6502\n",
      "Epoch 13: loss improved from 0.65251 to 0.65019, saving model to ./temp/cartesian_Dom/4/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 0.6502 - accuracy: 0.8270 - dice_coef_loss: 0.6502\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6485 - accuracy: 0.8412 - dice_coef_loss: 0.6485\n",
      "Epoch 14: loss improved from 0.65019 to 0.64853, saving model to ./temp/cartesian_Dom/4/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 0.6485 - accuracy: 0.8412 - dice_coef_loss: 0.6485\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6497 - accuracy: 0.8305 - dice_coef_loss: 0.6497\n",
      "Epoch 15: loss did not improve from 0.64853\n",
      "100/100 [==============================] - 26s 257ms/step - loss: 0.6497 - accuracy: 0.8305 - dice_coef_loss: 0.6497\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6519 - accuracy: 0.8515 - dice_coef_loss: 0.6519\n",
      "Epoch 16: loss did not improve from 0.64853\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6519 - accuracy: 0.8515 - dice_coef_loss: 0.6519\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6505 - accuracy: 0.8438 - dice_coef_loss: 0.6505\n",
      "Epoch 17: loss did not improve from 0.64853\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6505 - accuracy: 0.8438 - dice_coef_loss: 0.6505\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6485 - accuracy: 0.8488 - dice_coef_loss: 0.6485\n",
      "Epoch 18: loss improved from 0.64853 to 0.64851, saving model to ./temp/cartesian_Dom/4/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 0.6485 - accuracy: 0.8488 - dice_coef_loss: 0.6485\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6385 - accuracy: 0.8432 - dice_coef_loss: 0.6385\n",
      "Epoch 19: loss improved from 0.64851 to 0.63850, saving model to ./temp/cartesian_Dom/4/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 265ms/step - loss: 0.6385 - accuracy: 0.8432 - dice_coef_loss: 0.6385\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6401 - accuracy: 0.8607 - dice_coef_loss: 0.6401\n",
      "Epoch 20: loss did not improve from 0.63850\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6401 - accuracy: 0.8607 - dice_coef_loss: 0.6401\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6510 - accuracy: 0.8641 - dice_coef_loss: 0.6510\n",
      "Epoch 21: loss did not improve from 0.63850\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6510 - accuracy: 0.8641 - dice_coef_loss: 0.6510\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6610 - accuracy: 0.8547 - dice_coef_loss: 0.6610\n",
      "Epoch 22: loss did not improve from 0.63850\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6610 - accuracy: 0.8547 - dice_coef_loss: 0.6610\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6454 - accuracy: 0.8404 - dice_coef_loss: 0.6442\n",
      "Epoch 23: loss did not improve from 0.63850\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6454 - accuracy: 0.8404 - dice_coef_loss: 0.6442\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6510 - accuracy: 0.8583 - dice_coef_loss: 0.6510\n",
      "Epoch 24: loss did not improve from 0.63850\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6510 - accuracy: 0.8583 - dice_coef_loss: 0.6510\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6562 - accuracy: 0.8540 - dice_coef_loss: 0.6562\n",
      "Epoch 25: loss did not improve from 0.63850\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6562 - accuracy: 0.8540 - dice_coef_loss: 0.6562\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6479 - accuracy: 0.8597 - dice_coef_loss: 0.6479\n",
      "Epoch 26: loss did not improve from 0.63850\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6479 - accuracy: 0.8597 - dice_coef_loss: 0.6479\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6313 - accuracy: 0.8614 - dice_coef_loss: 0.6313\n",
      "Epoch 27: loss improved from 0.63850 to 0.63132, saving model to ./temp/cartesian_Dom/4/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.6313 - accuracy: 0.8614 - dice_coef_loss: 0.6313\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6415 - accuracy: 0.8697 - dice_coef_loss: 0.6415\n",
      "Epoch 28: loss did not improve from 0.63132\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6415 - accuracy: 0.8697 - dice_coef_loss: 0.6415\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6461 - accuracy: 0.8605 - dice_coef_loss: 0.6461\n",
      "Epoch 29: loss did not improve from 0.63132\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6461 - accuracy: 0.8605 - dice_coef_loss: 0.6461\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6520 - accuracy: 0.8700 - dice_coef_loss: 0.6516\n",
      "Epoch 30: loss did not improve from 0.63132\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6520 - accuracy: 0.8700 - dice_coef_loss: 0.6516\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6604 - accuracy: 0.8711 - dice_coef_loss: 0.6604\n",
      "Epoch 31: loss did not improve from 0.63132\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6604 - accuracy: 0.8711 - dice_coef_loss: 0.6604\n",
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - ETA: 0s - loss: 0.6343 - accuracy: 0.8769 - dice_coef_loss: 0.6343\n",
      "Epoch 32: loss did not improve from 0.63132\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6343 - accuracy: 0.8769 - dice_coef_loss: 0.6343\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6317 - accuracy: 0.8725 - dice_coef_loss: 0.6317\n",
      "Epoch 33: loss did not improve from 0.63132\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6317 - accuracy: 0.8725 - dice_coef_loss: 0.6317\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6450 - accuracy: 0.8818 - dice_coef_loss: 0.6450\n",
      "Epoch 34: loss did not improve from 0.63132\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6450 - accuracy: 0.8818 - dice_coef_loss: 0.6450\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6345 - accuracy: 0.8717 - dice_coef_loss: 0.6345\n",
      "Epoch 35: loss did not improve from 0.63132\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6345 - accuracy: 0.8717 - dice_coef_loss: 0.6345\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6439 - accuracy: 0.8745 - dice_coef_loss: 0.6439\n",
      "Epoch 36: loss did not improve from 0.63132\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6439 - accuracy: 0.8745 - dice_coef_loss: 0.6439\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6320 - accuracy: 0.8584 - dice_coef_loss: 0.6320\n",
      "Epoch 37: loss did not improve from 0.63132\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6320 - accuracy: 0.8584 - dice_coef_loss: 0.6320\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6510 - accuracy: 0.8834 - dice_coef_loss: 0.6522\n",
      "Epoch 38: loss did not improve from 0.63132\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6510 - accuracy: 0.8834 - dice_coef_loss: 0.6522\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6396 - accuracy: 0.8838 - dice_coef_loss: 0.6396\n",
      "Epoch 39: loss did not improve from 0.63132\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6396 - accuracy: 0.8838 - dice_coef_loss: 0.6396\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6304 - accuracy: 0.8824 - dice_coef_loss: 0.6304\n",
      "Epoch 40: loss improved from 0.63132 to 0.63037, saving model to ./temp/cartesian_Dom/4/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.6304 - accuracy: 0.8824 - dice_coef_loss: 0.6304\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6313 - accuracy: 0.8897 - dice_coef_loss: 0.6313\n",
      "Epoch 41: loss did not improve from 0.63037\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6313 - accuracy: 0.8897 - dice_coef_loss: 0.6313\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6481 - accuracy: 0.8862 - dice_coef_loss: 0.6481\n",
      "Epoch 42: loss did not improve from 0.63037\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6481 - accuracy: 0.8862 - dice_coef_loss: 0.6481\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6405 - accuracy: 0.8712 - dice_coef_loss: 0.6405\n",
      "Epoch 43: loss did not improve from 0.63037\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6405 - accuracy: 0.8712 - dice_coef_loss: 0.6405\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6271 - accuracy: 0.8904 - dice_coef_loss: 0.6271\n",
      "Epoch 44: loss improved from 0.63037 to 0.62708, saving model to ./temp/cartesian_Dom/4/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.6271 - accuracy: 0.8904 - dice_coef_loss: 0.6271\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6290 - accuracy: 0.8920 - dice_coef_loss: 0.6282\n",
      "Epoch 45: loss did not improve from 0.62708\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6290 - accuracy: 0.8920 - dice_coef_loss: 0.6282\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6244 - accuracy: 0.8901 - dice_coef_loss: 0.6244\n",
      "Epoch 46: loss improved from 0.62708 to 0.62442, saving model to ./temp/cartesian_Dom/4/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.6244 - accuracy: 0.8901 - dice_coef_loss: 0.6244\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6328 - accuracy: 0.8931 - dice_coef_loss: 0.6328\n",
      "Epoch 47: loss did not improve from 0.62442\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6328 - accuracy: 0.8931 - dice_coef_loss: 0.6328\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6344 - accuracy: 0.8884 - dice_coef_loss: 0.6344\n",
      "Epoch 48: loss did not improve from 0.62442\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6344 - accuracy: 0.8884 - dice_coef_loss: 0.6344\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6279 - accuracy: 0.8952 - dice_coef_loss: 0.6279\n",
      "Epoch 49: loss did not improve from 0.62442\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6279 - accuracy: 0.8952 - dice_coef_loss: 0.6279\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6348 - accuracy: 0.8942 - dice_coef_loss: 0.6348\n",
      "Epoch 50: loss did not improve from 0.62442\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6348 - accuracy: 0.8942 - dice_coef_loss: 0.6348\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6517 - accuracy: 0.8912 - dice_coef_loss: 0.6517\n",
      "Epoch 51: loss did not improve from 0.62442\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6517 - accuracy: 0.8912 - dice_coef_loss: 0.6517\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6377 - accuracy: 0.8960 - dice_coef_loss: 0.6365\n",
      "Epoch 52: loss did not improve from 0.62442\n",
      "100/100 [==============================] - 26s 257ms/step - loss: 0.6377 - accuracy: 0.8960 - dice_coef_loss: 0.6365\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6316 - accuracy: 0.8956 - dice_coef_loss: 0.6316\n",
      "Epoch 53: loss did not improve from 0.62442\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6316 - accuracy: 0.8956 - dice_coef_loss: 0.6316\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6331 - accuracy: 0.8913 - dice_coef_loss: 0.6331\n",
      "Epoch 54: loss did not improve from 0.62442\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6331 - accuracy: 0.8913 - dice_coef_loss: 0.6331\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6326 - accuracy: 0.8995 - dice_coef_loss: 0.6326\n",
      "Epoch 55: loss did not improve from 0.62442\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6326 - accuracy: 0.8995 - dice_coef_loss: 0.6326\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6513 - accuracy: 0.8917 - dice_coef_loss: 0.6513\n",
      "Epoch 56: loss did not improve from 0.62442\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6513 - accuracy: 0.8917 - dice_coef_loss: 0.6513\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6135 - accuracy: 0.8963 - dice_coef_loss: 0.6135\n",
      "Epoch 57: loss improved from 0.62442 to 0.61348, saving model to ./temp/cartesian_Dom/4/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 0.6135 - accuracy: 0.8963 - dice_coef_loss: 0.6135\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6333 - accuracy: 0.9105 - dice_coef_loss: 0.6333\n",
      "Epoch 58: loss did not improve from 0.61348\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6333 - accuracy: 0.9105 - dice_coef_loss: 0.6333\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6189 - accuracy: 0.8877 - dice_coef_loss: 0.6189\n",
      "Epoch 59: loss did not improve from 0.61348\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6189 - accuracy: 0.8877 - dice_coef_loss: 0.6189\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - ETA: 0s - loss: 0.6240 - accuracy: 0.8984 - dice_coef_loss: 0.6236\n",
      "Epoch 60: loss did not improve from 0.61348\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6240 - accuracy: 0.8984 - dice_coef_loss: 0.6236\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6422 - accuracy: 0.9066 - dice_coef_loss: 0.6422\n",
      "Epoch 61: loss did not improve from 0.61348\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6422 - accuracy: 0.9066 - dice_coef_loss: 0.6422\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6385 - accuracy: 0.8964 - dice_coef_loss: 0.6385\n",
      "Epoch 62: loss did not improve from 0.61348\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6385 - accuracy: 0.8964 - dice_coef_loss: 0.6385\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6307 - accuracy: 0.8996 - dice_coef_loss: 0.6307\n",
      "Epoch 63: loss did not improve from 0.61348\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6307 - accuracy: 0.8996 - dice_coef_loss: 0.6307\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6379 - accuracy: 0.9049 - dice_coef_loss: 0.6379\n",
      "Epoch 64: loss did not improve from 0.61348\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6379 - accuracy: 0.9049 - dice_coef_loss: 0.6379\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6216 - accuracy: 0.9114 - dice_coef_loss: 0.6216\n",
      "Epoch 65: loss did not improve from 0.61348\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6216 - accuracy: 0.9114 - dice_coef_loss: 0.6216\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6135 - accuracy: 0.9068 - dice_coef_loss: 0.6135\n",
      "Epoch 66: loss did not improve from 0.61348\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6135 - accuracy: 0.9068 - dice_coef_loss: 0.6135\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6142 - accuracy: 0.9054 - dice_coef_loss: 0.6146\n",
      "Epoch 67: loss did not improve from 0.61348\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6142 - accuracy: 0.9054 - dice_coef_loss: 0.6146\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6194 - accuracy: 0.9110 - dice_coef_loss: 0.6194\n",
      "Epoch 68: loss did not improve from 0.61348\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6194 - accuracy: 0.9110 - dice_coef_loss: 0.6194\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6362 - accuracy: 0.9050 - dice_coef_loss: 0.6362\n",
      "Epoch 69: loss did not improve from 0.61348\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6362 - accuracy: 0.9050 - dice_coef_loss: 0.6362\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6466 - accuracy: 0.8982 - dice_coef_loss: 0.6466\n",
      "Epoch 70: loss did not improve from 0.61348\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6466 - accuracy: 0.8982 - dice_coef_loss: 0.6466\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6155 - accuracy: 0.9090 - dice_coef_loss: 0.6155\n",
      "Epoch 71: loss did not improve from 0.61348\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.6155 - accuracy: 0.9090 - dice_coef_loss: 0.6155\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6059 - accuracy: 0.9088 - dice_coef_loss: 0.6059\n",
      "Epoch 72: loss improved from 0.61348 to 0.60591, saving model to ./temp/cartesian_Dom/4/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.6059 - accuracy: 0.9088 - dice_coef_loss: 0.6059\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6315 - accuracy: 0.9005 - dice_coef_loss: 0.6315\n",
      "Epoch 73: loss did not improve from 0.60591\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6315 - accuracy: 0.9005 - dice_coef_loss: 0.6315\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6285 - accuracy: 0.9067 - dice_coef_loss: 0.6285\n",
      "Epoch 74: loss did not improve from 0.60591\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6285 - accuracy: 0.9067 - dice_coef_loss: 0.6285\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6288 - accuracy: 0.9086 - dice_coef_loss: 0.6278\n",
      "Epoch 75: loss did not improve from 0.60591\n",
      "100/100 [==============================] - 26s 257ms/step - loss: 0.6288 - accuracy: 0.9086 - dice_coef_loss: 0.6278\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6295 - accuracy: 0.9057 - dice_coef_loss: 0.6295\n",
      "Epoch 76: loss did not improve from 0.60591\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6295 - accuracy: 0.9057 - dice_coef_loss: 0.6295\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6070 - accuracy: 0.9176 - dice_coef_loss: 0.6070\n",
      "Epoch 77: loss did not improve from 0.60591\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6070 - accuracy: 0.9176 - dice_coef_loss: 0.6070\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6212 - accuracy: 0.9126 - dice_coef_loss: 0.6212\n",
      "Epoch 78: loss did not improve from 0.60591\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6212 - accuracy: 0.9126 - dice_coef_loss: 0.6212\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6163 - accuracy: 0.9063 - dice_coef_loss: 0.6163\n",
      "Epoch 79: loss did not improve from 0.60591\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6163 - accuracy: 0.9063 - dice_coef_loss: 0.6163\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6283 - accuracy: 0.9085 - dice_coef_loss: 0.6283\n",
      "Epoch 80: loss did not improve from 0.60591\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6283 - accuracy: 0.9085 - dice_coef_loss: 0.6283\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6344 - accuracy: 0.9119 - dice_coef_loss: 0.6344\n",
      "Epoch 81: loss did not improve from 0.60591\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6344 - accuracy: 0.9119 - dice_coef_loss: 0.6344\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6308 - accuracy: 0.9161 - dice_coef_loss: 0.6315\n",
      "Epoch 82: loss did not improve from 0.60591\n",
      "100/100 [==============================] - 26s 257ms/step - loss: 0.6308 - accuracy: 0.9161 - dice_coef_loss: 0.6315\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6281 - accuracy: 0.9016 - dice_coef_loss: 0.6281\n",
      "Epoch 83: loss did not improve from 0.60591\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6281 - accuracy: 0.9016 - dice_coef_loss: 0.6281\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6185 - accuracy: 0.9154 - dice_coef_loss: 0.6185\n",
      "Epoch 84: loss did not improve from 0.60591\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6185 - accuracy: 0.9154 - dice_coef_loss: 0.6185\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6251 - accuracy: 0.9193 - dice_coef_loss: 0.6251\n",
      "Epoch 85: loss did not improve from 0.60591\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6251 - accuracy: 0.9193 - dice_coef_loss: 0.6251\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6110 - accuracy: 0.9106 - dice_coef_loss: 0.6110\n",
      "Epoch 86: loss did not improve from 0.60591\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6110 - accuracy: 0.9106 - dice_coef_loss: 0.6110\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6211 - accuracy: 0.9123 - dice_coef_loss: 0.6211\n",
      "Epoch 87: loss did not improve from 0.60591\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6211 - accuracy: 0.9123 - dice_coef_loss: 0.6211\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6292 - accuracy: 0.9126 - dice_coef_loss: 0.6292\n",
      "Epoch 88: loss did not improve from 0.60591\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6292 - accuracy: 0.9126 - dice_coef_loss: 0.6292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6155 - accuracy: 0.9115 - dice_coef_loss: 0.6161\n",
      "Epoch 89: loss did not improve from 0.60591\n",
      "100/100 [==============================] - 26s 257ms/step - loss: 0.6155 - accuracy: 0.9115 - dice_coef_loss: 0.6161\n",
      "Epoch 90/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6203 - accuracy: 0.9105 - dice_coef_loss: 0.6203\n",
      "Epoch 90: loss did not improve from 0.60591\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6203 - accuracy: 0.9105 - dice_coef_loss: 0.6203\n",
      "Epoch 91/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6198 - accuracy: 0.9183 - dice_coef_loss: 0.6198\n",
      "Epoch 91: loss did not improve from 0.60591\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6198 - accuracy: 0.9183 - dice_coef_loss: 0.6198\n",
      "Epoch 92/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6407 - accuracy: 0.9171 - dice_coef_loss: 0.6407\n",
      "Epoch 92: loss did not improve from 0.60591\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6407 - accuracy: 0.9171 - dice_coef_loss: 0.6407\n",
      "Epoch 93/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6124 - accuracy: 0.9172 - dice_coef_loss: 0.6124\n",
      "Epoch 93: loss did not improve from 0.60591\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6124 - accuracy: 0.9172 - dice_coef_loss: 0.6124\n",
      "Epoch 94/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6324 - accuracy: 0.9103 - dice_coef_loss: 0.6324\n",
      "Epoch 94: loss did not improve from 0.60591\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6324 - accuracy: 0.9103 - dice_coef_loss: 0.6324\n",
      "Epoch 95/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6098 - accuracy: 0.9226 - dice_coef_loss: 0.6098\n",
      "Epoch 95: loss did not improve from 0.60591\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6098 - accuracy: 0.9226 - dice_coef_loss: 0.6098\n",
      "Epoch 96/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6073 - accuracy: 0.9215 - dice_coef_loss: 0.6073\n",
      "Epoch 96: loss did not improve from 0.60591\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6073 - accuracy: 0.9215 - dice_coef_loss: 0.6073\n",
      "Epoch 97/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6275 - accuracy: 0.9186 - dice_coef_loss: 0.6284\n",
      "Epoch 97: loss did not improve from 0.60591\n",
      "100/100 [==============================] - 26s 257ms/step - loss: 0.6275 - accuracy: 0.9186 - dice_coef_loss: 0.6284\n",
      "Epoch 98/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5935 - accuracy: 0.9163 - dice_coef_loss: 0.5935\n",
      "Epoch 98: loss improved from 0.60591 to 0.59346, saving model to ./temp/cartesian_Dom/4/checkpoint.hdf5\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 0.5935 - accuracy: 0.9163 - dice_coef_loss: 0.5935\n",
      "Epoch 99/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6154 - accuracy: 0.9226 - dice_coef_loss: 0.6154\n",
      "Epoch 99: loss did not improve from 0.59346\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6154 - accuracy: 0.9226 - dice_coef_loss: 0.6154\n",
      "Epoch 100/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6182 - accuracy: 0.9241 - dice_coef_loss: 0.6182\n",
      "Epoch 100: loss did not improve from 0.59346\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6182 - accuracy: 0.9241 - dice_coef_loss: 0.6182\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "PARAM_BETA_TEST_NUM = 6\n",
    "data_gen_args = dict(rotation_range = 50,      # TODO: improve the data augmentation\n",
    "                width_shift_range =0.2,\n",
    "                height_shift_range =0.2,\n",
    "                shear_range = 0.35,\n",
    "                zoom_range = 0.05,\n",
    "                horizontal_flip = True,\n",
    "                fill_mode = 'nearest',\n",
    "                rescale = 1./255)\n",
    "#Train polar models\n",
    "working_parent_folder = PARAM_PATH_TEMP_POLAR\n",
    "Polar_history = []\n",
    "for i in range(K):\n",
    "    working_test_folder_i = os.path.join(working_parent_folder, str(i), PARAM_SUB_FOLDER_POLAR)\n",
    "    temp_folder_path = os.path.join(working_parent_folder,'temp')\n",
    "    os.mkdir(temp_folder_path)\n",
    "    for j in range(K):\n",
    "        if i != j:\n",
    "            for subfolder_name in ['image','label']:\n",
    "                subfolder_path = os.path.join(working_parent_folder,str(j),'polar',subfolder_name)\n",
    "                temp_subfolder_path = os.path.join(temp_folder_path,subfolder_name)\n",
    "                for root, dirs, files in os.walk(subfolder_path):\n",
    "                    for file in files:\n",
    "                        src_file = os.path.join(root, file)\n",
    "                        dest_file = os.path.join(temp_subfolder_path,os.path.relpath(src_file, subfolder_path))\n",
    "                        os.makedirs(os.path.dirname(dest_file), exist_ok=True)\n",
    "                        shutil.copy(src_file, dest_file)\n",
    "    test_gene = trainGenerator(batch_size, temp_folder_path, PARAM_IMG_FOLDER, PARAM_MSK_FOLDER, data_gen_args)\n",
    "    model = unet(PARAM_BETA1[PARAM_BETA_TEST_NUM], PARAM_BETA2[PARAM_BETA_TEST_NUM]) \n",
    "    model_checkpoint = ModelCheckpoint(os.path.join(working_parent_folder,str(i),'checkpoint.hdf5'), monitor = 'loss', verbose=1, save_best_only=True)\n",
    "    test_run = model.fit(test_gene, verbose = 1, steps_per_epoch = 100, epochs = 100, callbacks = [model_checkpoint])\n",
    "    Polar_history.append(test_run)\n",
    "    shutil.rmtree(temp_folder_path)\n",
    "    \n",
    "#Train cartesian models\n",
    "data_gen_args = dict(rotation_range = 80,      # TODO: improve the data augmentation\n",
    "                width_shift_range =0.02,\n",
    "                height_shift_range =0.02,\n",
    "                shear_range = 0.35,\n",
    "                zoom_range = 0.075,\n",
    "                horizontal_flip = True,\n",
    "                fill_mode = 'nearest',\n",
    "                rescale = 1./255)\n",
    "working_parent_folder = PARAM_PATH_TEMP_CARTE\n",
    "Cartesian_history = []\n",
    "for i in range(K):\n",
    "    working_test_folder_i = os.path.join(working_parent_folder, str(i), PARAM_SUB_FOLDER_CARTE)\n",
    "    temp_folder_path = os.path.join(working_parent_folder,'temp')\n",
    "    os.mkdir(temp_folder_path)\n",
    "    for j in range(K):\n",
    "        if i != j:\n",
    "            for subfolder_name in ['image','label']:\n",
    "                subfolder_path = os.path.join(working_parent_folder,str(j),'carte',subfolder_name)\n",
    "                temp_subfolder_path = os.path.join(temp_folder_path,subfolder_name)\n",
    "                for root, dirs, files in os.walk(subfolder_path):\n",
    "                    for file in files:\n",
    "                        src_file = os.path.join(root, file)\n",
    "                        dest_file = os.path.join(temp_subfolder_path,os.path.relpath(src_file, subfolder_path))\n",
    "                        os.makedirs(os.path.dirname(dest_file), exist_ok=True)\n",
    "                        shutil.copy(src_file, dest_file)\n",
    "    test_gene = trainGenerator(batch_size, temp_folder_path, PARAM_IMG_FOLDER, PARAM_MSK_FOLDER, data_gen_args)\n",
    "    model = unet(PARAM_BETA1[PARAM_BETA_TEST_NUM], PARAM_BETA2[PARAM_BETA_TEST_NUM]) \n",
    "    model_checkpoint = ModelCheckpoint(os.path.join(working_parent_folder,str(i),'checkpoint.hdf5'), monitor = 'loss', verbose=1, save_best_only=True)\n",
    "    test_run = model.fit(test_gene, verbose = 1, steps_per_epoch = 100, epochs = 100, callbacks = [model_checkpoint])\n",
    "    Cartesian_history.append(test_run)\n",
    "    shutil.rmtree(temp_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last block there trained all 10 models, the 10 models' checkpoints are saved in the temp folder of each test model. For example, if stored in folder `temp/polar_Dom/0/`, the model is assumed to be trained on all folders except the current folder -- namingly, `folder 1,2,3,4`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for single_run in Polar_history:\n",
    "    plt.plot(single_run.history['loss'])\n",
    "    plt.plot(single_run.history['accuracy'])\n",
    "    plt.title('Polar Run')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['loss', 'accuracy'], loc='upper left')\n",
    "    plt.show()\n",
    "print('________________________________________________')\n",
    "for single_run in Cartesian_history:\n",
    "    plt.plot(single_run.history['loss'])\n",
    "    plt.plot(single_run.history['accuracy'])\n",
    "    plt.title('Cartesian Run')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['loss', 'accuracy'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, all of the ten models from each group are trained, the next step is to load each of them and predict using every other images that are not in the training set as inputs. Note that `polar dominant models` should only take `polar` images as inputs and `cartesian dominant models` should only take `cartesian` images as inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block is supposed to get and check the number of images and masks in the source folder. *could be moved to previous blocks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WE have a matching number of images and labels here, count =  7404\n"
     ]
    }
   ],
   "source": [
    "#Predict using polar dominant images\n",
    "image_extension = 'tif'\n",
    "image_count = 0\n",
    "img_pattern = os.path.join(PARAM_PATH_POLAR, PARAM_IMG_FOLDER, f'*.{image_extension}')\n",
    "image_files = glob.glob(img_pattern)\n",
    "msk_pattern = os.path.join(PARAM_PATH_POLAR, PARAM_MSK_FOLDER, f'*.{image_extension}')\n",
    "msk_files = glob.glob(msk_pattern)\n",
    "#length matching check\n",
    "if len(image_files) == len(msk_files):\n",
    "    print('WE have a matching number of images and labels here, count = ', len(msk_files))\n",
    "else:\n",
    "    print('Something is wrong with the original data set. [Unmatching number of images/masks.]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5 #if we don't want to train again, run this\n",
    "PARAM_BETA_TEST_NUM = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a numpy array to showcase the distribution of files\n",
    "\n",
    "n = len(image_files)\n",
    "m = K * 2\n",
    "\n",
    "filematrix = np.zeros((n,m))\n",
    "for img_type in ['polar', 'carte']:\n",
    "    #\n",
    "    img_extenstion = 'tif'\n",
    "    #\n",
    "    for_counter = 0\n",
    "    if img_type == 'polar':\n",
    "        working_parent_folder = PARAM_PATH_TEMP_POLAR\n",
    "    else:\n",
    "        working_parent_folder = PARAM_PATH_TEMP_CARTE\n",
    "        for_counter += 1\n",
    "    for i in range(K):\n",
    "        image_path = os.path.join(working_parent_folder, str(i), img_type, PARAM_IMG_FOLDER)\n",
    "        img_pattern = os.path.join(image_path, f'*.{image_extension}')\n",
    "        image_files = glob.glob(img_pattern)\n",
    "        for file_name in image_files:\n",
    "            file_name_shorten = os.path.basename(file_name)\n",
    "            file_name_raw, ext = os.path.splitext(file_name_shorten)\n",
    "            filematrix[int(file_name_raw),i + for_counter * K] = 1\n",
    "        #number_of_ones = np.count_nonzero(filematrix == 1)\n",
    "        #print(number_of_ones)  #uncomment this line when png file is not satisfactory, we can track the number of ones during each step  \n",
    "plt.imsave('filematrix.png', filematrix, cmap = 'binary')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we will have a .png file saved to the root directory, zoom in and we should be able to see only one pixel is colored black in each row.\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_filematrix = np.copy(filematrix)\n",
    "for row in augmented_filematrix:\n",
    "    for for_counter in range(2):\n",
    "        zero_count = 0\n",
    "        for index in range(K):\n",
    "            real_index = index + for_counter * K\n",
    "            if row[real_index] == 0:\n",
    "                zero_count += 1\n",
    "        if zero_count == 5:\n",
    "            row[for_counter*K:for_counter*K + 5] = 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_indices, col_indices = np.where(augmented_filematrix == 1)\n",
    "indices = list(zip(row_indices, col_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(image1, image2):\n",
    "    # Ensure the input images have the same shape\n",
    "    smooth = 1\n",
    "    if image1.shape != image2.shape:\n",
    "        raise ValueError(\"Input images must have the same shape.\")\n",
    "    img1_f = (image1-1)*(-1)\n",
    "    img2_f = (image2-1)*(-1)\n",
    "    # Calculate the intersection (logical AND) between the two binary images\n",
    "    intersection_o = np.logical_and(image1, image2).sum()\n",
    "    intersection_f = np.logical_and(img1_f, img2_f).sum()\n",
    "    # Calculate the sum of pixels in each image\n",
    "    sum_image1_o = image1.sum()\n",
    "    sum_image2_o = image2.sum()\n",
    "    sum_image1_f = img1_f.sum()\n",
    "    sum_image2_f = img2_f.sum()\n",
    "    #if(sum_image1 == sum_image2 == 0):#I'm not so sure about this o.0\n",
    "        #return 1.0\n",
    "    # Calculate the Dice coefficient\n",
    "    dice = (2.0 * intersection_o + smooth) / (sum_image1_o + sum_image2_o + smooth)\n",
    "    dice_f = (2.0 * intersection_f + smooth) / (sum_image1_f + sum_image2_f + smooth)\n",
    "    dice_avg = (dice + dice_f) / 2.0\n",
    "    return dice_avg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next block, we will loop through all sub folders, register all images appears in the training set, and reverse, and take the images from that specific big class, directly from the original data folder, and throw them to a temporary folder.\n",
    "\n",
    "Now that we have the `filematrix`, we can go over each coloumn, pull out files into a temporary folder and make a round of prediction using that model if the value is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fcd01b07f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fcd01b07f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Done with folder  0\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fcd01a12710> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fcd01a12710> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Done with folder  1\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fca8b3b53b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fca8b3b53b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Done with folder  2\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fcd0246e9e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fcd0246e9e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Done with folder  3\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fcd01e79680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fcd01e79680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Done with folder  4\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fcd026a3710> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fcd026a3710> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Done with folder  5\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fcdd00e45f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fcdd00e45f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Done with folder  6\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fcb0d5b80e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fcb0d5b80e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Done with folder  7\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fcd03a9d0e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fcd03a9d0e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with folder  8\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fcb0d597200> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fcb0d597200> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Done with folder  9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scorematrix = np.zeros((n,m))\n",
    "for img_type in ['polar', 'carte']:\n",
    "    for_counter = 0\n",
    "    if img_type == 'polar':\n",
    "        working_parent_folder = PARAM_PATH_TEMP_POLAR\n",
    "        src_folder = PARAM_PATH_POLAR\n",
    "    else:\n",
    "        working_parent_folder = PARAM_PATH_TEMP_CARTE\n",
    "        src_folder = PARAM_PATH_CARTE\n",
    "        for_counter += 1\n",
    "    for i in range(K):\n",
    "        current_folder_index = i + for_counter * K\n",
    "        temp_test_folder_name = 'temptest'\n",
    "        #print(working_parent_folder)\n",
    "        if os.path.exists(temp_test_folder_name):\n",
    "            shutil.rmtree(temp_test_folder_name)\n",
    "        temp_test_img_folder = os.path.join(temp_test_folder_name,PARAM_IMG_FOLDER)\n",
    "        temp_test_msk_folder = os.path.join(temp_test_folder_name,PARAM_MSK_FOLDER)\n",
    "        os.makedirs(temp_test_img_folder)\n",
    "        os.makedirs(temp_test_msk_folder)\n",
    "        for indice in indices:\n",
    "            if indice[1] == current_folder_index:\n",
    "                img_name = str(indice[0]) + '.' + img_extenstion\n",
    "                src = os.path.join(src_folder,PARAM_IMG_FOLDER,img_name)\n",
    "                shutil.copy2(src, temp_test_img_folder)\n",
    "                src = os.path.join(src_folder,PARAM_MSK_FOLDER,img_name)\n",
    "                shutil.copy2(src, temp_test_msk_folder)\n",
    "        \n",
    "        model_path = os.path.join(working_parent_folder, str(i), 'checkpoint.hdf5')\n",
    "        current_model = unet(PARAM_BETA1[PARAM_BETA_TEST_NUM], PARAM_BETA2[PARAM_BETA_TEST_NUM])\n",
    "        current_model.load_weights(model_path) \n",
    "        for test_image_name in os.listdir(temp_test_img_folder):\n",
    "            test_image_name_raw, ext = os.path.splitext(test_image_name)\n",
    "            image_path = os.path.join(temp_test_img_folder, test_image_name)\n",
    "            ground_truth_mask_path = os.path.join(temp_test_msk_folder, test_image_name)\n",
    "            \n",
    "            test_image = cv2.imread(image_path)\n",
    "            test_image = cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB)\n",
    "            test_image = test_image / 255.0\n",
    "            test_image = np.expand_dims(test_image,axis = 0)\n",
    "\n",
    "            ground_truth_mask = cv2.imread(ground_truth_mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "            ground_truth_mask = ground_truth_mask / 255.0\n",
    "            ground_truth_mask = ground_truth_mask.astype(np.uint8)\n",
    "            \n",
    "            prediction = current_model.predict(test_image, verbose = 0)\n",
    "            \n",
    "            threshold = 0.5\n",
    "            binary_mask = (prediction > threshold).astype(np.uint8)\n",
    "            binary_mask = binary_mask[0,:,:,0]\n",
    "            dice = dice_coefficient(ground_truth_mask, binary_mask)\n",
    "            scorematrix[int(test_image_name_raw), current_folder_index] = dice\n",
    "        print('Done with folder ', current_folder_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now compare the scores of each row and see which one big group it should belong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This block contains the code to decide if one img should stay in its own group by looking for the absolute maximum performance\n",
    "decision = np.zeros(n)\n",
    "current_i = 0\n",
    "for score_row, file_row in zip(scorematrix, filematrix):\n",
    "    if(np.argmax(score_row) != np.argmax(file_row)):\n",
    "        if(np.argmax(file_row) >= K):#need to move from polar to cartesian\n",
    "            decision[current_i] = 1\n",
    "        else:\n",
    "            decision[current_i] = -1\n",
    "    current_i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of moving: polar -> cartesian:  1906\n",
      "The number of moving: cartesian -> polar:  2774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fca8b15f490>]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "number_of_ones_decision = np.count_nonzero(decision == 1)\n",
    "print('The number of moving: polar -> cartesian: ', number_of_ones_decision)\n",
    "number_of_minusones_decision = np.count_nonzero(decision == -1)\n",
    "print('The number of moving: cartesian -> polar: ', number_of_minusones_decision)\n",
    "plt.plot(np.sort(decision),'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actually, what if I just make the result into another list of dice score, and sort on this numpy list to do the next round of training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7404, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAfHElEQVR4nO3deZhcdZ3v8fe3u1O9793pvbMRAlmAhDaQUcCL4iTggNfl3uDooILRGXl0xjvXi48Oz73MH47ceVzmGUbNIF4eRw2KW0ajqEgUVCAJBMie7iZJd6eT3ve963f/qJNQNN3p6qS6Ty2f1/PU0+ecOqn6JBU+/PI7p84x5xwiIhL/UvwOICIi0aFCFxFJECp0EZEEoUIXEUkQKnQRkQSR5tcbl5SUuKVLl/r19iIicWnfvn0dzrnS6Z7zrdCXLl3K3r17/Xp7EZG4ZGYnZ3pOUy4iIglChS4ikiBU6CIiCUKFLiKSICIqdDPbbGZHzazezO6b5vkPmVm7me33HvdEP6qIiFzIrGe5mFkq8BBwC9AM7DGznc65Q1N2fcw5d+88ZBQRkQhEMkLfCNQ75xqdc2PADuCO+Y0lIiJzFcl56FVAU9h6M3DdNPu9x8xuBI4Bf+eca5q6g5ltA7YB1NbWzj2tiIjPzvSOsGPPKYLBi7/0+NuuLOPqmoLohfJE64tF/wl8zzk3amYfAx4Fbp66k3NuO7AdoK6uThdiF5G48/i+Jr7ym+OYXfxrLM7L8K3QW4CasPVqb9t5zrnOsNWHgQcvPZqISOzpGhwnO5DKwQc2+x3lDSKZQ98DrDSzZWYWALYCO8N3MLOKsNXbgcPRiygiEjsa2geoKMj0O8a0Zh2hO+cmzOxe4AkgFXjEOXfQzB4A9jrndgKfNLPbgQmgC/jQPGYWEVlwA6MTPPrHE/zuWDt//dYVfseZlvl1T9G6ujqni3OJSKwaGZ/kyJl+DrT08lJTD784cIaB0QluWFnCw3fVkZ6W6ksuM9vnnKub7jnfrrYoIuI35xzt/aOc6hqiqXuIU53DnOgc5NDpPurbB5j0zmTJz1zELavL+OCmJWyoLfQ59cxU6CKSsCaDju6hMdr6RmnpGQ4Vt/c4V+Ij48Hz+5tBeV4GV5Tn8o41ZaypzGdtVR5VBZnYpZzWskBU6CISV4JeSXcMjNHeP0rHwOhrP88vh57rGhxl6uni2YFUaoqyWFaSzU2Xl1JbnEVNURa1RVlUFWSSscifqZRoUKGLiO+CQUfP8Pjry7k/VNAd/WPez9D2zsGx81Mh4QJpKZTmpFOSE6CqIIOrq/MpzU2nJCf0qCrMpLYoi8KsRXEx2r4YKnQRmRfOOQZGJ+gcGON07/D5kfPU0u4YGKVzYIyJ6Uo6NYWSnAAluelU5GewrupcSYe2leakU+KVdl5GWsIWdaRU6CISsYnJIF1DY3T0j9E9NEbn4BhtfSO094fmqLsGx84/uofGGJ98Y0kvSrXzo+ayvAzWVOZRkpN+fjR9/mdOOnmZKum5UKGLJLnhsUlveiNUzOcebd6IunNwlO7BMbqHxukdHp/2NQKpKZTnZ1Cam051YRZXVxdQmB2gKHsRRdnpVOZnsDgvVNT5mYk75eE3FbpIAhqfDNI9GBpBdw6M0TX02kj6bN8IZ84vjzIwOvGGX28GxdkBSnLSKc4JUF1YQGHWIgqyApTmnNueTmHWIhbnZmgkHSNU6CJxZHRikrO9o5zuHQ4Vc+8ILT2h+enOwTF6hkIj6Y6BUab7zmAgNYVSbz768rJcblhZyuK80PRGae5rj6KsAGmpuqFZvFGhi/jMOUf30Lh3mt1rpdw9FDr1rrl7mDN9w7T1haZBpsrNSKMsL4Oi7ADLS3IoyFpEWV5G6GBhdoDC7ADF2QFKczXdkehU6CLzrG9knJMdQ7T0DJ+f7mjpHj4/sj7TO8LYZHDaX5ubnkZVYSbl+RmsrsijqiCLioIMKvMzKc8PHVTMzVi0wL8jiVUqdJFLMBl0nO0b4WzfCD3eVEdT9zAnOwc52TnEyc5BuodefyAxLcUoy8tgSXEW62sLKM/PoCw3dNCwKCtAftYiirIDFGYF4vpLLrLwVOgiFzAwOsGpziGau4c40zdCa29o3rq1d5jTPaGfU0/NSzGoyM9kaUkWm9dWsKQ4i6XFWVQXZlGen0FRVoCUFE17SPSp0CXpjU0EOdk5SEP7AA3tg5zqHOLVzkGauoZo7R153b6LUo3FuRmU52dwTU0Bt11VQXVhJuXeHHZRdoDy/AzfrsQnyU2FLkmjd3ichvYB6tsGQuXdNkhj+wAnu4Ze91Xykpx0lhZnsWl5MSsW57C0OJuaokwq8jMpztboWmKXCl0SSjDoON07TEP7IA1tA9S3D9DQFhp5dwy8dobIolRjWUk2q8pzuXVdBSsWZ7OiNIflpTnkpOs/C4lP+psrcWlkfJJXOwbPj7TPjbwbOwZedznU/MxFXLY4h5uvKGVFaU7osTiHmsJMnWctCUeFLjHt3F1jDrf2hQrbm+du6h46/8UZM6guzGRFaQ6bVhR7xZ3NisU5FGcHdN61JA0VusSM/pFxDreGbvl18HQfB1p6X3fXmMxFqSwtyebqmgLevaHq/Ih7eWm2Tu8TQYUuPhkZn2TfyW4OtPRyuLWPl5t7aewYPP98SU46a6vy+PM1ZayuzOfKilxqCrN0QFLkAlTosiAGRyfYe7Kb3UfbeK6xi+Nt/efP367Iz2BNZT7v3lDFmsp81lTmsTgvw+fEIvFHhS7zYmR8khdOdvOnxk7+2NDJS009TAQdGYtS2FBbyN1vWc51y4q4piZ0mVURuXQqdIma5u4hnn+1iyePtPHk4bOMjAdJMVhXlc9Hb1zOpuXF1C0tJCugv3Yi80H/ZclFcc7R2jvCy809PPdqF88c7+B42wAQmv/+r+urePuVZbxpWRF5uniUyIJQocuc9A6N84sDrTz6p5Mcbu0DID0thWuXFLJ1Yy2blhezqjyXVB28FFlwKnSZVd/IOM8c7+AnL7aw+2g7Y5NBVpRm8w/vXM362gLWVObp2iUiMUCFLtMaHpvkV4fO8OMXW3j6eAeTQUdpbjofuH4J71pfybqqfH1hRyTGqNDlPOcczzZ28fi+Zn55oJXBsUmqCjL56A3LuenyUjYuK9JUikgMU6EL/SPj7HqllUeeOcHRs/3kpqfxzqsqedf6Kq5bVqQv84jECRV6khqbCPLbI238dH8LTx5pY2wiyBXluTz43qu4/epKfZVeJA6p0JPMiY5BfvhCM4/taaKtf5SSnADv31jL7ddUsr6mQPPiInEsokI3s83AV4FU4GHn3D/NsN97gMeBNznn9kYtpVyS/pFxfvZyKz/Y28QLp3pIMXjzZSV84d3ruOnyUl1GViRBzFroZpYKPATcAjQDe8xsp3Pu0JT9coFPAc/NR1CZu97hcf7tqXq+9YcTjE0Gubwsh89sXsV7NlRTpmuliCScSEboG4F651wjgJntAO4ADk3Z7x+BLwL/M6oJZc6CQcejfzrBV35znL6Rcd69vpr3bKhi04piTamIJLBICr0KaApbbwauC9/BzDYANc65n5vZjIVuZtuAbQC1tbVzTysXNBl0/HBfM9/64wkOt/Zxw8oS7ttyBWsq8/2OJiIL4JIPippZCvAl4EOz7euc2w5sB6irq3Oz7C5z8PyrXdz/0wMcOdPPlRV5/PP7ruY9G6o0IhdJIpEUegtQE7Ze7W07JxdYC+z2yqMc2Glmt+vA6Pw73TPMl359jB+/2EJ5Xgb/cud6/uKqChW5SBKKpND3ACvNbBmhIt8KvP/ck865XqDk3LqZ7Qb+XmU+v4bGJvjqk8f55tOvAvDB65fwqbet1LXFRZLYrIXunJsws3uBJwidtviIc+6gmT0A7HXO7ZzvkPJ6+5t6+B/f309D+yDvu7aaT75tJTVFWX7HEhGfRTSH7pzbBeyasu3+GfZ966XHkpnsPtrGJ77zArkZi/jOPdfx5stKZv9FIpIU9E3ROOGcY/vvG/niL49weVku3/rwm6jIz/Q7lojEEBV6HPhTQydf/10DvzvWzuY15Xz5v19DZkDXWhGR11Ohx7CxiSBf/s0xvra7gZKcAJ/ZvIqP37hCVz8UkWmp0GPUyc5BPvDN52jqGmbrm2r4h3euJjtdH5eIzEwNEYMOtPRy73dfoGdonEc/spGbLi/1O5KIxAFdZi/GvNTUw53bn2V0Isj2D9apzEUkYhqhx5DfH2vn4/+xj5KcdHZsu57KAp3FIiKR0wg9Ruw50cXHvr2PyoJMHvuYylxE5k4j9BjwUlMPH/7WHioKMvjeR6+nNDfd70giEoc0QvfZ3hNd/NUjz1OUHeC796jMReTiqdB9tPtoG+9/+DnyMtP4zj3XUZ6vuwiJyMXTlItPnn81NGd+WWkO3757I8U5GpmLyKXRCN0H/SPj/N1j+6ksyFSZi0jUaITugwf+8xCtvcN8/2ObVOYiEjUaoS+wg6d7+cG+Zj5643Lqlhb5HUdEEogKfQG19Y3wwW8+T2HWIv7mrZf5HUdEEoymXBaIc477f3qQrsEx/uPu68jPXOR3JBFJMBqhL5DvPHeKXx48w+duvZK3rNRdhkQk+lToC+AHe5v4/E8OsKYyj7vfsszvOCKSoFTo8+xk5yCf/8kBNtQWsGPb9bo5hYjMGxX6PHLO8ckd+wmkpfBvf3ktuRmaNxeR+aNCnyfOOR584igvNfXw+duu1Nf6RWTeqdDnyRMHz/K13Q3cubGG/1ZX43ccEUkCKvR5MDEZ5Au/OMyK0mz+8Y61mGneXETmnwp9HnxtdwMnO4f4+3esIi1Vf8QisjDUNlHWOTDKvz/dyM1XLGbz2nK/44hIElGhR9mjfzpJ38gEn77lck21iMiCUqFHUVPXEF/f3cBt6ypYW5XvdxwRSTIq9Ch69I8nmAgG+dxtV/odRUSSkAo9So6e6efhZ17ljmuqqCzI9DuOiCQhFXqU/OtT9QRSU/jsliv8jiIiSSqiQjezzWZ21Mzqzey+aZ7/uJm9Ymb7zewZM1sd/aixq6lriCcOnOH919WyOE/fCBURf8xa6GaWCjwEbAFWA3dOU9jfdc6tc85dAzwIfCnaQWPZvz/dCKArKYqIryIZoW8E6p1zjc65MWAHcEf4Ds65vrDVbMBFL2Js6x4cY8fzTWxZV05NUZbfcUQkiUVyx6IqoClsvRm4bupOZvYJ4NNAALh5uhcys23ANoDa2tq5Zo1JP3v5NGOTQY3ORcR3UTso6px7yDm3AvhfwOdn2Ge7c67OOVdXWloarbf21e6j7RRnB1in885FxGeRFHoLEH65wGpv20x2AO+6hExxo29knN8fb+f2ayr1rVAR8V0khb4HWGlmy8wsAGwFdobvYGYrw1ZvA45HL2Ls+vXBs4xPOm5bV+F3FBGR2efQnXMTZnYv8ASQCjzinDtoZg8Ae51zO4F7zeztwDjQDdw1n6Fjxa5XWqnMz+DaJYV+RxERieigKM65XcCuKdvuD1v+VJRzxby+kXGePt7BBzct0XSLiMQEfVP0Ij15+Cxjk0FuXadL5IpIbFChX6Sfv3yG8rwM1tdoukVEYoMK/SL0e2e3bFlXTkqKpltEJDao0C/Ck4fbGJsI6uwWEYkpKvSL8PNXWinLS2dDraZbRCR2qNDnqHNglN1H29iytkLTLSISU1Toc/S7Y+2MTzr+4mpNt4hIbFGhz9HTxzsoyg7o7BYRiTkq9DkIBh2/P9bOjStLNN0iIjFHhT4HL7f00jk4xltXLfY7iojIG6jQ5+CVll4ANi4r8jmJiMgbqdDnoKFtgOxAKhX5um+oiMQeFfoc1LcNsGJxji7GJSIxSYUeofHJIAdP93JZaY7fUUREpqVCj9AXdh2he2icLfq6v4jEKBV6BCYmg/z4xWauqSngltVlfscREZmWCj0CzzZ20T00zj03LPM7iojIjFToEfj5K6fJDqTy9is1OheR2KVCn4Vzjt1H27lpVSkZi1L9jiMiMiMV+ixOdQ3R2jvCphUlfkcREbkgFfosfnXwLABvXlHscxIRkQtToc/iySNnWVWWy3Kdfy4iMU6FfgHOOY6e6eeamgK/o4iIzEqFfgGvdgzSPTTO2qo8v6OIiMxKhX4BzzZ2AfCWlaU+JxERmZ0K/QIOtfaSm5HG0uIsv6OIiMxKhX4BR1r7uUxXVxSROKFCn8Hw2CR7T3ZzdXWB31FERCKiQp/Bj15sBuCmVZo/F5H4oEKfwbONXZTlpXOTDoiKSJyIqNDNbLOZHTWzejO7b5rnP21mh8zsZTN70syWRD/qwjrVOcjKxbmkpGj+XETiw6yFbmapwEPAFmA1cKeZrZ6y24tAnXPuKuBx4MFoB11oJ7uGqNXZLSISRyIZoW8E6p1zjc65MWAHcEf4Ds65p5xzQ97qs0B1dGMurN7hcXqGxllSpEIXkfgRSaFXAU1h683etpncDfziUkL5raV7GIDqQhW6iMSPtGi+mJl9AKgDbprh+W3ANoDa2tpovnVUdQyMArA4L93nJCIikYtkhN4C1IStV3vbXsfM3g58DrjdOTc63Qs557Y75+qcc3WlpbF79si5Qi/ODvicREQkcpEU+h5gpZktM7MAsBXYGb6Dma0HvkGozNuiH3NhHTs7AMDivAyfk4iIRG7WQnfOTQD3Ak8Ah4HvO+cOmtkDZna7t9v/BXKAH5jZfjPbOcPLxYVjZ/vJzUgjJz2qM1IiIvMqosZyzu0Cdk3Zdn/Y8tujnMtXB0/38l9WLfY7hojInOibolMMjU1wtm+UVeW5fkcREZkTFfoUR8/0A7C0ONvnJCIic6NCn+JQax8AV1Xn+5xERGRuVOhT7DvRTUlOgKqCTL+jiIjMiQp9ipNdQ1xepotyiUj8UaGHcc5xsnOQinyNzkUk/qjQw3QOjtExMMbaqjy/o4iIzJkKPczJztAFIzVCF5F4pEIP89SR0FULrq7RGS4iEn9U6GF2HWhleUm2RugiEpdU6J6R8Uka2we5dV2F31FERC6KCt1zbv5ct50TkXilQve80tILwLVLCn1OIiJycVTonmNn+0kxqNFt50QkTqnQPS+e6mZtVT6BNP2RiEh8Unt5GtoHWVOpLxSJSPxSoQO9w+N0DY5RW6RL5opI/FKhAwe8A6JXVOimFiISv1TowPGzoZtarKnQlIuIxC8VOtDUPUzmolRKc9P9jiIictFU6EBL9zBVhZmY6RroIhK/VOhAc8+Q7lAkInFPhc5rI3QRkXiW9IU+ODpB99C4RugiEveSvtBbeoYBqNYIXUTinAq9W4UuIokh6Qu92RuhVxXoolwiEt+SvtDP9A6TmmI6B11E4l7SF/qJjiEqCzJITdE56CIS35K+0BvaB1i5WNdwEZH4l9SFHgw6Xu0YZHmJrrIoIvEvokI3s81mdtTM6s3svmmev9HMXjCzCTN7b/Rjzo/TvcOMTgRZXprjdxQRkUs2a6GbWSrwELAFWA3caWarp+x2CvgQ8N1oB5xPje2DACwv1QhdROJfWgT7bATqnXONAGa2A7gDOHRuB+fcCe+54DxknDeN7QMAmnIRkYQQyZRLFdAUtt7sbZszM9tmZnvNbG97e/vFvERUNXYMkpueplMWRSQhLOhBUefcdudcnXOurrS0dCHfeloN7QMsL83WZXNFJCFEUugtQE3YerW3Le7Vtw2wQgdERSRBRFLoe4CVZrbMzALAVmDn/Maaf50Do5ztG2Vlmc5BF5HEMGuhO+cmgHuBJ4DDwPedcwfN7AEzux3AzN5kZs3A+4BvmNnB+QwdDQdP9wGwtkr3ERWRxBDJWS4453YBu6Zsuz9seQ+hqZi4sfdEFykG1y4p9DuKiEhUJO03RRvaB6kuzCIrENH/00REYl7SFnpjxyArF+uAqIgkjqQsdOccLd1Duo+oiCSUpCz0lp5h+kYmdIaLiCSUpCz00z0jACwp0l2KRCRxJGWhHzrdC8BlmkMXkQSSlIW+v6mHxbnpVORn+B1FRCRqkrLQ957sZn1tga7hIiIJJekKvX9knObuYa6qLvA7iohIVCVdof+hvgPQ/LmIJJ6kK/RTXUMAXK0RuogkmKQr9DO9o2QFUinL000tRCSxJF2ht/YOU56XoQOiIpJwkq7Qj7cNsFw3tRCRBJRUhT4xGeRExyCXl6nQRSTxJFWhn+0fZSLoqNVX/kUkASVVobd0DwNQUaCrLIpI4kmqQj/ZOQjoolwikpiSqtAb2gdJTTFdB11EElJSFfoz9e2srcpnUWpS/bZFJEkkTbONTkxypLWfP1tR7HcUEZF5kTSFfuzMABNBx9rKfL+jiIjMi6Qp9IPeTS3WVuX5nEREZH4kUaH3kZueRk2hznARkcSURIXey+XluaSk6BouIpKYkqLQe4fGebm5l7qlhX5HERGZN0lR6C+39DARdNxwWanfUURE5k1SFPofGzox0wFREUlsCV/ooxOTfO/5U9xyZRkFWQG/44iIzJuEL/SjZ/rpGRrnXeur/I4iIjKvEr7Qf/RCCwBXVmi6RUQSW0SFbmabzeyomdWb2X3TPJ9uZo95zz9nZkujnvQiDI1NsGPPKW6+YjHLSrL9jiMiMq9mLXQzSwUeArYAq4E7zWz1lN3uBrqdc5cBXwa+GO2gc3W6Z5iP/L89jE4EueeGZX7HERGZd2kR7LMRqHfONQKY2Q7gDuBQ2D53AP/bW34c+FczM+eci2JWAB7bc4rtv2/EOQg6R9DBZNDhvOVz23qGxkhJMf75vVfzZytKoh1DRCTmRFLoVUBT2HozcN1M+zjnJsysFygGOsJ3MrNtwDaA2traiwpclJ3OFeV5pKQYKQYpZphBqhkpZqSkgJmRHUjl/dct0VSLiCSNSAo9apxz24HtAHV1dRc1er9ldRm3rC6Lai4RkUQQyUHRFqAmbL3a2zbtPmaWBuQDndEIKCIikYmk0PcAK81smZkFgK3Azin77ATu8pbfC/x2PubPRURkZrNOuXhz4vcCTwCpwCPOuYNm9gCw1zm3E/gm8G0zqwe6CJW+iIgsoIjm0J1zu4BdU7bdH7Y8ArwvutFERGQuEv6boiIiyUKFLiKSIFToIiIJQoUuIpIgzK+zC82sHTh5kb+8hCnfQo1B8ZAR4iOnMkaHMkaH3xmXOOemvf2ab4V+Kcxsr3Ouzu8cFxIPGSE+cipjdChjdMRyRk25iIgkCBW6iEiCiNdC3+53gAjEQ0aIj5zKGB3KGB0xmzEu59BFROSN4nWELiIiU6jQRUQSRNwV+mw3rJ7n937EzNrM7EDYtiIz+7WZHfd+Fnrbzcz+xcv5spltCPs1d3n7Hzezu6Z7r0vIWGNmT5nZITM7aGafirWcZpZhZs+b2Utexv/jbV/m3WS83rvpeMDbPuNNyM3ss972o2b259HKGPb6qWb2opn9LBYzmtkJM3vFzPab2V5vW8x81t5rF5jZ42Z2xMwOm9mmWMpoZqu8P79zjz4z+9tYyhgx51zcPAhdvrcBWA4EgJeA1Qv4/jcCG4ADYdseBO7zlu8Dvugt3wr8AjDgeuA5b3sR0Oj9LPSWC6OYsQLY4C3nAscI3dw7ZnJ675XjLS8CnvPe+/vAVm/714G/9pb/Bvi6t7wVeMxbXu39HUgHlnl/N1Kj/Jl/Gvgu8DNvPaYyAieAkinbYuaz9l7/UeAebzkAFMRaxrCsqcAZYEmsZrxg/oV8syj8YW8Cnghb/yzw2QXOsJTXF/pRoMJbrgCOesvfAO6cuh9wJ/CNsO2v228e8v4UuCVWcwJZwAuE7lPbAaRN/awJXYt/k7ec5u1nUz//8P2ilK0aeBK4GfiZ956xlvEEbyz0mPmsCd297FW8EzBiMeOUXO8A/hDLGS/0iLcpl+luWF3lU5Zzypxzrd7yGeDcDU9nyrpgvwfvn/3rCY2AYyqnN5WxH2gDfk1o5NrjnJuY5v1edxNy4NxNyOf7z/IrwGeAoLdeHIMZHfArM9tnoZuwQ2x91suAduBb3tTVw2aWHWMZw20Fvuctx2rGGcVbocc0F/rfckycB2pmOcAPgb91zvWFPxcLOZ1zk865awiNgjcCV/iZZyozeyfQ5pzb53eWWbzFObcB2AJ8wsxuDH8yBj7rNELTlF9zzq0HBglNX5wXAxkB8I6H3A78YOpzsZJxNvFW6JHcsHqhnTWzCgDvZ5u3faas8/57MLNFhMr8O865H8VqTgDnXA/wFKHpiwIL3WR86vvNdBPy+cz4ZuB2MzsB7CA07fLVGMuIc67F+9kG/JjQ/xxj6bNuBpqdc895648TKvhYynjOFuAF59xZbz0WM15QvBV6JDesXmjhN8i+i9Cc9bntf+UdEb8e6PX++fYE8A4zK/SOmr/D2xYVZmaE7vF62Dn3pVjMaWalZlbgLWcSmuM/TKjY3ztDxuluQr4T2OqdYbIMWAk8H42MzrnPOueqnXNLCf09+61z7i9jKaOZZZtZ7rllQp/RAWLos3bOnQGazGyVt+ltwKFYyhjmTl6bbjmXJdYyXthCTthH6aDFrYTO3GgAPrfA7/09oBUYJzTyuJvQPOmTwHHgN0CRt68BD3k5XwHqwl7nI0C99/hwlDO+hdA/DV8G9nuPW2MpJ3AV8KKX8QBwv7d9OaGyqyf0z950b3uGt17vPb887LU+52U/CmyZp8/9rbx2lkvMZPSyvOQ9Dp777yGWPmvvta8B9nqf908InQESaxmzCf2LKj9sW0xljOShr/6LiCSIeJtyERGRGajQRUQShApdRCRBqNBFRBKECl1EJEGo0EVEEoQKXUQkQfx/eZqWqf3yFWcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_range = 7404\n",
    "merged_score = np.copy(scorematrix)\n",
    "max_values = np.max(merged_score, axis=1)\n",
    "max_values = max_values[:,np.newaxis]\n",
    "#plt.plot(merged_score)\n",
    "sorted_merged_score = np.sort(max_values,0)\n",
    "plt.plot(sorted_merged_score[0:x_range])\n",
    "print(sorted_merged_score.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23310156713775435\n",
      "0.44736828661459116\n",
      "0.3096427226198024\n",
      "0.2559900058953482\n",
      "0.03626361391255759\n",
      "0.43021125655664605\n",
      "0.059992009588646436\n",
      "0.1428627133978799\n",
      "0.184223921511658\n",
      "0.15114664957087978\n",
      "0.2473492867895083\n",
      "0.21307283667623417\n",
      "0.31397804615000224\n",
      "0.4330708322639421\n",
      "0.280930370079579\n",
      "0.2027048328319508\n",
      "0.46232471623814525\n",
      "0.45625666871975357\n",
      "0.0\n",
      "0.45787643943180767\n",
      "0.4261766094033113\n",
      "0.44699222478915024\n",
      "0.44486927410320587\n",
      "0.4193239539763728\n",
      "0.37199459519691996\n",
      "0.43888135814590595\n",
      "0.47278433224718636\n",
      "0.4590184221918234\n",
      "0.3872730737620262\n",
      "0.4980392006680118\n",
      "0.34382946303394496\n",
      "0.287797692432656\n",
      "0.33100110600740884\n",
      "0.3953398757675176\n",
      "0.3882406697201025\n",
      "0.40502973596200204\n",
      "0.4672913844380256\n",
      "0.4496488179323277\n",
      "0.3963923968468292\n",
      "0.4041203613944713\n",
      "0.4223039611435053\n",
      "0.4545409003104881\n",
      "0.4150533219976556\n",
      "0.33876306756733804\n",
      "0.37578470592470015\n",
      "0.35939936047752935\n",
      "0.34208473372594594\n",
      "0.2430279209590567\n",
      "0.4675867066923743\n",
      "0.058232627674294594\n",
      "0.2947601446079207\n",
      "0.41716349307963574\n",
      "0.4540082876230866\n",
      "0.44049746992335037\n",
      "0.4499125340851546\n",
      "0.4270215702976656\n",
      "0.4441248805482436\n",
      "0.4153871960561527\n",
      "0.354209778577349\n",
      "0.31661491953905363\n",
      "0.44567980012163294\n",
      "0.3566770625553878\n",
      "0.4550482527266296\n",
      "0.45757553154620123\n",
      "0.2938184749750226\n",
      "0.41763645963613577\n",
      "0.4497313849261337\n",
      "0.454726505957573\n",
      "0.4167226534649106\n",
      "0.4353884663750495\n",
      "0.4253336105145405\n",
      "0.3983967094184311\n",
      "0.29340240732761724\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.19818286524019246\n",
      "0.0\n",
      "0.38212454790403877\n",
      "0.36970841972199353\n",
      "0.4371726481720255\n",
      "0.4252446497871867\n",
      "0.3112158664485784\n",
      "0.4334824314611597\n",
      "0.44257200408807956\n",
      "0.3786116093931171\n",
      "0.47106377255614773\n",
      "0.41554619812870264\n",
      "0.4980392006680118\n",
      "0.3301055716603746\n",
      "0.3734348924699159\n",
      "0.46623809956287776\n",
      "0.4797575438282636\n",
      "0.3672185321850409\n",
      "0.45798214076389904\n",
      "0.4200123513002134\n",
      "0.42809652087289174\n",
      "0.44254934134224583\n",
      "0.46466211870801705\n",
      "0.3158529775423903\n",
      "0.3614972633746109\n",
      "0.2373466802740903\n",
      "0.35658451222162557\n",
      "0.45107184957339486\n",
      "0.33846622858541076\n",
      "0.06946786185759013\n",
      "0.05881779059790163\n",
      "0.3014700650415714\n",
      "0.15132648287693598\n",
      "0.07792339488359534\n",
      "0.4132499095551752\n",
      "0.0018290411087373312\n",
      "0.42922227083741077\n",
      "0.0028950966556674377\n",
      "0.4980392006680118\n",
      "0.4054725040839347\n",
      "0.003854426537575241\n",
      "0.4474992061733727\n",
      "0.3980238050303632\n",
      "0.43499478084386417\n",
      "0.4696794601937466\n",
      "0.3663599159679732\n",
      "0.42338531746954966\n",
      "0.42659737694926897\n",
      "0.46372531106693216\n",
      "0.44549126122309485\n",
      "0.4652515957415539\n",
      "0.4672105793362014\n",
      "0.4782743369885059\n",
      "0.4411073678301509\n",
      "0.460688542162418\n",
      "0.4771110498323945\n",
      "0.40495779637757817\n",
      "0.4758929536626277\n",
      "0.47494836168203736\n",
      "0.46783974423155367\n",
      "0.43849241786943477\n",
      "0.46268737756201145\n",
      "0.4416280018295221\n",
      "0.42425288456286286\n",
      "0.3010324407873439\n",
      "0.41752199897800296\n",
      "0.44415050678105905\n",
      "0.43324430372536066\n",
      "0.46470308934880356\n",
      "0.45338234521828374\n",
      "0.39740826689395503\n",
      "0.4501553226828986\n",
      "0.4641561926515674\n",
      "0.44461095164115355\n",
      "0.4089437096590076\n",
      "0.4533878571785159\n",
      "0.4778284369925363\n",
      "0.4334929257727854\n",
      "0.461169251740927\n",
      "0.46820066695875984\n",
      "0.47786743780020574\n",
      "0.3981329637875485\n",
      "0.4557161156455705\n",
      "0.4558047445596184\n",
      "0.41889317994738057\n",
      "0.43619372354128405\n",
      "0.43762672812903125\n",
      "0.4426858755659537\n",
      "0.4775194762129586\n",
      "0.4636973876710924\n",
      "0.46359766992914603\n",
      "0.463600796425745\n",
      "0.46138542438021746\n",
      "0.4726038653955183\n",
      "0.46507423803544207\n",
      "0.36845262091979053\n",
      "0.46886418746641917\n",
      "0.4733837958177798\n",
      "0.46998942514766023\n",
      "0.4722009091892291\n",
      "0.4637769300905636\n",
      "0.41351988680807417\n",
      "0.4747724362493268\n",
      "0.4699218592971342\n",
      "0.39024802488675814\n",
      "0.4688542459510267\n",
      "0.3919119540981192\n",
      "0.472886152960263\n",
      "0.4706838368685048\n",
      "0.473523578017919\n",
      "0.41210422792286416\n",
      "0.47639210472100724\n",
      "0.4573488375777387\n",
      "0.4591965586142596\n",
      "0.3675834027168709\n",
      "0.46127367478011505\n",
      "0.4443894941456502\n",
      "0.43286138477543334\n",
      "0.4452934338112978\n",
      "0.44025253409822995\n",
      "0.42188695327305076\n",
      "0.43207337001257584\n",
      "0.3803373674453934\n",
      "0.46405503890845373\n",
      "0.4547833770292905\n",
      "0.4183486121895307\n",
      "0.449110345483493\n",
      "0.43264301135019423\n",
      "0.4425925424835485\n",
      "0.4310397484793184\n",
      "0.4752902105865055\n",
      "0.417030543161469\n",
      "0.42124364720434115\n",
      "0.4499210264729208\n",
      "0.44339531975796975\n",
      "0.43155963588864965\n",
      "0.47339909872922503\n",
      "0.47419452446256527\n",
      "0.3867903383699295\n",
      "0.4666641205315684\n",
      "0.42908591272467456\n",
      "0.4214658863989496\n",
      "0.40637027442119106\n",
      "0.4370827829463259\n",
      "0.4615726703721092\n",
      "0.43898906576130237\n",
      "0.4510432361897536\n",
      "0.4554615506123069\n",
      "0.43346828926700576\n",
      "0.46139511084506185\n",
      "0.4621712559068531\n",
      "0.45357049400523697\n",
      "0.4521668720979116\n",
      "0.4277015066908166\n",
      "0.3921436143380301\n",
      "0.4578575898931901\n",
      "0.4551693292031642\n",
      "0.4653424827363249\n",
      "0.4208398950710186\n",
      "0.456464685226375\n",
      "0.2223466490345521\n",
      "0.4270955126038763\n",
      "0.46713356342173035\n",
      "0.4216120195934163\n",
      "0.4710656663341571\n",
      "0.39035044711018885\n",
      "0.4671180245905548\n",
      "0.47138454046809164\n",
      "0.46946524700968817\n",
      "0.4740956264609378\n",
      "0.4553867834761166\n",
      "0.4598087261859297\n",
      "0.4384728356418\n",
      "0.21705543274522768\n",
      "0.420336647760505\n",
      "0.46280380813509786\n",
      "0.4424607628832703\n",
      "0.3624551903719832\n",
      "0.33845543010061346\n",
      "0.35995576252612455\n",
      "0.3554461051604455\n",
      "0.4730660852060446\n",
      "0.4595425086219121\n",
      "0.47281184446641766\n",
      "0.2210193026985754\n",
      "0.33982442271716046\n",
      "0.4438288039317702\n",
      "0.2810414056075896\n",
      "0.3759542415198256\n",
      "0.0023512673468601584\n",
      "0.4173289705910092\n",
      "0.45501019661954434\n",
      "0.45013891825573654\n",
      "0.4717907575768703\n",
      "0.4749108827334107\n",
      "0.46901282326233895\n",
      "0.4759810129486216\n",
      "0.46812668027672105\n",
      "0.476715309476798\n",
      "0.4790648543327658\n",
      "0.4838053597967706\n",
      "0.48475626982277475\n",
      "0.4766554612195402\n",
      "0.4642111151701121\n",
      "0.477245854165159\n",
      "0.4805669730932125\n",
      "0.4980392006680118\n",
      "0.44619611516631985\n",
      "0.2250466011215413\n",
      "0.4452797424770632\n",
      "0.460595439940868\n",
      "0.4610393922133199\n",
      "0.4980392006680118\n",
      "0.47650309638377303\n",
      "0.43984269117463104\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.45302005737627954\n",
      "0.4980392006680118\n",
      "0.44394777771588656\n",
      "0.4423793441082487\n",
      "0.4411861478930856\n",
      "0.46870865854179916\n",
      "0.47383769880859283\n",
      "0.014688629788832775\n",
      "0.4661029256184634\n",
      "0.45572830614303833\n",
      "0.3634017998933663\n",
      "0.3582515694817246\n",
      "0.37526229787194526\n",
      "0.45441852259499754\n",
      "0.4980392006680118\n",
      "0.47852024155172984\n",
      "0.4569588159277041\n",
      "0.4711950651175996\n",
      "0.40746517930730963\n",
      "0.4248464002436021\n",
      "0.42160817267923845\n",
      "0.4036395023229629\n",
      "0.38256899357885044\n",
      "0.3788581189139697\n",
      "0.3153569426978017\n",
      "0.43721963623434396\n",
      "0.4063441613490925\n",
      "0.4580172046766193\n",
      "0.4532999728673429\n",
      "0.4386333754092821\n",
      "0.44052920781623395\n",
      "0.46478577023133455\n",
      "0.4670902814018005\n",
      "0.3184314320730048\n",
      "0.44971050507202687\n",
      "0.47381360609679635\n",
      "0.46601969274226623\n",
      "0.47180416614187964\n",
      "0.4740355146442332\n",
      "0.45983012301374687\n",
      "0.4525887246253133\n",
      "0.4735725630864421\n",
      "0.4686145804319106\n",
      "0.4667621603235658\n",
      "0.46833703860119646\n",
      "0.43198271596114096\n",
      "0.469712361933918\n",
      "0.46957401326483905\n",
      "0.45606853516272317\n",
      "0.4572699363448761\n",
      "0.4755722750481191\n",
      "0.47752850804237906\n",
      "0.43149151876196806\n",
      "0.4689736805230579\n",
      "0.4082757360531314\n",
      "0.43383780519132864\n",
      "0.4159396875027672\n",
      "0.36189030401554795\n",
      "0.4284561309450644\n",
      "0.45870690093799305\n",
      "0.475924007670408\n",
      "0.4716757643037874\n",
      "0.476557777986031\n",
      "0.4835159548336141\n",
      "0.45909491992064444\n",
      "0.4688601873875025\n",
      "0.474817772597613\n",
      "0.4697230279045185\n",
      "0.4528997684324708\n",
      "0.4619858650788527\n",
      "0.45471182998662124\n",
      "0.4594920520282483\n",
      "0.4741930858759421\n",
      "0.46960071574252876\n",
      "0.47388654507158373\n",
      "0.4716646044749954\n",
      "0.38670477755041094\n",
      "0.39309949882311124\n",
      "0.40662493391168125\n",
      "0.4980392006680118\n",
      "0.33701489548520175\n",
      "0.46131480633555233\n",
      "0.0\n",
      "0.4980392006680118\n",
      "0.4588808357326217\n",
      "0.43712118763835517\n",
      "0.4706864991876807\n",
      "0.4751555590906779\n",
      "0.03425633947209384\n",
      "0.4980392006680118\n",
      "0.46189854232871486\n",
      "0.391611666224876\n",
      "0.46166628357847694\n",
      "0.43650602968492136\n",
      "0.42452463663188317\n",
      "0.42248677207682944\n",
      "0.3114606145516699\n",
      "0.4601440832711959\n",
      "0.47460124355402944\n",
      "0.46324804571277395\n",
      "0.3066909658245191\n",
      "0.4683396757530652\n",
      "0.4673122416548284\n",
      "0.3729492596369597\n",
      "0.4802417803102682\n",
      "0.3818525243075062\n",
      "0.3927862671767722\n",
      "0.4052815653718717\n",
      "0.4014110281195649\n",
      "0.4460560750142915\n",
      "0.4434142675421026\n",
      "0.2894738535379231\n",
      "0.41061878914074784\n",
      "0.35772493470455907\n",
      "0.4527688179695888\n",
      "0.3115051183099609\n",
      "0.43451217692126887\n",
      "0.4474282518234958\n",
      "0.45674080344213186\n",
      "0.38816416530501413\n",
      "0.21292051328331882\n",
      "0.4316050669213856\n",
      "0.1310053073803478\n",
      "0.4445369949494848\n",
      "0.43655642540055106\n",
      "0.43679391518241334\n",
      "0.365823321682518\n",
      "0.29391583211053424\n",
      "0.4042603230025251\n",
      "0.2626894449014285\n",
      "0.3810172593500601\n",
      "0.4400703551914086\n",
      "0.40965301370460533\n",
      "0.2540486172359603\n",
      "0.3917886436274846\n",
      "0.35773538477821215\n",
      "0.4347234970382519\n",
      "0.0\n",
      "0.37986371950928194\n",
      "0.37587774710944033\n",
      "0.35689155103083053\n",
      "0.46814646445791275\n",
      "0.3431784136493284\n",
      "0.23784033755401426\n",
      "0.3665404981542852\n",
      "0.3720957623975833\n",
      "0.36784842071713286\n",
      "0.303888949100257\n",
      "0.3520422532913342\n",
      "0.11976512915680962\n",
      "0.4425302416253215\n",
      "0.3633314398683986\n",
      "0.4696538274894478\n",
      "0.39313235171246824\n",
      "0.46014301033891386\n",
      "0.4500760415627459\n",
      "0.4436096457199048\n",
      "0.47009260644369044\n",
      "0.39654056922245545\n",
      "0.46217577502794444\n",
      "0.4525143109984076\n",
      "0.3312769066082288\n",
      "0.4656389471671508\n",
      "0.4030360225704448\n",
      "0.40108850154243414\n",
      "0.0\n",
      "0.3265481643234766\n",
      "0.474431652682991\n",
      "0.40971143553948336\n",
      "0.4314156697413283\n",
      "0.31959510383162265\n",
      "0.4423746141818102\n",
      "0.06719264013184946\n",
      "0.33879300537602736\n",
      "0.4433821743288842\n",
      "0.34813647064067366\n",
      "0.34564856293729873\n",
      "0.4714363538082825\n",
      "0.4338635323068902\n",
      "0.4273439336458475\n",
      "0.44850610919176953\n",
      "0.47690022717440783\n",
      "0.45212898437289095\n",
      "0.46209332766494504\n",
      "0.4619399375042157\n",
      "0.4598338674635494\n",
      "0.42048710812270385\n",
      "0.2652000551041445\n",
      "0.0\n",
      "0.36214502972240264\n",
      "0.33642357924518773\n",
      "0.26137541643046946\n",
      "0.4480551672121578\n",
      "0.40808888766609214\n",
      "0.4730742174895725\n",
      "0.3484135506710939\n",
      "0.4030688528756994\n",
      "0.019895681859011895\n",
      "0.4408908809791667\n",
      "0.4416780694136156\n",
      "0.37009957489925066\n",
      "0.03543414334467704\n",
      "0.46573758835955825\n",
      "0.39991902809476565\n",
      "0.4446997308926863\n",
      "0.44865292333292145\n",
      "0.45941402489309713\n",
      "0.44420666586542246\n",
      "0.3960109570304073\n",
      "0.44201479629278717\n",
      "0.47131418840322753\n",
      "0.4630839441769848\n",
      "0.35394502692990903\n",
      "0.36254459619603496\n",
      "0.45974652787894765\n",
      "0.36426996356659713\n",
      "0.23147976624157976\n",
      "0.4497096235314615\n",
      "0.4005558652290366\n",
      "0.23360562780582483\n",
      "0.3047591357124991\n",
      "0.458351060490666\n",
      "0.3373643591816462\n",
      "0.3796339493299083\n",
      "0.4980392006680118\n",
      "0.24411824601665788\n",
      "0.3496752550653605\n",
      "0.19587534764853606\n",
      "0.40585921909050404\n",
      "0.3230042753510335\n",
      "0.46027036457602183\n",
      "0.46491715601401784\n",
      "0.3502430055815803\n",
      "0.3423388789039098\n",
      "0.3455325725140294\n",
      "0.4237831683059854\n",
      "0.41781903636281714\n",
      "0.2546916567432036\n",
      "0.2452972401710448\n",
      "0.3807894489247355\n",
      "0.4168775225509924\n",
      "0.255580211871648\n",
      "0.3640663465701687\n",
      "0.40950300290491615\n",
      "0.4371143774206811\n",
      "0.45188754145443233\n",
      "0.45299297887163253\n",
      "0.46445960199208275\n",
      "0.4812471343033128\n",
      "0.47257552606768\n",
      "0.3661828333888652\n",
      "0.43951854439752525\n",
      "0.3137753032758192\n",
      "0.32783721239622254\n",
      "0.3222320110196748\n",
      "0.41621589237166734\n",
      "0.46525028667489365\n",
      "0.4526668010041641\n",
      "0.44724695169626644\n",
      "0.357819143546025\n",
      "0.36878911535358244\n",
      "0.39931931612229005\n",
      "0.4615550597430173\n",
      "0.4753778297407572\n",
      "0.46083440636286666\n",
      "0.34666001315080663\n",
      "0.37745815147534\n",
      "0.4059824993282392\n",
      "0.4725069473987382\n",
      "0.46042653933954203\n",
      "0.3954292183257947\n",
      "0.008049816679949\n",
      "0.428546038783146\n",
      "0.4354132320503454\n",
      "0.3900098231038363\n",
      "0.3201691874927203\n",
      "0.45224418560515295\n",
      "0.4623457085578415\n",
      "0.41000965588521093\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.24459976313406645\n",
      "0.21865397621224741\n",
      "0.45022747543287606\n",
      "0.3920692445435651\n",
      "0.44943019196908346\n",
      "0.44491959970111655\n",
      "0.4389117165826556\n",
      "0.2169551628757213\n",
      "0.036692099424801915\n",
      "0.4980392006680118\n",
      "0.46178033811847236\n",
      "0.09125102785158072\n",
      "0.3931386663395275\n",
      "0.3687397614266911\n",
      "0.42223524254434625\n",
      "0.4413899896423743\n",
      "0.0\n",
      "0.45392151931137065\n",
      "0.36498733002905187\n",
      "0.37505880072580944\n",
      "0.4350300165282012\n",
      "0.44889381820418284\n",
      "0.3799034367392884\n",
      "0.0\n",
      "0.44658766168111397\n",
      "0.0\n",
      "0.24855004032690634\n",
      "0.4637136325751975\n",
      "0.4362509613193266\n",
      "0.450428136436773\n",
      "0.3786969457653331\n",
      "0.45259302356335734\n",
      "0.45560076404240685\n",
      "0.43425843576858636\n",
      "0.45862439959803203\n",
      "0.48088426969308845\n",
      "0.38263655354114506\n",
      "0.4672131526702323\n",
      "0.45234726082227716\n",
      "0.46725646941257054\n",
      "0.47493176505531715\n",
      "0.4683541185320257\n",
      "0.45170051722222426\n",
      "0.4441970813519862\n",
      "0.47084559178106156\n",
      "0.4685958522967592\n",
      "0.4595165033107888\n",
      "0.4683705392896207\n",
      "0.44171739929967374\n",
      "0.45421018045549033\n",
      "0.37622270750369613\n",
      "0.3359650005163852\n",
      "0.40637907545679686\n",
      "0.3330993421951697\n",
      "0.0\n",
      "0.4676361729790142\n",
      "0.28288849009902817\n",
      "0.42151710894076116\n",
      "0.47274428680984726\n",
      "0.3564459178651968\n",
      "0.46690910605208163\n",
      "0.4449978299225574\n",
      "0.41042010177453436\n",
      "0.4092339548328803\n",
      "0.4666423475189863\n",
      "0.47401226621787396\n",
      "0.408766441078429\n",
      "0.45892280755516235\n",
      "0.4300142159967715\n",
      "0.4654180907710891\n",
      "0.43052712389700926\n",
      "0.44611553332307796\n",
      "0.4449005770747754\n",
      "0.4511603858722786\n",
      "0.47663866859732856\n",
      "0.45972839554235256\n",
      "0.4089369711932617\n",
      "0.4318872712716408\n",
      "0.3764318529395452\n",
      "0.369391393583213\n",
      "0.034459558760161275\n",
      "0.40992991086542463\n",
      "0.47002643410042366\n",
      "0.477141976478027\n",
      "0.04177850288857806\n",
      "0.4381070481572311\n",
      "0.42373954955393195\n",
      "0.40554543512931424\n",
      "0.32543208380575944\n",
      "0.45881065508743435\n",
      "0.4737504768580675\n",
      "0.4425082404430223\n",
      "0.4767750655145017\n",
      "0.46602063879671596\n",
      "0.4980392006680118\n",
      "0.4051621080360098\n",
      "0.45436244336675063\n",
      "0.43474851853122387\n",
      "0.3750385367945746\n",
      "0.22435075781930083\n",
      "0.3964807571211994\n",
      "0.46315567476280833\n",
      "0.46551982904317174\n",
      "0.45795926115140717\n",
      "0.4565441765751708\n",
      "0.4458947275119249\n",
      "0.45585202221138876\n",
      "0.467964968146932\n",
      "0.4382182417305752\n",
      "0.4093018248250868\n",
      "0.43868396097583057\n",
      "0.4465780749189523\n",
      "0.42895607148517517\n",
      "0.46324293166498\n",
      "0.39716486085611585\n",
      "0.41682536202876164\n",
      "0.46828646240249566\n",
      "0.42796644336832773\n",
      "0.4980392006680118\n",
      "0.4667354571029235\n",
      "0.4785069335950717\n",
      "0.47499703059854204\n",
      "0.4617707701545974\n",
      "0.47385724820605096\n",
      "0.3757151162751706\n",
      "0.46414148279596107\n",
      "0.4603098048657353\n",
      "0.47193528319643\n",
      "0.4208892009109665\n",
      "0.4712645840739054\n",
      "0.4785908570008589\n",
      "0.4714288990964491\n",
      "0.48049501560217933\n",
      "0.46346066803775887\n",
      "0.4660843366643697\n",
      "0.4680927097618586\n",
      "0.4779557083651547\n",
      "0.46935009448305837\n",
      "0.4265901193675933\n",
      "0.47606024410269104\n",
      "0.4676976269080585\n",
      "0.4326329583618435\n",
      "0.46861359971458577\n",
      "0.4444819783091309\n",
      "0.4383795423213765\n",
      "0.4144631050074895\n",
      "0.4100165954304172\n",
      "0.4423691875851448\n",
      "0.3872562595960519\n",
      "0.3461907803587122\n",
      "0.40050995630189895\n",
      "0.3610225153865734\n",
      "0.4025711165902205\n",
      "0.4582324452204194\n",
      "0.4084281588741957\n",
      "0.33741413157811645\n",
      "0.4205073255751931\n",
      "0.4980392006680118\n",
      "0.4245978236749448\n",
      "0.3724607626042711\n",
      "0.40619905120116806\n",
      "0.4593132818038712\n",
      "0.4670723183965359\n",
      "0.4536592897639387\n",
      "0.4789343733951433\n",
      "0.4410594353150798\n",
      "0.46365510494934553\n",
      "0.4279738483577877\n",
      "0.4299954973197053\n",
      "0.462997576355926\n",
      "0.4775773296968961\n",
      "0.4429149941816138\n",
      "0.420323613594458\n",
      "0.4729537422528564\n",
      "0.46106336037493584\n",
      "0.4591914376488759\n",
      "0.4635023953993726\n",
      "0.4731312531114898\n",
      "0.4513972884650223\n",
      "0.4535581426972658\n",
      "0.4706294451378528\n",
      "0.0\n",
      "0.4675313215908785\n",
      "0.43860464177028563\n",
      "0.44184054011720203\n",
      "0.4773475817756401\n",
      "0.44683615948018707\n",
      "0.42764797471624366\n",
      "0.39258192778783485\n",
      "0.4500827833025148\n",
      "0.44114261424978046\n",
      "0.4740580070170807\n",
      "0.45005979686174935\n",
      "0.34582522885415923\n",
      "0.4613920176858104\n",
      "0.4519384267787113\n",
      "0.4655602202480274\n",
      "0.4597307437049498\n",
      "0.4659695379332545\n",
      "0.450962710717898\n",
      "0.003091172844791652\n",
      "0.47371717235837896\n",
      "0.359410913668482\n",
      "0.47297179653266197\n",
      "0.4980392006680118\n",
      "0.4683498408436647\n",
      "0.4399161070262754\n",
      "0.4625561570911023\n",
      "0.4417780899009852\n",
      "0.4353727852619733\n",
      "0.48109952445217147\n",
      "0.06924601578334466\n",
      "0.4625385060229504\n",
      "0.43619311967344293\n",
      "0.46118254896545047\n",
      "0.47891563117589137\n",
      "0.4631888978590234\n",
      "0.46983632857476265\n",
      "0.4193196726284603\n",
      "0.4756712851744296\n",
      "0.4055908235208009\n",
      "0.34151136675250227\n",
      "0.4250743848643489\n",
      "0.43782893676483614\n",
      "0.4582700812396323\n",
      "0.3843060774256091\n",
      "0.45819118297038114\n",
      "0.0\n",
      "0.4741281144326051\n",
      "0.4434664366397147\n",
      "0.4663511058718417\n",
      "0.431039072336536\n",
      "0.4495037225506629\n",
      "0.0\n",
      "0.44292874752417216\n",
      "0.46309946037714994\n",
      "0.4621865673351312\n",
      "0.44450957988445083\n",
      "0.47370285788409683\n",
      "0.0\n",
      "0.42871654700614353\n",
      "0.43704443940299637\n",
      "0.4529142296605054\n",
      "0.4473583706462615\n",
      "0.3931443120948084\n",
      "0.2317275656006709\n",
      "0.4364416508071207\n",
      "0.420725991983506\n",
      "0.4873480419282497\n",
      "0.476837595415184\n",
      "0.0\n",
      "0.0\n",
      "0.4637732615731905\n",
      "0.46944342126593774\n",
      "0.4676152216024341\n",
      "0.4980392006680118\n",
      "0.0\n",
      "0.4887482774602377\n",
      "0.48302768291693365\n",
      "0.48412708425064827\n",
      "0.4834089660324724\n",
      "0.4771979614803779\n",
      "0.47443413326270617\n",
      "0.48360609658423337\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.03720357803371898\n",
      "0.385122966888464\n",
      "0.4799125648079092\n",
      "0.472846068075273\n",
      "0.47076731501221636\n",
      "0.4783083870163653\n",
      "0.4768655007675584\n",
      "0.22310654700872298\n",
      "0.48229733621552906\n",
      "0.4833872185229124\n",
      "0.2917710779491815\n",
      "0.45616749526236533\n",
      "0.4607142126315185\n",
      "0.4425028272278675\n",
      "0.46621918294245634\n",
      "0.4732965415602228\n",
      "0.46645670985410315\n",
      "0.4653733879942913\n",
      "0.44748113568176096\n",
      "0.46095243639098277\n",
      "0.4351533702069403\n",
      "0.47354477506896536\n",
      "0.3323124350159675\n",
      "0.47959258525474663\n",
      "0.4525610343296138\n",
      "0.45672979230170396\n",
      "0.3974329193413283\n",
      "0.4780801658432692\n",
      "0.4391545097238884\n",
      "0.4249303013817701\n",
      "0.42846401333487616\n",
      "0.4421176814888183\n",
      "0.4622961766575963\n",
      "0.4671175072485267\n",
      "0.4980392006680118\n",
      "0.4634726084515317\n",
      "0.41817172724718243\n",
      "0.4531139359957461\n",
      "0.44694111572992345\n",
      "0.4688074742760661\n",
      "0.4439900470539136\n",
      "0.4083942724173861\n",
      "0.46986111074437614\n",
      "0.4737901801246498\n",
      "0.44127433900932106\n",
      "0.47093179753142506\n",
      "0.42852175900294587\n",
      "0.2030844010292341\n",
      "0.4712504789123944\n",
      "0.45557834841000233\n",
      "0.440409002487772\n",
      "0.3721485720620548\n",
      "0.45985683338174743\n",
      "0.4509731061796237\n",
      "0.4412302420141738\n",
      "0.4648090506784046\n",
      "0.44607241078154747\n",
      "0.025698085299320418\n",
      "0.4814585130928482\n",
      "0.43644717864169724\n",
      "0.47373508735019154\n",
      "0.46121727291279785\n",
      "0.47118919820255223\n",
      "0.4736294065728332\n",
      "0.4686352947261225\n",
      "0.47427399892197475\n",
      "0.013721688459044342\n",
      "0.4980392006680118\n",
      "0.44407227088021645\n",
      "0.465352287543177\n",
      "0.41292956687432486\n",
      "0.47900165216667434\n",
      "0.47263645001518373\n",
      "0.31796191496599696\n",
      "0.4651456020662137\n",
      "0.42165926777169094\n",
      "0.04686864518473876\n",
      "0.48153058353048195\n",
      "0.48387945219313555\n",
      "0.4743479750309143\n",
      "0.4725863504203258\n",
      "0.4238726586704748\n",
      "0.48114398906686673\n",
      "0.4561743205791657\n",
      "0.00019783482331432603\n",
      "0.4980392006680118\n",
      "0.061416217867923825\n",
      "0.07316638453224167\n",
      "0.2989384481692594\n",
      "0.4108532422506927\n",
      "0.43050460706199106\n",
      "0.18310843268417026\n",
      "0.30199524412544265\n",
      "0.45082678800136566\n",
      "0.4980392006680118\n",
      "0.4670959043138809\n",
      "0.39161642494675947\n",
      "0.42485459717058727\n",
      "0.4117904734564321\n",
      "0.4109163083949486\n",
      "0.42173338965734775\n",
      "0.4270106451366363\n",
      "0.43127138084483024\n",
      "0.4341975922304677\n",
      "0.4237736785119361\n",
      "0.4303850102329304\n",
      "0.42400249707823456\n",
      "0.46461517667493146\n",
      "0.35975769395176393\n",
      "0.4746523514147888\n",
      "0.4602100613601045\n",
      "0.4539648401471195\n",
      "0.47030818616089104\n",
      "0.13328565791831742\n",
      "0.4980392006680118\n",
      "0.3807610817710201\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.3828714313660559\n",
      "0.4980392006680118\n",
      "0.3104858080752426\n",
      "0.04840152930533064\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.37222075389859066\n",
      "0.0\n",
      "0.025678213021397094\n",
      "0.4481424113374241\n",
      "0.32224221647385887\n",
      "0.4980392006680118\n",
      "0.14298448889043722\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.39804740087545276\n",
      "0.46347840719673833\n",
      "0.45141756837463215\n",
      "0.45640876168082806\n",
      "0.46045851562202117\n",
      "0.31063064430444975\n",
      "0.3489199094458713\n",
      "0.44705791728732064\n",
      "0.4980392006680118\n",
      "0.36596751943462696\n",
      "0.4672685988317452\n",
      "0.45976236671507487\n",
      "0.4136133698924399\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.46902712597755436\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4616426754902305\n",
      "0.4545459495778606\n",
      "0.4670357654815923\n",
      "0.45283081959314103\n",
      "0.4980392006680118\n",
      "0.3823033821033\n",
      "0.3872268962554335\n",
      "0.418655378651343\n",
      "0.4166516807015946\n",
      "0.4507904564432461\n",
      "0.4980392006680118\n",
      "0.3043689751909801\n",
      "0.4553373298359117\n",
      "0.3842148338732058\n",
      "0.4980392006680118\n",
      "0.429502859323312\n",
      "0.4291423145786572\n",
      "0.3910905896551497\n",
      "0.3996885280765183\n",
      "0.38966816607164223\n",
      "0.39818634795370744\n",
      "0.4980392006680118\n",
      "0.4186565445350695\n",
      "0.03444197643211877\n",
      "0.0\n",
      "0.4659121673456935\n",
      "0.4980392006680118\n",
      "0.4373268043552323\n",
      "0.3291749232690243\n",
      "0.382611498377992\n",
      "0.31615214234623124\n",
      "0.23018543446652312\n",
      "0.4980392006680118\n",
      "0.4636375752299908\n",
      "0.45006124499997835\n",
      "0.4667925697141341\n",
      "0.46189707316686424\n",
      "0.4768798508902895\n",
      "0.4980392006680118\n",
      "0.4801005996323626\n",
      "0.45621367053648\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.45022787282874505\n",
      "0.44197130978791227\n",
      "0.4420980829174647\n",
      "0.45163885599670545\n",
      "0.4980392006680118\n",
      "0.4372179887850341\n",
      "0.4245023140486715\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4634621103196375\n",
      "0.4980392006680118\n",
      "0.4737267754388441\n",
      "0.4241697401475314\n",
      "0.43606102146527914\n",
      "0.46254383614583805\n",
      "0.4599370124537788\n",
      "0.33443875521931987\n",
      "0.4360252624221122\n",
      "0.44889512264391046\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.07218950122099654\n",
      "0.43698174099614884\n",
      "0.438419171721586\n",
      "0.11715242000394997\n",
      "0.4206693809344596\n",
      "0.47334172464303403\n",
      "0.47777094285508764\n",
      "0.4799259846768363\n",
      "0.48411872442609755\n",
      "0.47735876145932027\n",
      "0.48169542538924215\n",
      "0.4788103197409606\n",
      "0.33110778734375423\n",
      "0.48293617827341484\n",
      "0.46345715217096556\n",
      "0.4980392006680118\n",
      "0.46957280273831303\n",
      "0.46020313594073076\n",
      "0.45506132579881425\n",
      "0.4980392006680118\n",
      "0.1558491978485242\n",
      "0.4630958929399456\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4403116076067056\n",
      "0.47220348407046703\n",
      "0.4666484346679642\n",
      "0.4980392006680118\n",
      "0.46464169002459904\n",
      "0.4081900427673241\n",
      "0.4701513975375791\n",
      "0.45645494794256064\n",
      "0.45376353571391337\n",
      "0.46917811961580913\n",
      "0.4980392006680118\n",
      "0.21586228958304698\n",
      "0.4676017938120322\n",
      "0.44522135117552\n",
      "0.45301591071226627\n",
      "0.4980392006680118\n",
      "0.475186245465165\n",
      "0.28386606563011973\n",
      "0.4418782481651574\n",
      "0.4730467997219008\n",
      "0.4980392006680118\n",
      "0.405078884194876\n",
      "0.4980392006680118\n",
      "0.23088008866787785\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4194561516529621\n",
      "0.29618039775249916\n",
      "0.4980392006680118\n",
      "0.4118189203855998\n",
      "0.26999322633447564\n",
      "0.43103551414740526\n",
      "0.47693588524208486\n",
      "0.4980392006680118\n",
      "0.45916508358580443\n",
      "0.4548058477409628\n",
      "0.4980392006680118\n",
      "0.4647374135341559\n",
      "0.28919114832879145\n",
      "0.45599705848137634\n",
      "0.47967115230642365\n",
      "0.46414873193243694\n",
      "0.4623349239457237\n",
      "0.4536278427018136\n",
      "0.47290725335401507\n",
      "0.4823976069689139\n",
      "0.4828174909712544\n",
      "0.46891644452079023\n",
      "0.46451605261056045\n",
      "0.2484714712260768\n",
      "0.4780403373208332\n",
      "0.4776584890865521\n",
      "0.47715512154542133\n",
      "0.4813338362480039\n",
      "0.4820441558175791\n",
      "0.4481269344305885\n",
      "0.4658098567913249\n",
      "0.4714495844966867\n",
      "0.41484611740043165\n",
      "0.4980392006680118\n",
      "0.47605401310168743\n",
      "0.46902073193035004\n",
      "0.4856151796753015\n",
      "0.48171084884798604\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.0\n",
      "0.11413251700137561\n",
      "0.4980392006680118\n",
      "0.4816289346658543\n",
      "0.468460497580272\n",
      "0.4557375071823751\n",
      "0.4817306400701561\n",
      "0.480326424523756\n",
      "0.4980392006680118\n",
      "0.4830188315563618\n",
      "0.4837398745832635\n",
      "0.46600671364046786\n",
      "0.479494170662185\n",
      "0.4765737074897339\n",
      "0.47542072390035384\n",
      "0.4558875207046989\n",
      "0.45467072863896985\n",
      "0.45741815391325974\n",
      "0.40848117009701695\n",
      "0.41704358277992193\n",
      "0.4011199302664181\n",
      "0.4980392006680118\n",
      "0.4436965435942334\n",
      "0.3866220750678791\n",
      "0.42317500738775066\n",
      "0.4470546924550356\n",
      "0.38740180043152306\n",
      "0.4139898395413833\n",
      "0.44661341813174793\n",
      "0.4301583277312633\n",
      "0.4346674281257608\n",
      "0.43664349491388005\n",
      "0.3544948206871506\n",
      "0.35670939487261905\n",
      "0.43452659796774745\n",
      "0.4604755975439735\n",
      "0.44749754943984454\n",
      "0.4378850583389986\n",
      "0.4612657170976604\n",
      "0.4101392926431031\n",
      "0.37069925050247043\n",
      "0.4980392006680118\n",
      "0.2658780972344249\n",
      "0.408633338592031\n",
      "0.4164950363066005\n",
      "0.3886459660995654\n",
      "0.3839985938380419\n",
      "0.43751735530256064\n",
      "0.44832590772203607\n",
      "0.4517392374642359\n",
      "0.4409375976832654\n",
      "0.37690764020123324\n",
      "0.37955250149350794\n",
      "0.4344915258019803\n",
      "0.2860284023863885\n",
      "0.40371237556656264\n",
      "0.3745128397336299\n",
      "0.4477624485813627\n",
      "0.3761316052819195\n",
      "0.3312257448345283\n",
      "0.44263896655916346\n",
      "0.463497638030796\n",
      "0.44868763742363904\n",
      "0.43240308204327743\n",
      "0.4980392006680118\n",
      "0.4423698639596596\n",
      "0.4980392006680118\n",
      "0.0002846797740176134\n",
      "0.3914676295184407\n",
      "0.4980392006680118\n",
      "0.4632990404256246\n",
      "0.430766915622676\n",
      "0.4980392006680118\n",
      "0.4777483817813925\n",
      "0.4764690364879907\n",
      "0.46811790539923376\n",
      "0.4736597108967788\n",
      "0.46446056433551897\n",
      "0.3404170194742805\n",
      "0.46116886086163605\n",
      "0.45982860284025534\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.0\n",
      "0.43023711154842476\n",
      "0.0\n",
      "0.01689620747838268\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4721687699822527\n",
      "0.47191332064180447\n",
      "0.47688479498671205\n",
      "0.48369050388332724\n",
      "0.47853902453819464\n",
      "0.46572683879392124\n",
      "0.4585319301200636\n",
      "0.46918948378647474\n",
      "0.4842093142000304\n",
      "0.4826953157124895\n",
      "0.468624701935699\n",
      "0.4589331208140431\n",
      "0.45711903008060556\n",
      "0.4174233112476108\n",
      "0.48527219562919655\n",
      "0.42773094861949684\n",
      "0.14069436094163446\n",
      "0.4880774580926764\n",
      "0.48296215144165866\n",
      "0.47920516991116807\n",
      "0.45247977860513106\n",
      "0.4828792457602926\n",
      "0.48421720409135405\n",
      "0.4761600135600419\n",
      "0.4812156787937614\n",
      "0.48367540157649347\n",
      "0.470942760458351\n",
      "0.48356118530632763\n",
      "0.4552398483671377\n",
      "0.48416820502730257\n",
      "0.4772081968356621\n",
      "0.4813910374133955\n",
      "0.4788790970141651\n",
      "0.48602980933955414\n",
      "0.4836341605764861\n",
      "0.48462589351876345\n",
      "0.48279354374287037\n",
      "0.4804398118513939\n",
      "0.4657245462274483\n",
      "0.4556038710644579\n",
      "0.47553545500011213\n",
      "0.44376237846421074\n",
      "0.47786264545525425\n",
      "0.44236207335935074\n",
      "0.484825351950202\n",
      "0.48556032197598614\n",
      "0.48363734171049816\n",
      "0.4601625996032685\n",
      "0.4980392006680118\n",
      "0.0\n",
      "0.4980392006680118\n",
      "0.4724608533984922\n",
      "0.3910868448311532\n",
      "0.41529739614077593\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4744336563223052\n",
      "0.47161731390275063\n",
      "0.46410886974832394\n",
      "0.472178302830339\n",
      "0.47008843990333515\n",
      "0.4759443076552243\n",
      "0.41153083092101717\n",
      "0.4324685785875468\n",
      "0.4980392006680118\n",
      "0.4504741345481531\n",
      "0.47293292606271825\n",
      "0.4980392006680118\n",
      "0.4597144578508726\n",
      "0.4603794881510434\n",
      "0.41516751311665784\n",
      "0.411521009485636\n",
      "0.46950386138360395\n",
      "0.474333132638487\n",
      "0.4804735999339444\n",
      "0.47110307071509866\n",
      "0.42126514680435256\n",
      "0.45964700554035515\n",
      "0.4720709795474864\n",
      "0.4747492250105864\n",
      "0.4671674354483395\n",
      "0.4842613236335723\n",
      "0.47410998238018864\n",
      "0.482913606853691\n",
      "0.4748556769296366\n",
      "0.47689704855543874\n",
      "0.36915692662042987\n",
      "0.43993660961410136\n",
      "0.47139948344495497\n",
      "0.4175550015195739\n",
      "0.47166974813032797\n",
      "0.40026789032985605\n",
      "0.42272614913384626\n",
      "0.48760055157422566\n",
      "0.3967262524236559\n",
      "0.4980392006680118\n",
      "0.0704842832618082\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4512480146934781\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.08786149381972004\n",
      "0.0\n",
      "0.4679963259880615\n",
      "0.46350558628337135\n",
      "0.47710585346056217\n",
      "0.4710084542759816\n",
      "0.4980392006680118\n",
      "0.45589070206034865\n",
      "0.40398262748509306\n",
      "0.459511094850139\n",
      "0.4626321198641421\n",
      "0.4744168018564496\n",
      "0.44897944228625697\n",
      "0.4739242779238961\n",
      "0.45885760915623675\n",
      "0.46510275219131975\n",
      "0.48254740206349117\n",
      "0.44615817049824236\n",
      "0.4312564455009201\n",
      "0.46042729238754093\n",
      "0.45853131812031933\n",
      "0.4562034726545156\n",
      "0.4709034428390009\n",
      "0.4453572185618989\n",
      "0.47700705761539924\n",
      "0.4726843128537876\n",
      "0.46401177200722465\n",
      "0.48065721748046253\n",
      "0.48730743703102175\n",
      "0.4789613404980203\n",
      "0.46671784220361284\n",
      "0.4678371330791656\n",
      "0.47534425569705036\n",
      "0.4809374778842627\n",
      "0.4841755859923113\n",
      "0.22411664621411265\n",
      "0.43657510131573585\n",
      "0.4611161007813655\n",
      "0.4652648020865173\n",
      "0.47068656856225805\n",
      "0.46316305016718246\n",
      "0.43583855498961344\n",
      "0.46223752083723807\n",
      "0.4736301560437103\n",
      "0.4714847149805092\n",
      "0.47295349415025106\n",
      "0.46857117508307733\n",
      "0.4594719642572852\n",
      "0.24450956124268153\n",
      "0.4716124488604117\n",
      "0.47246920306169626\n",
      "0.46178618231032903\n",
      "0.42097738683598834\n",
      "0.47567853811662125\n",
      "0.43489156208750335\n",
      "0.4516349735432292\n",
      "0.40073551194644885\n",
      "0.4625597518158235\n",
      "0.45060526290185804\n",
      "0.4503954155429899\n",
      "0.43466951116142266\n",
      "0.4187245994797655\n",
      "0.4081270997878737\n",
      "0.47937984474899425\n",
      "0.44456199657172574\n",
      "0.4521997313284369\n",
      "0.4980392006680118\n",
      "0.4866285502720356\n",
      "0.48298368886529985\n",
      "0.4642932336146761\n",
      "0.4798861357225771\n",
      "0.3723411153894473\n",
      "0.391576261367817\n",
      "0.4980392006680118\n",
      "0.40189779368106143\n",
      "0.43578826267673754\n",
      "0.4677706994370855\n",
      "0.440345069508848\n",
      "0.4716203404158658\n",
      "0.4627416783370689\n",
      "0.4980392006680118\n",
      "0.47279019938378014\n",
      "0.4234005248770681\n",
      "0.02053884964569082\n",
      "0.24852890315278092\n",
      "0.47045318964404553\n",
      "0.4538695343114961\n",
      "0.18906244761540172\n",
      "0.44445405529949766\n",
      "0.3762195438894305\n",
      "0.22758690739153936\n",
      "0.4815511697738026\n",
      "0.3683325075724836\n",
      "0.0\n",
      "0.4729080228133763\n",
      "0.48465990057988917\n",
      "0.37519680966369534\n",
      "0.4359262312422574\n",
      "0.451057071738631\n",
      "0.4980392006680118\n",
      "0.4742357100157299\n",
      "0.3797395256699056\n",
      "0.03650091865978265\n",
      "0.4980392006680118\n",
      "0.45172230853971124\n",
      "0.4980392006680118\n",
      "0.48057804805542803\n",
      "0.46351308037240946\n",
      "0.2881100813345976\n",
      "0.4167053256375013\n",
      "0.060539305389985525\n",
      "0.47475776886221194\n",
      "0.4980392006680118\n",
      "0.45244823117218186\n",
      "0.4792545825845961\n",
      "0.331730306191284\n",
      "0.479732531451535\n",
      "0.4655182075849931\n",
      "0.062242132470370415\n",
      "0.4809067332653985\n",
      "0.4601834078602817\n",
      "0.3298782914880829\n",
      "0.43528028664423535\n",
      "0.35724541746338717\n",
      "0.4356501240437232\n",
      "0.45925568543290424\n",
      "0.46468493598728067\n",
      "0.4793537325029012\n",
      "0.472449572282933\n",
      "0.4523322947321797\n",
      "0.4583374609477802\n",
      "0.4689818270427987\n",
      "0.48811690908552124\n",
      "0.41810889293800707\n",
      "0.46072438300097707\n",
      "0.45740685261297337\n",
      "0.4777677944662015\n",
      "0.4528540394478037\n",
      "0.45909369366859654\n",
      "0.45470131699184446\n",
      "0.4315235412529811\n",
      "0.40357507006286525\n",
      "0.469230492148932\n",
      "0.4418466132112556\n",
      "0.4579398010854315\n",
      "0.4420686642446765\n",
      "0.4980392006680118\n",
      "3.497955813781036e-05\n",
      "0.4524091560914267\n",
      "0.4980392006680118\n",
      "0.4705159319519779\n",
      "0.46192707033815555\n",
      "0.4260847764496953\n",
      "0.3800010102726923\n",
      "0.11466261672027395\n",
      "0.28104520321101434\n",
      "0.16253260987999069\n",
      "0.4241086605140368\n",
      "0.39386347763525553\n",
      "0.46281425859297753\n",
      "0.4336640758724056\n",
      "0.47935641415435\n",
      "0.44632720128793996\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.45047961469793574\n",
      "0.4717910038337597\n",
      "0.4521775379890657\n",
      "0.4269762688439673\n",
      "0.4362689776314333\n",
      "0.3979433401009206\n",
      "0.44981531483831916\n",
      "0.4574185387577892\n",
      "0.46642309746740973\n",
      "0.24178569281204676\n",
      "0.30831125693327455\n",
      "0.46178751053756223\n",
      "0.4980392006680118\n",
      "0.46379306504699674\n",
      "0.4237134934340497\n",
      "0.08598037635070808\n",
      "0.42168291572853916\n",
      "0.4980392006680118\n",
      "0.4077294289554538\n",
      "0.4431118310339701\n",
      "0.2738678878848592\n",
      "0.4052875726587164\n",
      "0.4392615223620587\n",
      "0.4980392006680118\n",
      "0.47108974135207354\n",
      "0.45456397912880503\n",
      "0.4560305893236787\n",
      "0.4776202566801234\n",
      "0.46953320766650897\n",
      "0.44802259364172153\n",
      "0.4282488919749983\n",
      "0.4695534554220954\n",
      "0.4523333783546235\n",
      "0.4532360571258474\n",
      "0.4538279789909679\n",
      "0.4504498693407472\n",
      "0.4177192147268028\n",
      "0.4161330627220835\n",
      "0.37260843810491323\n",
      "0.40147747182357807\n",
      "0.391445189432426\n",
      "0.39352167298299123\n",
      "0.47071808405427834\n",
      "0.46084023288557385\n",
      "0.4441018128116153\n",
      "0.46670345291683984\n",
      "0.34129328130855396\n",
      "0.4255209562098557\n",
      "0.41048828307055046\n",
      "0.0\n",
      "0.4980392006680118\n",
      "0.458974635780945\n",
      "0.0\n",
      "0.31821016688265347\n",
      "0.3973346366303379\n",
      "0.4291739068862115\n",
      "0.483641373052713\n",
      "0.22000144898434829\n",
      "0.030178882625479478\n",
      "0.0\n",
      "0.0004410355514112268\n",
      "0.4458541195976422\n",
      "0.35045826832635424\n",
      "0.22991605584757813\n",
      "0.16175421975667065\n",
      "0.3448495145260476\n",
      "0.3590884840464816\n",
      "0.03919715362483624\n",
      "0.0\n",
      "0.4538165527065638\n",
      "0.4509231680545507\n",
      "0.46581610062669115\n",
      "0.4494177381612275\n",
      "0.4601728628111147\n",
      "0.48953876937826485\n",
      "0.4571929838110971\n",
      "0.4653909081924608\n",
      "0.0\n",
      "0.353165667978793\n",
      "0.21965402584557636\n",
      "0.4297483910227707\n",
      "0.41579980567187824\n",
      "0.4834353687338512\n",
      "0.4717021330185488\n",
      "0.4694316715424092\n",
      "0.016703470750861673\n",
      "0.025817232789988785\n",
      "0.4710560416957435\n",
      "0.4296070973646904\n",
      "0.45858553818262116\n",
      "0.4754297988999439\n",
      "0.44883651664578533\n",
      "0.13171630617907368\n",
      "0.454307939404895\n",
      "0.42998374385108484\n",
      "0.4111120757631052\n",
      "0.35862518896410317\n",
      "0.4738934672989681\n",
      "0.0\n",
      "0.38146550521006645\n",
      "0.4066116793903158\n",
      "0.4980392006680118\n",
      "0.45444207523115976\n",
      "0.4745166036080097\n",
      "0.4510069699190084\n",
      "0.470897128021044\n",
      "0.4542744907289453\n",
      "0.37656730553408746\n",
      "0.46783803925482015\n",
      "0.46521387136925957\n",
      "0.439999337527575\n",
      "0.2995702674710539\n",
      "0.4596875993749891\n",
      "0.43168777118270657\n",
      "0.4821943206355223\n",
      "0.449929498909699\n",
      "0.4709110979540023\n",
      "0.47107857855224566\n",
      "0.4642076272405404\n",
      "0.4384756525008655\n",
      "0.4141224846203184\n",
      "0.4576243179050893\n",
      "0.433041029151511\n",
      "0.44800569006035834\n",
      "0.40520785452717034\n",
      "0.4460961443382753\n",
      "0.4365480968544356\n",
      "0.43302227733549126\n",
      "0.46938028772139523\n",
      "0.4595016858181877\n",
      "0.46153198797993594\n",
      "0.4614646807905264\n",
      "0.4449377791959844\n",
      "0.46799505882217585\n",
      "0.4583738166443538\n",
      "0.1559675493573394\n",
      "0.4523696177056077\n",
      "0.4750501615526381\n",
      "0.4551941429180648\n",
      "0.4534079241697176\n",
      "0.46687266271617495\n",
      "0.4363279923819489\n",
      "0.44487626921135803\n",
      "0.446590776152703\n",
      "0.4638821130949235\n",
      "0.42650503569012965\n",
      "0.3455185752644051\n",
      "0.4775599481954603\n",
      "0.4455806968225068\n",
      "0.4501849793363606\n",
      "0.44620396556614367\n",
      "0.40365394838651397\n",
      "0.3889160839420416\n",
      "0.40910781497681215\n",
      "0.44103779759249684\n",
      "0.4295678347611994\n",
      "0.47647660466809966\n",
      "0.3918358283178934\n",
      "0.37203872137965255\n",
      "0.4173249297450355\n",
      "0.3372018306352583\n",
      "0.4086673681180866\n",
      "0.38081364452186584\n",
      "0.4525024660928931\n",
      "0.3422151532798581\n",
      "0.31195162570202306\n",
      "0.4703425209689369\n",
      "0.4625156938782031\n",
      "0.43965687002744636\n",
      "0.42284081093013026\n",
      "0.4721440601782813\n",
      "0.47436698060558274\n",
      "0.4400190492318126\n",
      "0.43759890041320954\n",
      "0.44368973390502003\n",
      "0.4713565042314628\n",
      "0.452966314182005\n",
      "0.4830925128596036\n",
      "0.11956574907863272\n",
      "0.4980392006680118\n",
      "0.29032769434280414\n",
      "0.21106003426793477\n",
      "0.46797322454070545\n",
      "0.41075007034240324\n",
      "0.42036429618961063\n",
      "0.43631787641647335\n",
      "0.3347438431812212\n",
      "0.3474466823933175\n",
      "0.4508933286866624\n",
      "0.45469816105199795\n",
      "0.45701873258287434\n",
      "0.42117598126164957\n",
      "0.4678705752747261\n",
      "0.4338025252219082\n",
      "0.47260011813769204\n",
      "0.4796266854753146\n",
      "0.4427058573198118\n",
      "0.40119988184198807\n",
      "0.45470405085785903\n",
      "0.4383064291153965\n",
      "0.4614457179600055\n",
      "0.45564279014118325\n",
      "0.4402255455682891\n",
      "0.45213744972133474\n",
      "0.47733088559897635\n",
      "0.4530330749824585\n",
      "0.4620293706838529\n",
      "0.46918825360078614\n",
      "0.4686764005472313\n",
      "0.4378325383150759\n",
      "0.40291710537617903\n",
      "0.4563909288104572\n",
      "0.4781271039473408\n",
      "0.46152251918944237\n",
      "0.4241000046018755\n",
      "0.4688813624115338\n",
      "0.4521515279847402\n",
      "0.4549519362058156\n",
      "0.4488340025226036\n",
      "0.4641944613295197\n",
      "0.47608896162542275\n",
      "0.4714464207616413\n",
      "0.4635114586716354\n",
      "0.4736255531382928\n",
      "0.4677683157070464\n",
      "0.4631592976386668\n",
      "0.48230924057753616\n",
      "0.4586268619535278\n",
      "0.45040229262161036\n",
      "0.45682419968283117\n",
      "0.46910301691424067\n",
      "0.4325854232952558\n",
      "0.4980392006680118\n",
      "0.4788760503710643\n",
      "0.4612912670086288\n",
      "0.4470404536221678\n",
      "0.43402577216910737\n",
      "0.4519336540279294\n",
      "0.3636000660193435\n",
      "0.46413797328579487\n",
      "0.4672263794071893\n",
      "0.4625947925231866\n",
      "0.45928642794614755\n",
      "0.4596729887548144\n",
      "0.45402463344614397\n",
      "0.45824297756271637\n",
      "0.43723219612178615\n",
      "0.4496913192165295\n",
      "0.4540472984706138\n",
      "0.3778102806241179\n",
      "0.0\n",
      "0.43142743552628127\n",
      "0.4486194847594254\n",
      "0.37453909940485586\n",
      "0.47990370362825346\n",
      "0.46125727208932327\n",
      "0.46100193637380316\n",
      "0.46931572186460874\n",
      "0.472954163629428\n",
      "0.442455323796692\n",
      "0.44295416075150357\n",
      "0.4772252342649571\n",
      "0.4678760641038061\n",
      "0.4578328051801437\n",
      "0.4762075117289762\n",
      "0.4674264581885702\n",
      "0.44199486756942397\n",
      "0.4700295948828572\n",
      "0.4632493979497536\n",
      "0.47873960672469273\n",
      "0.4517156109306922\n",
      "0.4485259328468402\n",
      "0.47349651790558506\n",
      "0.4980392006680118\n",
      "0.42850206312710437\n",
      "0.178956960712937\n",
      "0.47322912836275\n",
      "0.47400550523531565\n",
      "0.48633354762829506\n",
      "0.48259329987151217\n",
      "0.46331114780072485\n",
      "0.4814294805013888\n",
      "0.4711724619456016\n",
      "0.3817046877214119\n",
      "0.4420708962540795\n",
      "0.460397362488591\n",
      "0.48298001479107866\n",
      "0.47727221801038455\n",
      "0.46489820277912447\n",
      "0.4568238953297032\n",
      "0.45916426316612685\n",
      "0.46347716022508517\n",
      "0.4781306844103786\n",
      "0.4433044380386044\n",
      "0.47356194121437195\n",
      "0.10566250650833912\n",
      "0.37949555836134524\n",
      "0.45016557402493723\n",
      "0.4403586732268613\n",
      "0.4980392006680118\n",
      "0.4571868278151607\n",
      "0.4469059136090968\n",
      "0.4252931480565135\n",
      "0.4980392006680118\n",
      "0.44792273382437264\n",
      "0.46919614958622075\n",
      "0.4648212047654651\n",
      "0.4421470432254237\n",
      "0.4619552521373143\n",
      "0.450444148344885\n",
      "0.4395069645886871\n",
      "0.3975726704952729\n",
      "0.4693638168398976\n",
      "0.4347579847665405\n",
      "0.27483560464397655\n",
      "0.4591698520358428\n",
      "0.4507717308311519\n",
      "0.47494720354572534\n",
      "0.44346496786997686\n",
      "0.3932770215999865\n",
      "0.23273018726762681\n",
      "0.44939302575529644\n",
      "0.46561993480321723\n",
      "0.4445119808342451\n",
      "0.47654318620301006\n",
      "0.46829673726066007\n",
      "0.4439413172106784\n",
      "0.47309088830222207\n",
      "0.4616272083197585\n",
      "0.429526534282658\n",
      "0.44922755495132743\n",
      "0.46357362786583517\n",
      "0.09550110274473188\n",
      "0.4327821187113522\n",
      "0.46053216295410143\n",
      "0.4852917739259886\n",
      "0.4980392006680118\n",
      "0.44572149181703097\n",
      "0.4595518249783919\n",
      "0.46538118275070156\n",
      "0.4842828183296606\n",
      "0.44200042174929405\n",
      "0.4577818923878864\n",
      "0.46281348940862005\n",
      "0.46687111043982404\n",
      "0.46317341214595276\n",
      "0.4688047386416288\n",
      "0.0\n",
      "0.43544596199547164\n",
      "0.4503305881455942\n",
      "0.47130781310533176\n",
      "0.4623106839488447\n",
      "0.47161177864105536\n",
      "0.45791678219861676\n",
      "0.4435293808645815\n",
      "0.4623770038120856\n",
      "0.44954281074586755\n",
      "0.48027306993086116\n",
      "0.4819996351633296\n",
      "0.45433066781805326\n",
      "0.4546401010101582\n",
      "0.45179377921673053\n",
      "0.4813618951278559\n",
      "0.4805509463660934\n",
      "0.4499300518322727\n",
      "0.401526008900182\n",
      "0.43004116807881954\n",
      "0.47997969255245887\n",
      "0.4294320391280185\n",
      "0.0015135644327315314\n",
      "0.4980392006680118\n",
      "0.4345062453388344\n",
      "0.4538852619849041\n",
      "0.4600862060455311\n",
      "0.4404120654910565\n",
      "0.422434622016869\n",
      "0.44606615213336687\n",
      "0.44260176816710156\n",
      "0.43450191722169956\n",
      "0.44737009164448643\n",
      "0.43923650997636754\n",
      "0.4539340163848377\n",
      "0.4181712703081359\n",
      "0.4256142630494037\n",
      "0.43760842830859714\n",
      "0.3103533504149483\n",
      "0.0017727211477835568\n",
      "0.0888387771329692\n",
      "0.41481275625201974\n",
      "0.4337469049550464\n",
      "0.0\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4084711273040401\n",
      "0.44900966516329155\n",
      "0.37950851362121485\n",
      "0.4253865903589907\n",
      "0.4648141266482868\n",
      "0.4578258349136407\n",
      "0.4229550158474315\n",
      "0.4486801982364895\n",
      "0.440688801666201\n",
      "0.42982795670323365\n",
      "0.4112316413834893\n",
      "0.4197749431410458\n",
      "0.2841249877442142\n",
      "0.3102665535868965\n",
      "0.44932429001862445\n",
      "0.4750364062852326\n",
      "0.44163138441894173\n",
      "0.4592016465478043\n",
      "0.14599408897321517\n",
      "0.4578813036042207\n",
      "0.21334655708360015\n",
      "0.4568411555699496\n",
      "0.3807825856406151\n",
      "0.4182105916967525\n",
      "0.44017005302500445\n",
      "0.476020040236737\n",
      "0.0\n",
      "0.47876641652285684\n",
      "0.45683226041625336\n",
      "0.4803160048881268\n",
      "0.2661754274355087\n",
      "0.3535683804672212\n",
      "0.30896903693538885\n",
      "0.43753559415203397\n",
      "0.3298559887485098\n",
      "0.39543231734680795\n",
      "0.4098349707805482\n",
      "0.40349991346675734\n",
      "0.41498545947034604\n",
      "0.45909189822226126\n",
      "0.0\n",
      "0.4119538010199604\n",
      "0.4383253823677029\n",
      "0.42495292261068685\n",
      "0.3788599508198301\n",
      "0.3888634007579246\n",
      "0.4052316895183855\n",
      "0.401582467289581\n",
      "0.3895423607613215\n",
      "0.44039004842299767\n",
      "0.4980392006680118\n",
      "0.4544846494251581\n",
      "0.45693432895833275\n",
      "0.4377965427395364\n",
      "0.38927207940192854\n",
      "0.36428000042018494\n",
      "0.4519572399550082\n",
      "0.3836317903064973\n",
      "0.21147798644048005\n",
      "0.11460879112264322\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.0\n",
      "0.446168745112869\n",
      "0.4546277059330105\n",
      "0.45738992220107705\n",
      "0.11348280149960865\n",
      "0.4330726488902615\n",
      "0.4598544214061642\n",
      "0.4388863192672988\n",
      "0.0030406827762789242\n",
      "0.4980392006680118\n",
      "0.47240082026688845\n",
      "0.04753852273654333\n",
      "0.42521765415146634\n",
      "0.4471563241750838\n",
      "0.4428611392633911\n",
      "0.4331429255896944\n",
      "0.4738409859595251\n",
      "0.47130794030169565\n",
      "0.2883132013082839\n",
      "0.45308834986533986\n",
      "0.4621482661946549\n",
      "0.38590881685111844\n",
      "0.45524908753128107\n",
      "0.46301691122836125\n",
      "0.42422498675522236\n",
      "0.46578803904237637\n",
      "0.4980392006680118\n",
      "0.4738830518743814\n",
      "0.4714598456704219\n",
      "0.4659015048557955\n",
      "0.368906358890453\n",
      "0.39359417692055576\n",
      "0.4221781956631538\n",
      "0.423629399044746\n",
      "0.46321529677311035\n",
      "0.4636809938944165\n",
      "0.4980392006680118\n",
      "0.44239012101135367\n",
      "0.0\n",
      "0.42884572888513695\n",
      "0.4605777856924749\n",
      "0.43522546609205154\n",
      "0.40676383302106384\n",
      "0.4980392006680118\n",
      "0.4740482597912528\n",
      "0.004137974111025381\n",
      "0.4236556855460182\n",
      "0.4584569142099194\n",
      "0.3651785499881495\n",
      "0.3566190560401547\n",
      "0.4724178471719785\n",
      "0.4525175331842948\n",
      "0.45185600654968616\n",
      "0.4459928915389351\n",
      "0.45942464262074917\n",
      "0.4980392006680118\n",
      "0.46523808730856897\n",
      "0.452427354015635\n",
      "0.4580721034925693\n",
      "0.45596531965001486\n",
      "0.46388961106209914\n",
      "0.4787580091056703\n",
      "0.44903382159352234\n",
      "0.4739187856537561\n",
      "0.46607840900496394\n",
      "0.466846750737652\n",
      "0.48273754001529423\n",
      "0.45473675874552366\n",
      "0.47333182868078505\n",
      "0.46833865261916946\n",
      "0.387013777051211\n",
      "0.4651197733929686\n",
      "0.4980392006680118\n",
      "0.2951214385276386\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.48753447737274413\n",
      "0.4980392006680118\n",
      "0.46744530266375955\n",
      "0.47743734390781994\n",
      "0.4545672067674991\n",
      "0.4544442689515281\n",
      "0.46658176364146736\n",
      "0.41503898068942774\n",
      "0.46857205872241453\n",
      "0.44148863347310596\n",
      "0.4553599402471121\n",
      "0.3949934013096738\n",
      "0.45278684965172133\n",
      "0.4616441609312784\n",
      "0.42693979866055465\n",
      "0.45020432592618187\n",
      "0.45745195829501806\n",
      "0.4703188782176382\n",
      "0.4775192189573424\n",
      "0.4426070207887184\n",
      "0.4102972806020641\n",
      "0.4627440738358959\n",
      "0.47670283383948464\n",
      "0.4734639356133646\n",
      "0.4754207963440427\n",
      "0.47369751202192584\n",
      "0.4830480825350265\n",
      "0.46098831868527096\n",
      "0.10168442300005717\n",
      "0.3797905542698528\n",
      "0.49314868331639305\n",
      "0.3454737797026001\n",
      "0.46966964267110256\n",
      "0.4776753037939657\n",
      "0.3243457666669568\n",
      "0.4656618057529774\n",
      "0.46377887829176073\n",
      "0.4537740770951657\n",
      "0.44222675898428043\n",
      "0.4321710566451967\n",
      "0.39380442447997993\n",
      "0.41952687905698\n",
      "0.4812982572192283\n",
      "0.4474921725834068\n",
      "0.4371710099434161\n",
      "0.45631619818756974\n",
      "0.4627258222985051\n",
      "0.47164431963177256\n",
      "0.42632219425946405\n",
      "0.4644982095439242\n",
      "0.4708838826646477\n",
      "0.4496783057874915\n",
      "0.3178819386029938\n",
      "0.08222679752354344\n",
      "0.4642886841613575\n",
      "0.45027792384427695\n",
      "0.3140748992600516\n",
      "0.4197217358521385\n",
      "0.43166192463796405\n",
      "0.4269040955924036\n",
      "0.4655264947925841\n",
      "0.4391732362276864\n",
      "0.46012278178704374\n",
      "0.4482257393988321\n",
      "0.46768024194450575\n",
      "0.4272224666230224\n",
      "0.35583451260923477\n",
      "0.44651353464071036\n",
      "0.4543054032085361\n",
      "0.43467631943039226\n",
      "0.44667480502485224\n",
      "0.4390844865911127\n",
      "0.4757251003160489\n",
      "0.4851155323193286\n",
      "0.4447330560625474\n",
      "0.46005123875383097\n",
      "0.4419368417857038\n",
      "0.0\n",
      "0.42571450032932\n",
      "0.32314455290674166\n",
      "0.4242024748568367\n",
      "0.44543747761072255\n",
      "0.11488560274949625\n",
      "0.4506107296130833\n",
      "0.3597913650148877\n",
      "0.4319902966208648\n",
      "0.28281835479879924\n",
      "0.462639952509511\n",
      "0.4980392006680118\n",
      "0.47300107773604483\n",
      "0.45232376897232957\n",
      "0.35785340169298796\n",
      "0.3910821276657439\n",
      "0.4980392006680118\n",
      "0.19082148681977157\n",
      "0.4980392006680118\n",
      "0.44735000705365274\n",
      "0.31640797281871746\n",
      "0.41935777501599264\n",
      "0.4006972080679826\n",
      "0.4980392006680118\n",
      "0.4385711818458905\n",
      "0.23971626956177577\n",
      "0.4980392006680118\n",
      "0.45856811308861734\n",
      "0.3875641275848706\n",
      "0.16959658359129814\n",
      "0.3827496079052397\n",
      "0.3259028225810867\n",
      "0.4177081055603237\n",
      "0.45970634745877714\n",
      "0.16471913492088222\n",
      "0.43090086176556325\n",
      "0.4598220688028997\n",
      "0.3871411416361181\n",
      "0.4643756745348899\n",
      "0.4362326306845865\n",
      "0.4677495294234466\n",
      "0.4980392006680118\n",
      "0.4521390464376579\n",
      "0.2867811957982416\n",
      "0.2897441423209239\n",
      "0.4484698094684108\n",
      "0.402749249735263\n",
      "0.4671085481449135\n",
      "0.2410555341550012\n",
      "0.48168280002941594\n",
      "0.4313705562523497\n",
      "0.43954567013865037\n",
      "0.475926120925345\n",
      "0.42481304647407425\n",
      "0.4550447823945515\n",
      "0.4519027359510333\n",
      "0.4407315797473012\n",
      "0.4605494294972562\n",
      "0.4896544931494464\n",
      "0.46510055589599575\n",
      "0.4591090959550907\n",
      "0.4326178992066752\n",
      "0.4489690305540783\n",
      "0.2513081418283842\n",
      "0.41469112184567536\n",
      "0.4568674167757507\n",
      "0.47221509334818385\n",
      "0.40785464350943246\n",
      "0.46670999558464804\n",
      "0.3214614188482594\n",
      "0.4275464163820591\n",
      "0.20505935860757843\n",
      "0.45974008572974395\n",
      "0.44612929805610874\n",
      "0.4543957362463007\n",
      "0.4505632550490051\n",
      "0.46015623712428794\n",
      "0.47324485382276\n",
      "0.21371306763047\n",
      "0.44836492974791026\n",
      "0.4405254391219351\n",
      "0.4456442548818654\n",
      "0.34559678558258233\n",
      "0.44742775534514884\n",
      "0.43842778433977825\n",
      "0.42725565819860106\n",
      "0.4733745248449058\n",
      "0.4362887209294689\n",
      "0.4607427524816721\n",
      "0.4408991571808571\n",
      "0.4755273199938644\n",
      "0.4710985759581088\n",
      "0.4063620902436543\n",
      "0.0\n",
      "0.09300363441261761\n",
      "0.1759265769595153\n",
      "0.010500387613591608\n",
      "0.260523487305393\n",
      "0.4040125687529176\n",
      "0.41114136195387047\n",
      "0.4133099238909874\n",
      "0.3461524927682494\n",
      "0.4749825412624021\n",
      "0.4061561675572363\n",
      "0.4548920649849083\n",
      "0.4348375466565669\n",
      "0.42561842136731004\n",
      "0.4016453640565134\n",
      "0.4134829946056775\n",
      "0.4247036754315006\n",
      "0.45498160686505146\n",
      "0.4754965878463371\n",
      "0.4384394783417006\n",
      "0.4412072855965121\n",
      "0.06648830334421514\n",
      "0.40692896133256334\n",
      "0.48263218757377746\n",
      "0.43176066434800114\n",
      "0.0\n",
      "0.4534375460485295\n",
      "0.4582342720657139\n",
      "0.433977805226201\n",
      "0.4756133063714684\n",
      "0.4018033363740057\n",
      "0.3172150706588829\n",
      "0.4579622010049556\n",
      "0.17197535669395064\n",
      "0.44282721801893543\n",
      "0.40695516339773113\n",
      "0.42205980100556917\n",
      "0.46108442726505666\n",
      "0.49049604293427485\n",
      "0.4246295114419925\n",
      "0.3450342053686121\n",
      "0.4387306149946023\n",
      "0.4015080406598992\n",
      "0.21281641340557383\n",
      "0.3968725212856852\n",
      "0.0\n",
      "0.4236912263722555\n",
      "0.4479164529242976\n",
      "0.46589777997520765\n",
      "0.4448923399504691\n",
      "0.3700118116515076\n",
      "0.3057619655800504\n",
      "0.4486865814867277\n",
      "0.3684903063307526\n",
      "0.4776083510654034\n",
      "0.42900539941337584\n",
      "0.4809763050947021\n",
      "0.4505333969950235\n",
      "0.4785348005433909\n",
      "0.4748855985418892\n",
      "0.3931075641914977\n",
      "0.43083416515325823\n",
      "0.47927754415506196\n",
      "0.46128127347204434\n",
      "0.46770441017315206\n",
      "0.46492113864850865\n",
      "0.4757512373368122\n",
      "0.4598376332718193\n",
      "0.476707736013413\n",
      "0.48062127933324966\n",
      "0.1663975622342508\n",
      "0.4030050066218116\n",
      "0.4204586563913319\n",
      "0.45008142465597434\n",
      "0.4648007308419403\n",
      "0.46516131428878954\n",
      "0.1619286144534579\n",
      "0.4537239444225897\n",
      "0.43589335595550505\n",
      "0.2509213257188773\n",
      "0.4352039312918114\n",
      "0.47592703016766413\n",
      "0.4508869613259694\n",
      "0.4171228892816087\n",
      "0.46004986942970677\n",
      "0.4206258377135015\n",
      "0.4594720200543906\n",
      "0.4364819293670983\n",
      "0.4185531955959648\n",
      "0.43979517310095034\n",
      "0.4532128261900994\n",
      "0.42200638353777065\n",
      "0.404018754178387\n",
      "0.19327406598657954\n",
      "0.43931594988303063\n",
      "0.4345446557979268\n",
      "0.46802365680055885\n",
      "0.0\n",
      "0.10737235166791734\n",
      "0.16905407221802007\n",
      "0.006705428130623253\n",
      "0.05948837697948837\n",
      "0.0\n",
      "0.18194829610453864\n",
      "0.23243518921722167\n",
      "0.44684908774056786\n",
      "0.4980392006680118\n",
      "0.39728213665521706\n",
      "0.0011392213679987722\n",
      "0.45171986030484396\n",
      "0.005732466234942973\n",
      "0.4545051326457663\n",
      "0.45498014075679166\n",
      "0.4848585011384866\n",
      "0.4641010016877992\n",
      "0.47118960810461535\n",
      "0.40933674568189704\n",
      "0.4816316788971572\n",
      "0.44557677986641187\n",
      "0.46488185651878\n",
      "0.41873999390523836\n",
      "0.4527199033841354\n",
      "0.4385407021285105\n",
      "0.4684930007958194\n",
      "0.0\n",
      "0.40138219854945656\n",
      "0.40480024697432293\n",
      "0.47169771817770073\n",
      "0.48282295783244683\n",
      "0.47669925559328136\n",
      "0.4620303106077086\n",
      "0.4433874125336914\n",
      "0.45407511707090975\n",
      "0.0\n",
      "0.3929403750640147\n",
      "0.4705025374304106\n",
      "0.4365211133554131\n",
      "0.4701665026002965\n",
      "0.44310206485801473\n",
      "0.46151475149704857\n",
      "0.4397222020972723\n",
      "0.47141597529373774\n",
      "0.4432320988900048\n",
      "0.4667915425260074\n",
      "0.47869405029995055\n",
      "0.47471756001166227\n",
      "0.4606017445407775\n",
      "0.4727094238296\n",
      "0.47179722321020984\n",
      "0.45228696346182\n",
      "0.0\n",
      "0.46782520836357344\n",
      "0.46433372110051485\n",
      "0.436325801995876\n",
      "0.4536625387875194\n",
      "0.4524516254165693\n",
      "0.0\n",
      "0.3138172142702273\n",
      "0.45715461039182215\n",
      "0.4483527770388742\n",
      "0.4475274343963363\n",
      "0.44932790398052647\n",
      "0.23494036697912368\n",
      "0.4716508182738305\n",
      "0.47313198201750983\n",
      "0.47704809978661106\n",
      "0.471823244936681\n",
      "0.4707490781000579\n",
      "0.4662904162882805\n",
      "0.4980392006680118\n",
      "0.4191978982570017\n",
      "0.01389954237564188\n",
      "0.0\n",
      "0.0\n",
      "0.4595914563704952\n",
      "0.35492258396593607\n",
      "0.44665519065745973\n",
      "0.4035410234625633\n",
      "0.39497570451179775\n",
      "0.4708932279117352\n",
      "0.4843735981308987\n",
      "0.0\n",
      "0.47460234145705876\n",
      "0.4184373695740638\n",
      "0.4733992443953007\n",
      "0.424550494585751\n",
      "0.33441810755269635\n",
      "0.4338791318077207\n",
      "0.4680723880113899\n",
      "0.4495229901129471\n",
      "0.4105373991789274\n",
      "0.4719788584835892\n",
      "0.4579540379108338\n",
      "0.45607498286315123\n",
      "0.0\n",
      "0.4464597643862731\n",
      "0.44199501022525134\n",
      "0.45948579974071596\n",
      "0.18780260246797673\n",
      "0.4359086390482672\n",
      "0.002942673525263645\n",
      "0.4047548177915153\n",
      "0.4980392006680118\n",
      "0.47591725759569686\n",
      "0.47203683319373385\n",
      "0.468774714260537\n",
      "0.298522618784547\n",
      "0.48688199393453835\n",
      "0.45195146134120484\n",
      "0.27636292222874403\n",
      "0.42700858632123795\n",
      "0.29255260089431195\n",
      "0.4523829619753396\n",
      "0.4484615676242352\n",
      "0.4713474797334961\n",
      "0.4746683216974908\n",
      "0.06262368466775517\n",
      "0.3440482169799708\n",
      "0.38677562513489805\n",
      "0.3961496967735053\n",
      "0.0\n",
      "0.08694677010653028\n",
      "0.3510208743526752\n",
      "0.3768132802839794\n",
      "0.33819248746050207\n",
      "0.3456907272591023\n",
      "0.30966216500398225\n",
      "0.2919476805313083\n",
      "0.4734217732599042\n",
      "0.4770476492733831\n",
      "0.48070312908462814\n",
      "0.480703288995358\n",
      "0.4742260675746533\n",
      "0.4650632373111389\n",
      "0.08686260274358047\n",
      "0.47309858229796437\n",
      "0.4393887215400028\n",
      "0.4765028311782393\n",
      "0.36765150447028194\n",
      "0.4248890237703192\n",
      "0.4286295636966948\n",
      "0.37228588350060154\n",
      "0.0\n",
      "0.4296414735011306\n",
      "0.0\n",
      "0.0\n",
      "0.3339101773014779\n",
      "0.44012037333031795\n",
      "0.4575496098531398\n",
      "0.39654908751533774\n",
      "0.4980392006680118\n",
      "0.3613600719956931\n",
      "0.01655810821025645\n",
      "0.011197649062015046\n",
      "0.4035825849503508\n",
      "0.3984635471546403\n",
      "0.16297812877767248\n",
      "0.11806808946745123\n",
      "0.0\n",
      "0.00030494072644969214\n",
      "0.4767666027033237\n",
      "0.4980392006680118\n",
      "0.22227868914418902\n",
      "0.33505055293593633\n",
      "0.4218196869651393\n",
      "0.4733997317419072\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4600323240310841\n",
      "0.0\n",
      "0.24803921562760897\n",
      "0.0\n",
      "0.0\n",
      "0.4347923841632233\n",
      "0.4485999590484531\n",
      "0.4980392006680118\n",
      "0.4544211174781635\n",
      "0.4604769123738849\n",
      "0.4505642305385368\n",
      "0.45308832588302367\n",
      "0.4699092813593608\n",
      "0.4721812000681205\n",
      "0.4551171884309795\n",
      "0.46984912486227043\n",
      "0.4688039146899979\n",
      "0.4703314743705971\n",
      "0.47171508318579597\n",
      "0.48080674777667715\n",
      "0.47829515254370725\n",
      "0.47935105674077805\n",
      "0.4664355052693295\n",
      "0.47436860992758423\n",
      "0.4487641425264976\n",
      "0.343048876021836\n",
      "0.4711858921545885\n",
      "0.4728058513127452\n",
      "0.44074757155987626\n",
      "0.4652080932777462\n",
      "0.4544207756375694\n",
      "0.4765245493938466\n",
      "0.47116166607614857\n",
      "0.4666617258932791\n",
      "0.470637803349599\n",
      "0.14534695175748547\n",
      "0.47759580342991276\n",
      "0.4980392006680118\n",
      "0.4740166720076125\n",
      "0.4477910465072507\n",
      "0.44467524757511595\n",
      "0.4782357968189623\n",
      "0.4636219573434707\n",
      "0.45257197738578403\n",
      "0.4640783243048537\n",
      "0.2832883101620826\n",
      "0.47606247843009114\n",
      "0.48493374299526687\n",
      "0.4794931336567546\n",
      "0.46937487766142877\n",
      "0.47519491280907306\n",
      "0.4597116153454881\n",
      "0.4714903467335087\n",
      "0.46068709857260687\n",
      "0.4429650400268365\n",
      "0.46402285676267135\n",
      "0.4653149893971772\n",
      "0.46845888922681067\n",
      "0.456187557154324\n",
      "0.46201672880818184\n",
      "0.46449287332033784\n",
      "0.47057495308215874\n",
      "0.46882401127787404\n",
      "0.463670647179459\n",
      "0.46180255285008487\n",
      "0.4471145231123795\n",
      "0.44548498810524845\n",
      "0.4670991239207783\n",
      "0.4566884192115896\n",
      "0.4718276520461516\n",
      "0.4475858743817756\n",
      "0.4771238230805463\n",
      "0.4693069821114642\n",
      "0.47970708750963986\n",
      "0.4774371560184701\n",
      "0.45338051162991194\n",
      "0.36358559287766595\n",
      "0.44740349276608893\n",
      "0.4776576352237968\n",
      "0.4450505685438151\n",
      "0.4625900010204593\n",
      "0.46531631579365296\n",
      "0.4749154241936667\n",
      "0.4593720070141701\n",
      "0.164705897254101\n",
      "0.048039335312604056\n",
      "0.40036205915827705\n",
      "0.4667610835865084\n",
      "0.46729857788553375\n",
      "0.4603311826088196\n",
      "0.45593240276664865\n",
      "0.44794827418457656\n",
      "0.4737859572002081\n",
      "0.4551193102052785\n",
      "0.4604420300052565\n",
      "0.4746715975520749\n",
      "0.3984721918440702\n",
      "0.4671128797104178\n",
      "0.46798264722476274\n",
      "0.47900790412267813\n",
      "0.46084350531053464\n",
      "0.4980392006680118\n",
      "0.4046982504652645\n",
      "0.4536812537301545\n",
      "0.4679082413315007\n",
      "0.4703202977533272\n",
      "0.4640881007311787\n",
      "0.45788199779068695\n",
      "0.45085673479590893\n",
      "0.42748514469525406\n",
      "0.40925991701878717\n",
      "0.4364959758836674\n",
      "0.4701136422100351\n",
      "0.46577237841473934\n",
      "0.43840387160089445\n",
      "0.4641941302489093\n",
      "0.4576326680999087\n",
      "0.4521121074920397\n",
      "0.43957026555815076\n",
      "0.4684975269741464\n",
      "0.444969218510881\n",
      "0.32823721795572725\n",
      "0.2987014574537298\n",
      "0.3384467415747363\n",
      "0.3590587978678377\n",
      "0.4438953271164711\n",
      "0.4416479957880254\n",
      "0.4032938552660698\n",
      "0.46774293146688745\n",
      "0.32875806551234743\n",
      "0.4768863754421198\n",
      "0.48100552543002345\n",
      "0.47789401645745505\n",
      "0.4749473727185351\n",
      "0.47742460300381945\n",
      "0.4852878651083136\n",
      "0.47451048200536644\n",
      "0.4745000637716988\n",
      "0.47400348235846385\n",
      "0.4776501464087103\n",
      "0.3937566430989266\n",
      "0.3726877244003609\n",
      "0.4762507458659298\n",
      "0.46876096878123835\n",
      "0.46990954982314326\n",
      "0.4355516939694795\n",
      "0.4090948098673152\n",
      "0.4223893194754882\n",
      "0.47457908376234753\n",
      "0.474691883958745\n",
      "0.481109931491\n",
      "0.47835023882873856\n",
      "0.4700404007213038\n",
      "0.46876911912676467\n",
      "0.48621831069506427\n",
      "0.47642306008869334\n",
      "0.469794724341561\n",
      "0.46840979244439124\n",
      "0.48090256023578\n",
      "0.47837048974601604\n",
      "0.4818628211082711\n",
      "0.4769132070010722\n",
      "0.44470144802430955\n",
      "0.4980392006680118\n",
      "0.47166379163036454\n",
      "0.48419924161320577\n",
      "0.467404319934107\n",
      "0.4675679737737423\n",
      "0.4695765310368947\n",
      "0.46472312335206195\n",
      "0.4486546370980153\n",
      "0.37604496894442063\n",
      "0.4482967402928898\n",
      "0.42001765605847224\n",
      "0.3678908410487162\n",
      "0.46644285151927894\n",
      "0.4846032411709164\n",
      "0.45452200388702785\n",
      "0.41159469831214124\n",
      "0.47520000384123984\n",
      "0.4698508657662004\n",
      "0.46189047513246995\n",
      "0.35774188477846447\n",
      "0.4051795859196193\n",
      "0.3847629137162013\n",
      "0.3929082488334243\n",
      "0.388830703389037\n",
      "0.41931894938604997\n",
      "0.18020017411622782\n",
      "0.23175920697137656\n",
      "0.002942673525263645\n",
      "0.0\n",
      "0.4762955417836507\n",
      "0.43537828986152305\n",
      "0.043976150225082965\n",
      "0.43149615776828315\n",
      "0.3634443673972145\n",
      "0.41842802691841136\n",
      "0.31133311438731615\n",
      "0.4502374735549639\n",
      "0.4646361251671091\n",
      "0.44161395343086157\n",
      "0.41319038307508904\n",
      "0.4450290881078177\n",
      "0.45883725435030315\n",
      "0.3922913716893865\n",
      "0.1615024214724797\n",
      "0.31482404226549454\n",
      "0.4142258787203568\n",
      "0.4980392006680118\n",
      "0.3289645904803668\n",
      "0.4487098297321865\n",
      "0.43366365959119785\n",
      "0.44933945519790414\n",
      "0.47751750663148407\n",
      "0.4596117689022654\n",
      "0.09230893524714183\n",
      "0.4110286966526221\n",
      "0.0\n",
      "0.44887128967124434\n",
      "0.4573436420587457\n",
      "0.4712676524582965\n",
      "0.46470442626382025\n",
      "0.35163566260878315\n",
      "0.4574172586829053\n",
      "0.4078626872147524\n",
      "0.4518695241191968\n",
      "0.434799202336076\n",
      "0.4521227077272478\n",
      "0.4325049534634903\n",
      "0.4166388833794581\n",
      "0.3597459254209778\n",
      "0.4047972369522727\n",
      "0.3121562037516344\n",
      "0.3999633092843832\n",
      "0.38918596152204404\n",
      "0.4980392006680118\n",
      "0.2909528436596043\n",
      "0.4039079319458646\n",
      "0.45271457532629217\n",
      "0.47238397304307195\n",
      "0.4706957711141837\n",
      "0.46352390486142636\n",
      "0.46906494883409494\n",
      "0.46777278932342575\n",
      "0.45969333471592644\n",
      "0.45820015450279983\n",
      "0.4642358734843491\n",
      "0.4571588901956946\n",
      "0.45349542772525536\n",
      "0.0\n",
      "0.4029964918391059\n",
      "0.47703729888661034\n",
      "0.4650504490349184\n",
      "0.4980392006680118\n",
      "0.3723452666675598\n",
      "0.0\n",
      "0.27664015770918887\n",
      "0.013621853482145008\n",
      "0.46234841967441126\n",
      "0.4347211867927463\n",
      "0.48517221882536155\n",
      "0.4759290838400923\n",
      "0.4767611142493891\n",
      "0.4697799336257389\n",
      "0.47343537955890885\n",
      "0.48182783099069093\n",
      "0.39970120719161994\n",
      "0.47978001194156306\n",
      "0.48077812203959386\n",
      "0.4888420860276387\n",
      "0.4824717447973537\n",
      "0.4805761565672396\n",
      "0.4642671167472252\n",
      "0.46017242589803875\n",
      "0.14896464353316455\n",
      "0.40427287016958385\n",
      "0.3079463647602272\n",
      "0.3962384099826133\n",
      "0.4744874776167533\n",
      "0.4658455698316632\n",
      "0.4807759300922529\n",
      "0.38589672305107026\n",
      "0.4770549583333347\n",
      "0.47308092105497196\n",
      "0.4745912162550026\n",
      "0.44501326168224975\n",
      "0.4830392014652259\n",
      "0.48591135872203955\n",
      "0.44992584798922935\n",
      "0.46112591571921263\n",
      "0.44409727279832134\n",
      "0.47997161261331883\n",
      "0.47316649987655696\n",
      "0.36461957846002924\n",
      "0.45352795527641226\n",
      "0.47800770628737027\n",
      "0.46718253236805\n",
      "0.4717967809314379\n",
      "0.3724264435716176\n",
      "0.4691411552078406\n",
      "0.4631156625419032\n",
      "0.4614621828587284\n",
      "0.44487379169369645\n",
      "0.41381526553094244\n",
      "0.4555102947430407\n",
      "0.47499764501023295\n",
      "0.47112774011195774\n",
      "0.4589327153089736\n",
      "0.48103578852755374\n",
      "0.4788046543273221\n",
      "0.40999356459672254\n",
      "0.47589516040241897\n",
      "0.47221981067380603\n",
      "0.4840924504669541\n",
      "0.48559962321022754\n",
      "0.48897182221086555\n",
      "0.4845601721037971\n",
      "0.47505894760232\n",
      "0.46582886554513464\n",
      "0.46500705704226636\n",
      "0.4735156516647208\n",
      "0.46975428555553733\n",
      "0.44764825336317643\n",
      "0.46584844965253763\n",
      "0.48126178093006894\n",
      "0.44240147003204655\n",
      "0.47495605384780454\n",
      "0.41540959020293017\n",
      "0.47029761386387653\n",
      "0.4755133267933572\n",
      "0.4151829116754264\n",
      "0.4575360165239223\n",
      "0.4730373990758668\n",
      "0.47196726168567144\n",
      "0.452657250588765\n",
      "0.4726953657179971\n",
      "0.43827911158511385\n",
      "0.46704496885168506\n",
      "0.4426244519278537\n",
      "0.4500262542727711\n",
      "0.4497482871147396\n",
      "0.4618751114372644\n",
      "0.0\n",
      "0.46093235527474924\n",
      "0.1953213712492323\n",
      "0.1839212586313271\n",
      "0.4300300094814165\n",
      "0.30725619125159537\n",
      "0.4797384161630309\n",
      "0.462354054375268\n",
      "0.4049617183724535\n",
      "0.35550337192384146\n",
      "0.4388709408741001\n",
      "0.4117304640687953\n",
      "0.45839831500679173\n",
      "0.3783234082698249\n",
      "0.47433252562889866\n",
      "0.4770870657960963\n",
      "0.46343931060248755\n",
      "0.4659992773069761\n",
      "0.456655703527243\n",
      "0.33177798834975947\n",
      "0.48121586319596904\n",
      "0.4213328049672019\n",
      "0.47026369810117336\n",
      "0.47212920214044324\n",
      "0.4762940126245259\n",
      "0.46399402895472464\n",
      "0.47102426214552967\n",
      "0.47354187781794516\n",
      "0.4778638581867985\n",
      "0.4681068807988302\n",
      "0.458252756540453\n",
      "0.3883126661232025\n",
      "0.4606866849660818\n",
      "0.4815233890017994\n",
      "0.4812829312649692\n",
      "0.48313204878758087\n",
      "0.4851237110824421\n",
      "0.48389492306073156\n",
      "0.47030397633236704\n",
      "0.4806865449756739\n",
      "0.38000177169966515\n",
      "0.4599574296536688\n",
      "0.4859422587759665\n",
      "0.4496061153308075\n",
      "0.4722628768644529\n",
      "0.4619809547497491\n",
      "0.46545529766330856\n",
      "0.4078069232275914\n",
      "0.4699995954057677\n",
      "0.4765026256582684\n",
      "0.46649544452061514\n",
      "0.4794524606484141\n",
      "0.3952637712902686\n",
      "0.43131050341293603\n",
      "0.41353691127659026\n",
      "0.4148374441762938\n",
      "0.4547640404450459\n",
      "0.43678558838657233\n",
      "0.47401499741007536\n",
      "0.3756100727374677\n",
      "0.4729057666248528\n",
      "0.44553967011959783\n",
      "0.4587080587769548\n",
      "0.4468240494992143\n",
      "0.44964437258959644\n",
      "0.43672921392422276\n",
      "0.4680101141038161\n",
      "0.4298047946324211\n",
      "0.46780826143619975\n",
      "0.47285504986026916\n",
      "0.42628156042496324\n",
      "0.4712734436152172\n",
      "0.4667712772303856\n",
      "0.42650066258766095\n",
      "0.261528565524368\n",
      "0.36070023688774283\n",
      "0.41733735278130524\n",
      "0.47528547814926186\n",
      "0.4536603028149877\n",
      "0.27736761959477585\n",
      "0.17132561534662716\n",
      "0.47911445204940595\n",
      "0.40260558102036587\n",
      "0.4410235123397163\n",
      "0.4266937868267545\n",
      "0.40702991671670635\n",
      "0.47936461022213095\n",
      "0.40413214617196097\n",
      "0.43924922865494215\n",
      "0.4658157199677632\n",
      "0.4386287931538832\n",
      "0.3835341814896292\n",
      "0.3438544386863005\n",
      "0.345997570585604\n",
      "0.4379684335516383\n",
      "0.4980392006680118\n",
      "0.4486723229398104\n",
      "0.4202803409305956\n",
      "0.0\n",
      "0.35568019194335504\n",
      "0.41084018141229145\n",
      "0.3720353186296916\n",
      "0.27935015669008306\n",
      "0.15687786464898276\n",
      "0.4794498515652572\n",
      "0.411664050913142\n",
      "0.40403043232418034\n",
      "0.4639023629637185\n",
      "0.47362159191431047\n",
      "0.4741679476092338\n",
      "0.4757531205285332\n",
      "0.4720277724386698\n",
      "0.48482564278085566\n",
      "0.48761787302169946\n",
      "0.4838956043830895\n",
      "0.48506074317086384\n",
      "0.48396048119958573\n",
      "0.4697405646247968\n",
      "0.46625463302737474\n",
      "0.4811333260313252\n",
      "0.4980392006680118\n",
      "0.48101996113328277\n",
      "0.4843419496569079\n",
      "0.475793582504218\n",
      "0.47862923758868486\n",
      "0.47236520513729174\n",
      "0.4849209373112355\n",
      "0.43831330974123295\n",
      "0.3849869431254708\n",
      "0.47137162780846326\n",
      "0.390669280635743\n",
      "0.0\n",
      "0.007843870496227064\n",
      "0.33632104060195067\n",
      "0.0\n",
      "0.1220268584104878\n",
      "0.3978147412936839\n",
      "0.27266053313144123\n",
      "0.0\n",
      "0.06841377408741821\n",
      "0.409933869000498\n",
      "0.09801373288101847\n",
      "0.3570420548323889\n",
      "0.32018173347437107\n",
      "0.35058490644510615\n",
      "0.37103416526543664\n",
      "0.32984130051335797\n",
      "0.13466809075174507\n",
      "0.3014115250816492\n",
      "0.3918340388938243\n",
      "0.31100406421363264\n",
      "0.4665696126379538\n",
      "0.39781961821824385\n",
      "0.38871613865295795\n",
      "0.4383524871848258\n",
      "0.4191109989314346\n",
      "0.4420862837968234\n",
      "0.41877371194227575\n",
      "0.0\n",
      "0.4458923410429173\n",
      "0.37271221905548635\n",
      "0.4530267172395616\n",
      "0.4980392006680118\n",
      "0.3958370159662792\n",
      "0.32358076481869813\n",
      "0.2752500840171585\n",
      "0.4980392006680118\n",
      "0.46457514479091705\n",
      "0.35976471980255426\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.45316348040043136\n",
      "0.4410961723172293\n",
      "0.4257528023014258\n",
      "0.4698744171129206\n",
      "0.41173255864408803\n",
      "0.47994531024178233\n",
      "0.4799679408560764\n",
      "0.3714669340110892\n",
      "0.47519232704012\n",
      "0.4712895670648977\n",
      "0.4813248561265383\n",
      "0.3233886022047596\n",
      "0.46998953444534286\n",
      "0.4882314356115759\n",
      "0.4847070929273318\n",
      "0.4842226283532444\n",
      "0.4690107249459187\n",
      "0.46667891669103867\n",
      "0.4782574142826208\n",
      "0.47220045627461366\n",
      "0.43217934524951657\n",
      "0.4529364283348747\n",
      "0.4190115737762189\n",
      "0.4186691200137319\n",
      "0.37567082134187757\n",
      "0.43017178618774227\n",
      "0.42407647929176395\n",
      "0.4309719985732813\n",
      "0.37633261664341616\n",
      "0.473018219739383\n",
      "0.3806620280617512\n",
      "0.4525664206896505\n",
      "0.44306611233996473\n",
      "0.4788354602585411\n",
      "0.4860444244525393\n",
      "0.4537100919760219\n",
      "0.42815258438602116\n",
      "0.4485681522240591\n",
      "0.44154867054724795\n",
      "0.4604174711332733\n",
      "0.43448359362087424\n",
      "0.40766860979325076\n",
      "0.38033539993117566\n",
      "0.4081090365122173\n",
      "0.4108971786664929\n",
      "0.43942423259600394\n",
      "0.45267998185658753\n",
      "0.39875625288678107\n",
      "0.44251510232130115\n",
      "0.4021763071353586\n",
      "0.43942128430217486\n",
      "0.42202984995222664\n",
      "0.45219920877711484\n",
      "0.39629064089753485\n",
      "0.3722823271752631\n",
      "0.4116974423546058\n",
      "0.45176253982800124\n",
      "0.4444702975666431\n",
      "0.4400524808440511\n",
      "0.45265881159375787\n",
      "0.4367321693161684\n",
      "0.41660091532118837\n",
      "0.4691255689780511\n",
      "0.4317365210286663\n",
      "0.4265264026590923\n",
      "0.34355291421210615\n",
      "0.459530663684283\n",
      "0.3716303525480417\n",
      "0.44399392251017633\n",
      "0.4216777096631373\n",
      "0.4531614600818535\n",
      "0.4679935145279552\n",
      "0.2730839434147419\n",
      "0.3210336752375106\n",
      "0.0\n",
      "0.39700634416334335\n",
      "0.0\n",
      "0.4641132745500593\n",
      "0.21370101257538301\n",
      "0.4407050920284658\n",
      "0.4212176923155519\n",
      "0.4447847249683615\n",
      "0.3786324676239738\n",
      "0.0\n",
      "0.3633439399144048\n",
      "0.3515163319921958\n",
      "0.0\n",
      "0.10247469523688069\n",
      "0.4980392006680118\n",
      "0.37185969095404475\n",
      "0.45324351824006487\n",
      "0.4572751475210894\n",
      "0.38410472018011305\n",
      "0.4525565397816902\n",
      "0.35354434544291874\n",
      "0.3231074685252296\n",
      "0.44706199432115584\n",
      "0.4980392006680118\n",
      "0.47188337074561326\n",
      "0.44273197380062124\n",
      "0.06054413645404155\n",
      "0.3408200248011036\n",
      "0.0\n",
      "0.43317522664989194\n",
      "0.4202480410517873\n",
      "0.001772535583759355\n",
      "0.4816716416295335\n",
      "0.46501965414317814\n",
      "0.4660916572601859\n",
      "0.47825781206964196\n",
      "0.4647005196743138\n",
      "0.4615758073113763\n",
      "0.4514022128581824\n",
      "0.4780708782622521\n",
      "0.06147810026094463\n",
      "0.2272725071691747\n",
      "0.09412693118772296\n",
      "0.2658781255504859\n",
      "0.45163701383508864\n",
      "0.14933323681794985\n",
      "0.43380697601145474\n",
      "0.4627347926600299\n",
      "0.45015539997849024\n",
      "0.44270429405379186\n",
      "0.4290759030588719\n",
      "0.4378494435703676\n",
      "0.2644497619320624\n",
      "0.44300517697491476\n",
      "0.0\n",
      "0.46726245935794697\n",
      "0.4195956504519962\n",
      "0.4388622419246617\n",
      "0.43987546713109066\n",
      "0.409844927606161\n",
      "0.40551180368500395\n",
      "0.44415766837796494\n",
      "0.43490322980364055\n",
      "0.3885551602625853\n",
      "0.4394615557705031\n",
      "0.43724381454654004\n",
      "0.41815682289582096\n",
      "0.21332722539565374\n",
      "0.4403553510749525\n",
      "0.4528861348778064\n",
      "0.3628910136643147\n",
      "0.4438657808799853\n",
      "0.4794806045770637\n",
      "0.4660413633668136\n",
      "0.47223879215014863\n",
      "0.3259318092024752\n",
      "0.47490599436853426\n",
      "0.4524935409643048\n",
      "0.45932240365014226\n",
      "0.4732887539551486\n",
      "0.47056468813478025\n",
      "0.47702122125773133\n",
      "0.47758531770996104\n",
      "0.4730447550333663\n",
      "0.48020868381529846\n",
      "0.4830055259622187\n",
      "0.4773701818344234\n",
      "0.4770183294470689\n",
      "0.43678173255241975\n",
      "0.0\n",
      "0.47031477086822177\n",
      "0.44906706493208604\n",
      "0.007655348514001195\n",
      "0.21630672495169592\n",
      "0.24067487422246037\n",
      "0.4221592807709043\n",
      "0.4475035408845245\n",
      "0.4549714931479686\n",
      "0.351832016271073\n",
      "0.42158839250864644\n",
      "0.44858279318415806\n",
      "0.42825047645018754\n",
      "0.3005525575498544\n",
      "0.3329071541323481\n",
      "0.4171115303431389\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4385961716160008\n",
      "0.44825379103283935\n",
      "0.4158887300112635\n",
      "0.4366301709118816\n",
      "0.38568418397661086\n",
      "0.4786727516628593\n",
      "0.4575135385533586\n",
      "0.4280714031488449\n",
      "0.4617950638812251\n",
      "0.476280699628777\n",
      "0.37573167603732605\n",
      "0.18179309759524995\n",
      "0.3568752470720357\n",
      "0.05052475787965093\n",
      "0.2442652851406656\n",
      "0.4796086903794614\n",
      "0.46399895057518287\n",
      "0.4617851699223906\n",
      "0.38491453773186785\n",
      "0.4694481048858049\n",
      "0.47739291068371253\n",
      "0.46433052083360365\n",
      "0.4708712042539116\n",
      "0.4799207773887061\n",
      "0.46692185815735915\n",
      "0.4805792628476993\n",
      "0.45771682021856425\n",
      "0.4329900718838453\n",
      "0.47918416745018305\n",
      "0.4777200769766207\n",
      "0.4832480531823546\n",
      "0.4716151035983385\n",
      "0.4647830356052882\n",
      "0.4672338190602395\n",
      "0.4685401164663323\n",
      "0.4700828557969913\n",
      "0.002627973587808383\n",
      "0.4689790239164776\n",
      "0.47445483519381965\n",
      "0.4704448980779978\n",
      "0.4852679484059162\n",
      "0.4819857233567056\n",
      "0.47018700036678224\n",
      "0.4837303656703808\n",
      "0.46421196289566596\n",
      "0.4654327635475262\n",
      "0.4820544178587577\n",
      "0.4546057886212381\n",
      "0.46907260847764076\n",
      "0.47679466659263864\n",
      "0.46518387085578183\n",
      "0.46675398459452\n",
      "0.46708250022524556\n",
      "0.476374118815234\n",
      "0.47139394832068465\n",
      "0.4691592879497524\n",
      "0.012169373938652004\n",
      "0.4591726243846719\n",
      "0.4634371699238538\n",
      "0.4685111644462139\n",
      "0.4556695011624184\n",
      "0.4696809806701402\n",
      "0.45382111806034997\n",
      "0.4690977173621312\n",
      "0.481010065779787\n",
      "0.4700213086458397\n",
      "0.47169713272473307\n",
      "0.4407946921204904\n",
      "0.46714326420635505\n",
      "0.4741506586526527\n",
      "0.4513750047054462\n",
      "0.46290401207915194\n",
      "0.1560882023025982\n",
      "0.4115703414105205\n",
      "0.4541438717932563\n",
      "0.0\n",
      "0.4593507055356526\n",
      "0.4559989062613148\n",
      "0.3089574706229901\n",
      "0.20201834503991645\n",
      "0.3849630543837364\n",
      "0.42368822451773447\n",
      "0.2918616005645478\n",
      "0.47766265703179345\n",
      "0.3876920103190667\n",
      "0.31645545141269404\n",
      "0.23580557776598363\n",
      "0.3999861269432067\n",
      "0.4680387940664345\n",
      "0.4711054512043201\n",
      "0.42456516100117964\n",
      "0.4691790458543452\n",
      "0.4623767575141355\n",
      "0.42022472594377497\n",
      "0.49078820186961153\n",
      "0.48842224784849275\n",
      "0.48708149483441365\n",
      "0.4857952784349995\n",
      "0.44285246091833336\n",
      "0.48644673032008695\n",
      "0.4706943012456\n",
      "0.4511952181894605\n",
      "0.4332041488461135\n",
      "0.45830666998361835\n",
      "0.42990189482417873\n",
      "0.483031493766735\n",
      "0.4767033531151477\n",
      "0.4796670186958318\n",
      "0.48209618573532403\n",
      "0.4800446960759923\n",
      "0.4695517861174716\n",
      "0.4857558795545173\n",
      "0.4823651497557213\n",
      "0.4785290445375348\n",
      "0.4792845179762652\n",
      "0.4781733527659875\n",
      "0.4682808451913045\n",
      "0.4804784126124886\n",
      "0.46946567663196037\n",
      "0.47295382312518036\n",
      "0.4891134558854229\n",
      "0.4717135845401561\n",
      "0.4719460449935965\n",
      "0.49240854459840044\n",
      "0.47864514228606714\n",
      "0.4750573078745813\n",
      "0.4770976270692737\n",
      "0.4829948916431304\n",
      "0.47493070886324007\n",
      "0.4703569121946465\n",
      "0.4738022280018165\n",
      "0.0\n",
      "0.4908707356050042\n",
      "0.4878791535912831\n",
      "0.48553397530712294\n",
      "0.47604395796585297\n",
      "0.45669929739149473\n",
      "0.4705130451913394\n",
      "0.4894748058674494\n",
      "0.475409066642321\n",
      "0.4832502304796532\n",
      "0.47318735782751503\n",
      "0.4771597412303004\n",
      "0.47471139728493106\n",
      "0.4980392006680118\n",
      "0.4873424507959887\n",
      "0.4639339037922117\n",
      "0.4863552590679159\n",
      "0.48992815328357225\n",
      "0.48086980003756324\n",
      "0.4798241454969616\n",
      "0.47083616952948554\n",
      "0.47761445490990423\n",
      "0.4697686097763479\n",
      "0.0020410574004546453\n",
      "0.486317833117171\n",
      "0.4805276106494193\n",
      "0.4846804194828048\n",
      "0.4765789521078346\n",
      "0.45825367870440703\n",
      "0.46960213962705205\n",
      "0.48190710425946104\n",
      "0.4527951640694755\n",
      "0.45934183370653753\n",
      "0.4788858611690168\n",
      "0.478075329721572\n",
      "0.4784828902599653\n",
      "0.4785978882726394\n",
      "0.46623544896991304\n",
      "0.4781292751803119\n",
      "0.47783713613478385\n",
      "0.27936818357268706\n",
      "0.46374797469432444\n",
      "0.4441695482772344\n",
      "0.4635651425664261\n",
      "0.4415060283490075\n",
      "0.437145029617815\n",
      "0.4567271820667579\n",
      "0.3778065714579171\n",
      "0.46160609080088577\n",
      "0.2576923729653058\n",
      "0.3869972813792163\n",
      "0.4308957547609931\n",
      "0.4429276069839874\n",
      "0.2666083914310726\n",
      "0.36851848942681503\n",
      "0.23481031367976557\n",
      "0.014168681813308128\n",
      "0.4554533521771549\n",
      "0.4747990462213702\n",
      "0.45144115135123125\n",
      "0.038926279943684555\n",
      "0.4555989553170835\n",
      "0.47294873414429434\n",
      "0.4704687267634552\n",
      "0.47113297492239414\n",
      "0.4677811333839081\n",
      "0.4735127016129772\n",
      "0.45274476270981406\n",
      "0.4634721226908314\n",
      "0.4525527170210807\n",
      "0.46187300031819245\n",
      "0.43949583696994143\n",
      "0.46364445655567305\n",
      "0.44419897897754496\n",
      "0.47336788463635654\n",
      "0.3665475840028902\n",
      "0.2874523476522897\n",
      "0.47884951150842925\n",
      "0.4569460379028059\n",
      "0.36473559488961915\n",
      "0.47175689136316595\n",
      "0.0\n",
      "0.47817352250017814\n",
      "0.3633117915278514\n",
      "0.4843860887962524\n",
      "0.4755570170498364\n",
      "0.4868847257099143\n",
      "0.48461178924168696\n",
      "0.47806281390235955\n",
      "0.4843697102099319\n",
      "0.482255404259404\n",
      "0.46261031333648067\n",
      "0.41222714983050907\n",
      "0.47403308010422734\n",
      "0.48064286679082924\n",
      "0.4825842678592129\n",
      "0.45812205748204987\n",
      "0.48117503485564656\n",
      "0.4803504623709438\n",
      "0.48049811041131824\n",
      "0.23506594760385202\n",
      "0.48420819464961984\n",
      "0.48557382990365355\n",
      "0.4980392006680118\n",
      "0.48124619015853465\n",
      "0.4876914875578997\n",
      "0.47682460327114656\n",
      "0.48693506122600205\n",
      "0.42663589519334666\n",
      "0.4752896582393736\n",
      "0.48313629107254147\n",
      "0.4767833434322874\n",
      "0.48753216824281165\n",
      "0.48567552716697954\n",
      "0.4687188229152063\n",
      "0.47298535832059807\n",
      "0.468849210722366\n",
      "0.31366016898431015\n",
      "0.48811000020843903\n",
      "0.48167889945443115\n",
      "0.46988249465760284\n",
      "0.474453227077355\n",
      "0.0\n",
      "0.3295360646738122\n",
      "0.38184167978143607\n",
      "0.45312881156933676\n",
      "0.4549331101359725\n",
      "0.4556238039836071\n",
      "0.036963681364257615\n",
      "0.3620574184140191\n",
      "0.3759106418277779\n",
      "0.4260394217004422\n",
      "0.3898032128864798\n",
      "0.24713327604207005\n",
      "0.4481895969348226\n",
      "0.3762837165129848\n",
      "0.0025862871542405505\n",
      "0.33124682752099766\n",
      "0.3827510247703513\n",
      "0.47369714824459636\n",
      "0.17772202465154546\n",
      "0.45695163102454084\n",
      "0.43357048798089753\n",
      "0.347833306375427\n",
      "0.2134156657311884\n",
      "0.24806595548078506\n",
      "0.19006358051501462\n",
      "0.0\n",
      "0.4503442004545026\n",
      "0.24800179494575217\n",
      "0.4376368564182263\n",
      "0.3773785208789385\n",
      "0.4980392006680118\n",
      "0.4416175614441073\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.0\n",
      "0.0008351631664386054\n",
      "0.4177051443631587\n",
      "0.4194024946796736\n",
      "0.31188529583621805\n",
      "0.30322711528663593\n",
      "0.22172631846000515\n",
      "0.45752914197360606\n",
      "0.471667064703364\n",
      "0.4535299023373005\n",
      "0.463320407246345\n",
      "0.4210196625869621\n",
      "0.4178072464028242\n",
      "0.334671083783013\n",
      "0.4441100474824185\n",
      "0.39739473738750647\n",
      "0.4980392006680118\n",
      "0.4597345962338472\n",
      "0.4710825913023402\n",
      "0.4531788746622202\n",
      "0.480434821778189\n",
      "0.46721791528483475\n",
      "0.4752025595123629\n",
      "0.4691897225374723\n",
      "0.447293994112821\n",
      "0.4594565581235946\n",
      "0.42756303168260307\n",
      "0.45721995099503887\n",
      "0.4569003117021297\n",
      "0.4802206032988418\n",
      "0.4676445275879197\n",
      "0.47730261893920595\n",
      "0.46942622844257065\n",
      "0.4662522861961941\n",
      "0.3997189864520744\n",
      "0.34163096015612626\n",
      "0.4723479119059898\n",
      "0.4724935169428335\n",
      "0.4655541160452084\n",
      "0.4534587333553267\n",
      "0.4706665805469255\n",
      "0.44802753517815624\n",
      "0.45353632316757087\n",
      "0.4541136732390341\n",
      "0.47592057824511635\n",
      "0.3284843683396876\n",
      "0.4474765889210366\n",
      "0.478483129355395\n",
      "0.4748485552663846\n",
      "0.4573830933661072\n",
      "0.44588356420221814\n",
      "0.47706582977927897\n",
      "0.45982905908515376\n",
      "0.4599477317751704\n",
      "0.47666029256546866\n",
      "0.4980392006680118\n",
      "0.45463770816350574\n",
      "0.43667352233375667\n",
      "0.4678587277815736\n",
      "0.4500115783899134\n",
      "0.46709996647153124\n",
      "0.14325699445258236\n",
      "0.46319704905688885\n",
      "0.4784042336115197\n",
      "0.4708031204746239\n",
      "0.4701586647704991\n",
      "0.4085324915934921\n",
      "0.4596312046420229\n",
      "0.4482361275546708\n",
      "0.4345350166092357\n",
      "0.36621624647852336\n",
      "0.38770052316586695\n",
      "0.4703461946127524\n",
      "0.2639234446621601\n",
      "0.46370673720318156\n",
      "0.3916673349579762\n",
      "0.45938851623819227\n",
      "0.4788456138552084\n",
      "0.4379847413087652\n",
      "0.45451282232503737\n",
      "0.45466916086086373\n",
      "0.46483954594031773\n",
      "0.45721641338325725\n",
      "0.060539305389985525\n",
      "0.4599625500921962\n",
      "0.47795235383729223\n",
      "0.4276645860769664\n",
      "0.4308069175400677\n",
      "0.46424420150145274\n",
      "0.4450611415013834\n",
      "0.42452825193640426\n",
      "0.42785411390141703\n",
      "0.3829847729741976\n",
      "0.0013971218722495433\n",
      "0.4544199862517051\n",
      "0.45602018838395236\n",
      "0.4115541510379658\n",
      "0.0\n",
      "0.30895113927794093\n",
      "0.454297394841182\n",
      "0.46685860384969535\n",
      "0.45013434452860174\n",
      "0.416223095572812\n",
      "0.47155994027883424\n",
      "0.45801539241524736\n",
      "0.46097398902538833\n",
      "0.16562517103781477\n",
      "0.4506264043573823\n",
      "0.003854426537575241\n",
      "0.004369482126526849\n",
      "0.41873652295570235\n",
      "0.33634195907957715\n",
      "0.4243652817054181\n",
      "0.30258008805473724\n",
      "0.30616551114305735\n",
      "0.4300618601991666\n",
      "0.4108358846478453\n",
      "0.40446580310115227\n",
      "0.12303924554748802\n",
      "0.26948786777692396\n",
      "0.429891249409941\n",
      "0.477624940165986\n",
      "0.47155157707225787\n",
      "0.3526310516509573\n",
      "0.47668645634957635\n",
      "0.4837821905177142\n",
      "0.3627772077697816\n",
      "0.4164433589467629\n",
      "0.1473282117311166\n",
      "0.007959518356561605\n",
      "0.08734901059957145\n",
      "0.38468024334650064\n",
      "0.44998911763565136\n",
      "0.46454133166989175\n",
      "0.4520175081611928\n",
      "0.4030750201232317\n",
      "0.4458469221619877\n",
      "0.4487382615867477\n",
      "0.46911245209865055\n",
      "0.4788832361041385\n",
      "0.4409183015761455\n",
      "0.45855057430237733\n",
      "0.46518446022748244\n",
      "0.466905108980802\n",
      "0.4590840956869729\n",
      "0.4764365803655292\n",
      "0.4691027315667513\n",
      "0.47792624131254063\n",
      "0.4683395213246761\n",
      "0.45905825352611634\n",
      "0.45904261523070333\n",
      "0.4355624117772747\n",
      "0.4535300325842624\n",
      "0.45368773853967975\n",
      "0.43457365893211475\n",
      "0.4653915985860898\n",
      "0.46535411839263\n",
      "0.39751881521970833\n",
      "0.33686102510200594\n",
      "0.4213185921651369\n",
      "0.3927971570847477\n",
      "0.44743751615244404\n",
      "0.0771366776153019\n",
      "0.38181688742815456\n",
      "0.023340140702924693\n",
      "0.42660405503620896\n",
      "0.42495014476810095\n",
      "0.4571897631740456\n",
      "0.4546033733035606\n",
      "0.47377454869836405\n",
      "0.45556746182509483\n",
      "0.4751499393366001\n",
      "0.4786024001654332\n",
      "0.474403581188427\n",
      "0.47131969650809014\n",
      "0.47728479546889957\n",
      "0.4700047801818857\n",
      "0.4724444032698396\n",
      "0.47294118064594404\n",
      "0.4680138412197855\n",
      "0.4684547259764238\n",
      "0.45346970707848183\n",
      "0.4644390072857626\n",
      "0.4601104925058373\n",
      "0.4742284365502435\n",
      "0.46478084536336667\n",
      "0.4684018303575869\n",
      "0.4308957601066231\n",
      "0.4695614178927048\n",
      "0.4461654529650926\n",
      "0.4844998704120496\n",
      "0.4803471509148239\n",
      "0.45717956253737635\n",
      "0.4357004124008228\n",
      "0.44758280486019053\n",
      "0.45117872116580315\n",
      "0.4595667205926171\n",
      "0.44228644647109927\n",
      "0.446026212564089\n",
      "0.4406632133734921\n",
      "0.44306399042621536\n",
      "0.4426443503572116\n",
      "0.4509617536752645\n",
      "0.460143819627411\n",
      "0.437294984961238\n",
      "0.34950974985771777\n",
      "0.4778813108818869\n",
      "0.4572115244028959\n",
      "0.46374292095220926\n",
      "0.4583182118675933\n",
      "0.4565256010103008\n",
      "0.47162713935890727\n",
      "0.4696993411585172\n",
      "0.4593021418229046\n",
      "0.4427437056630557\n",
      "0.45721143402181874\n",
      "0.44131947402022637\n",
      "0.46099757579915285\n",
      "0.4562386126599343\n",
      "0.4652088382757398\n",
      "0.47275204551231065\n",
      "0.48241311274630155\n",
      "0.0\n",
      "0.4774296636345422\n",
      "0.4772627081193579\n",
      "0.46839226230059844\n",
      "0.47512734104222254\n",
      "0.45724350834039396\n",
      "0.47526368463052404\n",
      "0.463272355029929\n",
      "0.4761232562523929\n",
      "0.4786615223587747\n",
      "0.475477692930472\n",
      "0.3991195016589484\n",
      "0.43670687030148503\n",
      "0.27692842358128117\n",
      "0.07176397346619585\n",
      "0.44931792454611036\n",
      "0.4980392006680118\n",
      "0.39948095808020667\n",
      "0.4696025438020864\n",
      "0.398636827017198\n",
      "0.001887286188609465\n",
      "0.41218924103511584\n",
      "0.43880506688247595\n",
      "0.37169912359545265\n",
      "0.24862002592370033\n",
      "0.3070105124937089\n",
      "0.42564837710258385\n",
      "0.3851931588264516\n",
      "0.4980392006680118\n",
      "0.3384089657848425\n",
      "0.36369809923818575\n",
      "0.24803921562760897\n",
      "0.46718963071508707\n",
      "0.4980392006680118\n",
      "0.415643762259821\n",
      "0.2618400925969077\n",
      "0.4471949411395886\n",
      "0.06633938197125645\n",
      "0.3161809250046574\n",
      "0.008126221121642907\n",
      "0.4980392006680118\n",
      "0.4621738325983413\n",
      "0.445349934674006\n",
      "0.43183248652704154\n",
      "0.4389970410673964\n",
      "0.47519830531286\n",
      "0.4980392006680118\n",
      "0.32287194379945927\n",
      "0.47293985568944574\n",
      "0.421440405199744\n",
      "0.0\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4361449511307765\n",
      "0.4445287079827582\n",
      "0.4507801597169882\n",
      "0.0\n",
      "0.4596615889406663\n",
      "0.4980392006680118\n",
      "0.42477849150340347\n",
      "0.4432485015949393\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.0011471871727467288\n",
      "0.4584367764432581\n",
      "0.4980392006680118\n",
      "0.06187041414535073\n",
      "0.4311849172282192\n",
      "0.0\n",
      "0.27255277102269987\n",
      "0.4566734726149872\n",
      "0.3191554420800038\n",
      "0.3256386774558815\n",
      "0.38155566836059085\n",
      "0.38798884458226424\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.27118275510251366\n",
      "0.4980392006680118\n",
      "0.0\n",
      "0.3992249800607419\n",
      "0.43264726819832167\n",
      "0.03646575436924706\n",
      "0.44120807539638035\n",
      "0.46969725445375177\n",
      "0.4751767735381507\n",
      "0.48593714971901\n",
      "0.4755141172423976\n",
      "0.08318779760006223\n",
      "0.4814626058457834\n",
      "0.4774237810971701\n",
      "0.4775152704394705\n",
      "0.266045967344076\n",
      "0.48135126221840896\n",
      "0.4737937619894691\n",
      "0.4980392006680118\n",
      "0.47548732290019274\n",
      "0.4528464735876235\n",
      "0.4056565980459794\n",
      "0.0\n",
      "0.4980392006680118\n",
      "0.026407954928664164\n",
      "0.3816639408339514\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.45535600251840386\n",
      "0.4720312933544638\n",
      "0.468459748843716\n",
      "0.4980392006680118\n",
      "0.4647932202895552\n",
      "0.3663066408919432\n",
      "0.47661998661222743\n",
      "0.46420045684934524\n",
      "0.4529366000432619\n",
      "0.4519477986985008\n",
      "0.4635280072024672\n",
      "0.4980392006680118\n",
      "0.365695997675233\n",
      "0.45929838081784957\n",
      "0.4678510932963273\n",
      "0.4601404552294688\n",
      "0.1766102978099204\n",
      "0.4980392006680118\n",
      "0.08137260880161352\n",
      "0.43952374824415585\n",
      "0.23977405475676225\n",
      "0.44530201917165235\n",
      "0.46246780513228963\n",
      "0.4980392006680118\n",
      "0.39476374080927035\n",
      "0.4980392006680118\n",
      "0.0\n",
      "0.4980392006680118\n",
      "0.39841935502978915\n",
      "0.4980392006680118\n",
      "0.4449477551821689\n",
      "0.3799402659118614\n",
      "0.4980392006680118\n",
      "0.4304342924638702\n",
      "0.4142198294116741\n",
      "0.41279564900819604\n",
      "0.47852459843779177\n",
      "0.4980392006680118\n",
      "0.4420452247411811\n",
      "0.0\n",
      "0.45922047822227874\n",
      "0.4980392006680118\n",
      "0.4731998407759381\n",
      "0.0\n",
      "0.43165213070137226\n",
      "0.47918639439262084\n",
      "0.467382108934541\n",
      "0.47075597115352463\n",
      "0.4733589672134834\n",
      "0.47167010100023915\n",
      "0.0007446597850625076\n",
      "0.4823991729059531\n",
      "0.48339123068942547\n",
      "0.472650409712941\n",
      "0.4630099332842788\n",
      "0.08914273569181291\n",
      "0.4736452901340827\n",
      "0.4760398601041408\n",
      "0.47671871576226815\n",
      "0.48176003168316106\n",
      "0.4817413247147895\n",
      "0.15355175018544454\n",
      "0.46676144873234227\n",
      "0.4582636250509784\n",
      "0.4702790946686234\n",
      "0.4392704911712411\n",
      "0.4980392006680118\n",
      "0.4767420580796799\n",
      "0.4777360634301755\n",
      "0.4866429925659405\n",
      "0.47909812431436694\n",
      "0.4980392006680118\n",
      "0.0\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.2629344984976194\n",
      "0.4980392006680118\n",
      "0.48191906798407075\n",
      "0.46351880349601643\n",
      "0.45859172486847594\n",
      "0.48284854713572634\n",
      "0.48333360707024636\n",
      "0.4211512161188712\n",
      "0.4980392006680118\n",
      "0.48428638273486385\n",
      "0.48482278257333694\n",
      "0.4726023787082889\n",
      "0.480715242556536\n",
      "0.4778223237314507\n",
      "0.4749933645970808\n",
      "0.45083692950530757\n",
      "0.33959225137907717\n",
      "0.42888440710099857\n",
      "0.24887335350378376\n",
      "0.3833882218424465\n",
      "0.41263123590655293\n",
      "0.3968010502674153\n",
      "0.4980392006680118\n",
      "0.45534218702879914\n",
      "0.4039149764113109\n",
      "0.39989324039868185\n",
      "0.44522496534878564\n",
      "0.45072509128932736\n",
      "0.34720456219733564\n",
      "0.43150198794275285\n",
      "0.4294224150938584\n",
      "0.4607968678831833\n",
      "0.3592266196286931\n",
      "0.4115505516383834\n",
      "0.34984350402850956\n",
      "0.3499383666350728\n",
      "0.4451179490637592\n",
      "0.44561106984204485\n",
      "0.44543175554673137\n",
      "0.4513360013221708\n",
      "0.42852290214838934\n",
      "0.46472709449653926\n",
      "0.44022883963266646\n",
      "0.228263604889002\n",
      "0.4980392006680118\n",
      "0.31611853845764765\n",
      "0.4107632017779809\n",
      "0.4115445722675169\n",
      "0.32717765571315904\n",
      "0.4137627174957973\n",
      "0.42915049026314134\n",
      "0.05501654641300522\n",
      "0.3298365193187253\n",
      "0.427962519280355\n",
      "0.4496807490943805\n",
      "0.44354611240960307\n",
      "0.38011274638555737\n",
      "0.2848603506973489\n",
      "0.449103569567613\n",
      "0.3778382120628725\n",
      "0.40871307767475085\n",
      "0.4014370254406964\n",
      "0.43035801441381877\n",
      "0.4241663751617426\n",
      "0.378622772067132\n",
      "0.3464443895499489\n",
      "0.4550044782122709\n",
      "0.4189550347070943\n",
      "0.4489161832435823\n",
      "0.4606221199615784\n",
      "0.0\n",
      "0.4495101487677542\n",
      "0.4980392006680118\n",
      "0.0\n",
      "0.4244408705918201\n",
      "0.3874587458233672\n",
      "0.0\n",
      "0.43205798078288493\n",
      "0.4139990663151912\n",
      "0.4980392006680118\n",
      "0.47076041489033826\n",
      "0.4567526330982816\n",
      "0.46774703905161763\n",
      "0.47381272599649016\n",
      "0.4529799437365691\n",
      "0.3774477667193795\n",
      "0.2675071556218854\n",
      "0.45772230144290194\n",
      "0.45199126451481786\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.012325423708118306\n",
      "0.41039960378361023\n",
      "0.4980392006680118\n",
      "0.0\n",
      "0.4980392006680118\n",
      "0.4316594963957603\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4737291081742776\n",
      "0.4782432200433942\n",
      "0.47651254260262665\n",
      "0.484046617888769\n",
      "0.4754632013983674\n",
      "0.4366040301498141\n",
      "0.45877128259968614\n",
      "0.463280625225967\n",
      "0.4166614575862528\n",
      "0.48127911589545314\n",
      "0.48562021675614436\n",
      "0.46938370925793405\n",
      "0.46261375432947377\n",
      "0.39824872132776656\n",
      "0.4817785672541302\n",
      "0.4666266100314564\n",
      "0.424434558735352\n",
      "0.37133050772506926\n",
      "0.48538736801589266\n",
      "0.47286348988839616\n",
      "0.48295557546929896\n",
      "0.48015719873391005\n",
      "0.44641791688040283\n",
      "0.479313543303635\n",
      "0.48583216579861316\n",
      "0.47671939602280583\n",
      "0.4758869727356801\n",
      "0.4829130218057999\n",
      "0.4784773118483035\n",
      "0.4829323406999295\n",
      "0.47595101923767197\n",
      "0.4545104471790217\n",
      "0.4809814354842584\n",
      "0.4773831927586473\n",
      "0.4804051426521974\n",
      "0.4800708711802247\n",
      "0.48563707153860836\n",
      "0.48125636171384\n",
      "0.48456740348337507\n",
      "0.48131351035099307\n",
      "0.47752413293912543\n",
      "0.28267061212889877\n",
      "0.4708966506779823\n",
      "0.4265181729338504\n",
      "0.48140388771135173\n",
      "0.4022506641736629\n",
      "0.47946861329754153\n",
      "0.4414051082131051\n",
      "0.4855622753220632\n",
      "0.4869882841906566\n",
      "0.4853520705972942\n",
      "0.4614699544069412\n",
      "0.4387789896464811\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4562469993474555\n",
      "0.3937771919949778\n",
      "0.4382021673594731\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.0\n",
      "0.4980392006680118\n",
      "0.4262942126059728\n",
      "0.41643955454870524\n",
      "0.4980392006680118\n",
      "0.47330728346969436\n",
      "0.4658305254386557\n",
      "0.4629678727551726\n",
      "0.46260158924157724\n",
      "0.4683592189596207\n",
      "0.4591378940886837\n",
      "0.40174085009221316\n",
      "0.42891998424508854\n",
      "0.4980392006680118\n",
      "0.0769518950512999\n",
      "0.4756305050113717\n",
      "0.4763489357891279\n",
      "0.4980392006680118\n",
      "0.45145060640308643\n",
      "0.45995327503695066\n",
      "0.43019710423565793\n",
      "0.39553213372973617\n",
      "0.47089777441035474\n",
      "0.4704912251398988\n",
      "0.4752830688135517\n",
      "0.441241932649824\n",
      "0.4747891928048207\n",
      "0.46685349896098327\n",
      "0.45519554803298307\n",
      "0.46895879193966117\n",
      "0.48062273918706216\n",
      "0.4684728933077311\n",
      "0.4806017321317567\n",
      "0.48450049490338015\n",
      "0.4672088335307091\n",
      "0.481532345673945\n",
      "0.4432687356538309\n",
      "0.46966582297701154\n",
      "0.28429738688194145\n",
      "0.39835419517818516\n",
      "0.46201644827912397\n",
      "0.359830274748935\n",
      "0.4704184096522312\n",
      "0.4193154031661128\n",
      "0.3812185718087749\n",
      "0.48381988547757204\n",
      "0.4189769465364975\n",
      "0.357879552066704\n",
      "0.4980392006680118\n",
      "0.15961747459002606\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.464501004989764\n",
      "0.053594875906736186\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.05052328031958544\n",
      "0.3969148193541855\n",
      "0.0\n",
      "0.46850822575162354\n",
      "0.46896144568910003\n",
      "0.4782219681297458\n",
      "0.4781206184881319\n",
      "0.4980392006680118\n",
      "0.45661186435237333\n",
      "0.4526739382625297\n",
      "0.44793187709938953\n",
      "0.48133443211506904\n",
      "0.46031362348387084\n",
      "0.47042932726477243\n",
      "0.41900368313253994\n",
      "0.33310439744905423\n",
      "0.47433212404584896\n",
      "0.45371700547160776\n",
      "0.47769963361316126\n",
      "0.4539921127599948\n",
      "0.4009631211173382\n",
      "0.4522393894476244\n",
      "0.4537227740754331\n",
      "0.43828394641222607\n",
      "0.47138411338009095\n",
      "0.46199550258264166\n",
      "0.3866456554145125\n",
      "0.47608303962064874\n",
      "0.4699271467440295\n",
      "0.4619930213977673\n",
      "0.4803128224653252\n",
      "0.4853115167446501\n",
      "0.4803121114521348\n",
      "0.4731747460505037\n",
      "0.39103550858852737\n",
      "0.44717644392470157\n",
      "0.46801066912463113\n",
      "0.480114112809307\n",
      "0.4761462943965071\n",
      "0.06723076794150233\n",
      "0.41686225012401856\n",
      "0.45476190856949855\n",
      "0.4190967335780373\n",
      "0.46486268538935377\n",
      "0.46606556719178094\n",
      "0.1565743262160468\n",
      "0.46495642245937857\n",
      "0.47792323390874836\n",
      "0.46705969760446575\n",
      "0.47486157384405514\n",
      "0.46398645095963736\n",
      "0.46545842009344296\n",
      "0.46997852857619377\n",
      "0.36185450125582763\n",
      "0.46298193872115057\n",
      "0.47377898532417206\n",
      "0.1830545795374901\n",
      "0.33874517425306605\n",
      "0.47859938341707753\n",
      "0.45648347724761656\n",
      "0.4763274934720506\n",
      "0.4366525526210237\n",
      "0.45173535610622223\n",
      "0.4482771105235397\n",
      "0.46064873113044597\n",
      "0.45411404443284015\n",
      "0.4535966183897933\n",
      "0.4532007104055255\n",
      "0.45912971529510954\n",
      "0.441802924552906\n",
      "0.4042537249337879\n",
      "0.47582744650147163\n",
      "0.41855492159076935\n",
      "0.4456978198214906\n",
      "0.0007741675168325287\n",
      "0.4752869175131183\n",
      "0.4686742044392205\n",
      "0.45917536188970515\n",
      "0.47782793916383487\n",
      "0.36764526913645545\n",
      "0.24570030621649808\n",
      "0.3952951746978622\n",
      "0.4980392006680118\n",
      "0.07179934548366411\n",
      "0.43855630667502954\n",
      "0.46901501007518587\n",
      "0.44101319234866104\n",
      "0.4635368880353924\n",
      "0.4646183565528726\n",
      "0.4980392006680118\n",
      "0.4468247957148716\n",
      "0.46071206077724985\n",
      "0.32003360592265095\n",
      "0.10957029391908987\n",
      "0.37658944161787256\n",
      "0.4785413813265028\n",
      "0.3334413555164612\n",
      "0.23249764447347193\n",
      "0.45695791998029806\n",
      "0.3958940291478844\n",
      "0.3273452249999535\n",
      "0.3981932107887781\n",
      "0.4549402202485079\n",
      "0.3427478159302888\n",
      "0.0\n",
      "0.4573556908756973\n",
      "0.48158990895122605\n",
      "0.0\n",
      "0.4387148742077447\n",
      "0.45354882031246246\n",
      "0.4980392006680118\n",
      "0.47112770401736187\n",
      "0.3964732118091043\n",
      "0.4038407820370475\n",
      "0.03650091865978265\n",
      "0.4980392006680118\n",
      "0.44502764533324046\n",
      "0.4980392006680118\n",
      "0.4823327374057976\n",
      "0.45870866764461316\n",
      "0.11798954303920534\n",
      "0.37995939431495157\n",
      "0.4980392006680118\n",
      "0.45551260495997836\n",
      "0.4766855558554692\n",
      "0.4980392006680118\n",
      "0.4564655049294019\n",
      "0.4762116243523249\n",
      "0.21101365414917286\n",
      "0.4777737154005222\n",
      "0.4623913089881163\n",
      "0.3799247114784987\n",
      "0.4807654257950175\n",
      "0.46792149701418345\n",
      "0.44329782743745266\n",
      "0.37049888310566065\n",
      "0.3640198068119634\n",
      "0.18703642184435856\n",
      "0.45483667412658346\n",
      "0.4548354653488915\n",
      "0.46443625753165113\n",
      "0.471307735875292\n",
      "0.4748144265839921\n",
      "0.44977018471561236\n",
      "0.45483819741104875\n",
      "0.4424326210332081\n",
      "0.46952253953777556\n",
      "0.4844955725630404\n",
      "0.40711399675197274\n",
      "0.4683427592612184\n",
      "0.45300437675126237\n",
      "0.47699323055836884\n",
      "0.4476844916766524\n",
      "0.45437979965150055\n",
      "0.4586456132875175\n",
      "0.4367609807110681\n",
      "0.42440574615097126\n",
      "0.4031944654544016\n",
      "0.4701285609108507\n",
      "0.42100380722176134\n",
      "0.45469573742155384\n",
      "0.4389801384839866\n",
      "0.4980392006680118\n",
      "3.497955813781036e-05\n",
      "0.44095263169313026\n",
      "0.4980392006680118\n",
      "0.43757649314843994\n",
      "0.09387338724639278\n",
      "0.45454081673873803\n",
      "0.44984848853961357\n",
      "0.43760886539108756\n",
      "0.37903039631540814\n",
      "0.0020410574004546453\n",
      "0.03756738488428038\n",
      "0.18564549965540525\n",
      "0.3950228405258487\n",
      "0.30839967933029083\n",
      "0.4517721918523128\n",
      "0.39482868706461804\n",
      "0.47232378450895224\n",
      "0.4758914714649792\n",
      "0.41971065071936775\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.0009829085551517364\n",
      "0.4173652120015194\n",
      "0.4646438501164211\n",
      "0.4644479034992836\n",
      "0.4256491377535442\n",
      "0.4204364577517997\n",
      "0.4610389833093758\n",
      "0.16841642160942732\n",
      "0.4190614272028483\n",
      "0.45904713724889507\n",
      "0.47169685278573786\n",
      "0.2694190882195139\n",
      "0.0\n",
      "0.42928488339972726\n",
      "0.4980392006680118\n",
      "0.45410614510681274\n",
      "0.4251044694196968\n",
      "0.45682258900874034\n",
      "0.08598037635070808\n",
      "0.41424234050784475\n",
      "0.4980392006680118\n",
      "0.38543876250643916\n",
      "0.45114103705722924\n",
      "0.18162923092995176\n",
      "0.4242579603775792\n",
      "0.0969896946810356\n",
      "0.4980392006680118\n",
      "0.4757786946532614\n",
      "0.4598713432015685\n",
      "0.4700744088379275\n",
      "0.4577917242820524\n",
      "0.4721500347493654\n",
      "0.46589591436351924\n",
      "0.45067001727895795\n",
      "0.4218591336910168\n",
      "0.4731043431782693\n",
      "0.45502808371786096\n",
      "0.45391610994282616\n",
      "0.4546277514321494\n",
      "0.45850754378911757\n",
      "0.4490562903132943\n",
      "0.3912169888917862\n",
      "0.45700960855800427\n",
      "0.34553516426252706\n",
      "0.39148216236485334\n",
      "0.4077211707958453\n",
      "0.4067287455759794\n",
      "0.45773663517987434\n",
      "0.46499443330723705\n",
      "0.4495965718390626\n",
      "0.3837433672799917\n",
      "0.4476570280968894\n",
      "0.21891949411097975\n",
      "0.3440562997694041\n",
      "0.3368006221596862\n",
      "0.0\n",
      "0.4980392006680118\n",
      "0.40732990174789485\n",
      "0.0\n",
      "0.33405328896428554\n",
      "0.37366361315414187\n",
      "0.4661556623414042\n",
      "0.4400944611051135\n",
      "0.4867295530108262\n",
      "0.2525664872686396\n",
      "0.13185337810829725\n",
      "0.0148956683408883\n",
      "0.20536729321445174\n",
      "0.449124558025992\n",
      "0.40490938645358426\n",
      "0.20032205722609625\n",
      "0.16153509717059447\n",
      "0.43534146524816403\n",
      "0.3308558056200506\n",
      "0.17824920804486313\n",
      "0.0\n",
      "0.0\n",
      "0.46530447429355376\n",
      "0.4496694272260757\n",
      "0.4690908705245329\n",
      "0.45842476169348223\n",
      "0.44675874070118343\n",
      "0.4849755722141254\n",
      "0.4408045122187834\n",
      "0.4639465465057682\n",
      "0.4634137843253173\n",
      "0.0\n",
      "0.33902153602692947\n",
      "0.23634281272703794\n",
      "0.41859598021923433\n",
      "0.41802523390126783\n",
      "0.46230107599723064\n",
      "0.4584331215431207\n",
      "0.4620537478376288\n",
      "0.4454285448096808\n",
      "0.33126685275465634\n",
      "0.0\n",
      "0.4980392006680118\n",
      "0.46563961052885333\n",
      "0.3961042513503183\n",
      "0.4530550201146255\n",
      "0.46422082846175367\n",
      "0.43976326037305147\n",
      "0.10420959391164104\n",
      "0.44409032270546306\n",
      "0.43216620311377996\n",
      "0.4046567995383981\n",
      "0.4076534626695823\n",
      "0.29710402147886644\n",
      "0.45674420139968014\n",
      "0.4980392006680118\n",
      "0.24602974268070385\n",
      "0.42671983553612014\n",
      "0.0008196598551423332\n",
      "0.4331735795245675\n",
      "0.45094992177787274\n",
      "0.44402242514943446\n",
      "0.4337038142178458\n",
      "0.47170091208071974\n",
      "0.4376019377136897\n",
      "0.3768592652523273\n",
      "0.4785443556747612\n",
      "0.46319078148858345\n",
      "0.4419628357125176\n",
      "0.14037218906124904\n",
      "0.44307179283544423\n",
      "0.4140157233222367\n",
      "0.4566021023563034\n",
      "0.48227343175412557\n",
      "0.4375997727228602\n",
      "0.472583549260773\n",
      "0.4683453177334869\n",
      "0.464557903833695\n",
      "0.434288111092084\n",
      "0.4209322695719933\n",
      "0.4462721242542113\n",
      "0.40393134356966076\n",
      "0.4469879766232736\n",
      "0.41951195752239223\n",
      "0.4703152043649942\n",
      "0.45181943722690304\n",
      "0.43581041252725694\n",
      "0.4482735040496761\n",
      "0.4449660404008099\n",
      "0.45661134218556493\n",
      "0.4681103744989165\n",
      "0.4602102858838217\n",
      "0.4450307338132973\n",
      "0.46837225276137623\n",
      "0.428200655558971\n",
      "0.4698144937687682\n",
      "0.2835620122313792\n",
      "0.4479661232747127\n",
      "0.46014591797660276\n",
      "0.4685336020935984\n",
      "0.449157314254201\n",
      "0.45851023994241014\n",
      "0.4540777486386532\n",
      "0.4239831080993345\n",
      "0.40723652853131187\n",
      "0.4568398657862354\n",
      "0.46510250575164613\n",
      "0.42698859702373415\n",
      "0.2696681422102774\n",
      "0.46271591304905163\n",
      "0.46886968622347286\n",
      "0.4503529388576545\n",
      "0.4408968996113369\n",
      "0.40219309751835\n",
      "0.3413498642513887\n",
      "0.4234127785637801\n",
      "0.4361253121912313\n",
      "0.46465772595085564\n",
      "0.4270846872551767\n",
      "0.4785367297653053\n",
      "0.3954218282260017\n",
      "0.39587283162200865\n",
      "0.3989295230269429\n",
      "0.3716509678210415\n",
      "0.3780356488201367\n",
      "0.3950253058770672\n",
      "0.43734900049721764\n",
      "0.32868049357634416\n",
      "0.4734678808776317\n",
      "0.39061023477905055\n",
      "0.47180961964503887\n",
      "0.46672147664254376\n",
      "0.46890686213006794\n",
      "0.4085282088063797\n",
      "0.4271189274009162\n",
      "0.46990358959089124\n",
      "0.44325037287702107\n",
      "0.42701087415572286\n",
      "0.43987628460977796\n",
      "0.4574514375398497\n",
      "0.47442029546135017\n",
      "0.415689005137975\n",
      "0.4845254129595673\n",
      "0.07991462083605075\n",
      "0.4980392006680118\n",
      "0.4183882109034899\n",
      "0.0\n",
      "0.4300458504524115\n",
      "0.4116965751137177\n",
      "0.40908026284232357\n",
      "0.43374136026271476\n",
      "0.297296463226864\n",
      "0.4059528800430303\n",
      "0.1713462060311509\n",
      "0.2011978723581582\n",
      "0.45668761963438\n",
      "0.4678909035834934\n",
      "0.4104455185470837\n",
      "0.43476891092981196\n",
      "0.44183159703978303\n",
      "0.42972915269277406\n",
      "0.47236231850184984\n",
      "0.4670472397837755\n",
      "0.47667170877710807\n",
      "0.4435760872342627\n",
      "0.1423674503241459\n",
      "0.4658529121487776\n",
      "0.4335953405491149\n",
      "0.46513903063884865\n",
      "0.44792459101659665\n",
      "0.4202104361537123\n",
      "0.4659142304502424\n",
      "0.4780200141580622\n",
      "0.44220653802443083\n",
      "0.45091912743067825\n",
      "0.46708189945634015\n",
      "0.46300388320618857\n",
      "0.46858371180566466\n",
      "0.4068899253698101\n",
      "0.3430195536076577\n",
      "0.4507037281678018\n",
      "0.47039954245342513\n",
      "0.4618943478711101\n",
      "0.4346839083568596\n",
      "0.41764937289469434\n",
      "0.4661031102322053\n",
      "0.45988691287613676\n",
      "0.45596519679198005\n",
      "0.41470656936029704\n",
      "0.46934966656340665\n",
      "0.4766117405475886\n",
      "0.47117693452506076\n",
      "0.4739080168989016\n",
      "0.4615219946792159\n",
      "0.46580454128446197\n",
      "0.4077712383716308\n",
      "0.46904507784231614\n",
      "0.46430192533818077\n",
      "0.4633927457312746\n",
      "0.466333707041913\n",
      "0.46439350648668204\n",
      "0.47432886410487\n",
      "0.4199995665788957\n",
      "0.4980392006680118\n",
      "0.4638524357544027\n",
      "0.45863530056837115\n",
      "0.44685192887232345\n",
      "0.4533812988321166\n",
      "0.44613515498666106\n",
      "0.44460981741461125\n",
      "0.39936517387645887\n",
      "0.4659744077817526\n",
      "0.4673895645909533\n",
      "0.4727547928889882\n",
      "0.4409047253872648\n",
      "0.4586652751691135\n",
      "0.45416344079579357\n",
      "0.4414058982382809\n",
      "0.44659270793200684\n",
      "0.4205848801371395\n",
      "0.45231570607709304\n",
      "0.4432264428282304\n",
      "0.1775721745817363\n",
      "0.0\n",
      "0.44359969638851027\n",
      "0.4375655877913649\n",
      "0.40688356539079396\n",
      "0.4765693834340859\n",
      "0.3951795127865773\n",
      "0.4341170481220137\n",
      "0.4542401849095041\n",
      "0.46805297808455554\n",
      "0.46980485594646154\n",
      "0.4488982938913243\n",
      "0.43003661110852914\n",
      "0.4800403010211923\n",
      "0.4696922162614714\n",
      "0.46173989096231866\n",
      "0.481494947168349\n",
      "0.4095166363013488\n",
      "0.45725042422184947\n",
      "0.4442793721551619\n",
      "0.46804610381207096\n",
      "0.45855512226263717\n",
      "0.47644815744185914\n",
      "0.4610586554981485\n",
      "0.4289441771965924\n",
      "0.4720990735596972\n",
      "0.4980392006680118\n",
      "0.3940108303034552\n",
      "0.35900313846893595\n",
      "0.0\n",
      "0.48240189545123957\n",
      "0.4729231312142511\n",
      "0.4858312695763414\n",
      "0.47258460220575915\n",
      "0.4743678969620767\n",
      "0.4622579014198137\n",
      "0.4721349527289781\n",
      "0.41766606850185223\n",
      "0.433137537209277\n",
      "0.0\n",
      "0.4608901809372007\n",
      "0.4590677267085116\n",
      "0.48147601076711055\n",
      "0.47838333283297846\n",
      "0.4685533124329316\n",
      "0.4633244923253559\n",
      "0.46879039951692064\n",
      "0.47070776513929175\n",
      "0.4765056043299568\n",
      "0.44028065529584715\n",
      "0.47638190867566077\n",
      "0.3662072610543394\n",
      "0.0\n",
      "0.37593716383061077\n",
      "0.47744122684268525\n",
      "0.44036419299517254\n",
      "0.4980392006680118\n",
      "0.45896932167632687\n",
      "0.44337172912651485\n",
      "0.42738445383696827\n",
      "0.4980392006680118\n",
      "0.4440341673900201\n",
      "0.44275699709002825\n",
      "0.46759501847748947\n",
      "0.4544319818028139\n",
      "0.42577479204394614\n",
      "0.46001201565774386\n",
      "0.44072829138538533\n",
      "0.4330912025602501\n",
      "0.413859423043592\n",
      "0.46878472366396146\n",
      "0.4251877876504775\n",
      "0.23888734950305474\n",
      "0.40940737078923656\n",
      "0.4653019033478882\n",
      "0.4589752912042253\n",
      "0.47128133473650874\n",
      "0.4397052551389883\n",
      "0.4806050917527497\n",
      "0.24569507591297013\n",
      "0.4574480264673524\n",
      "0.4804908563048854\n",
      "0.444811763097883\n",
      "0.4778268504443054\n",
      "0.4735931383310759\n",
      "0.46563762278108606\n",
      "0.45923661522309484\n",
      "0.4740361505807108\n",
      "0.44477458653586827\n",
      "0.4232131551082547\n",
      "0.45759466018216555\n",
      "0.4734062933037676\n",
      "0.0\n",
      "0.4596156227296714\n",
      "0.45904347994392847\n",
      "0.4824083188702876\n",
      "0.42794142919694667\n",
      "0.4980392006680118\n",
      "0.4493278344572429\n",
      "0.4641987271901196\n",
      "0.46214250949287305\n",
      "0.4819023932233505\n",
      "0.4064403315367082\n",
      "0.4732077182933296\n",
      "0.4575494472814493\n",
      "0.4715659746916148\n",
      "0.4696707959154896\n",
      "0.4571946823206579\n",
      "0.4736181604290626\n",
      "0.4980392006680118\n",
      "0.44296098671214584\n",
      "0.4461023024977684\n",
      "0.3930773846262085\n",
      "0.4568887138386691\n",
      "0.4708262206265304\n",
      "0.45919069819949243\n",
      "0.4332766736711474\n",
      "0.4596784512932155\n",
      "0.45382901824382604\n",
      "0.42579103940490726\n",
      "0.48062637651937323\n",
      "0.48024294711131044\n",
      "0.4548530156075617\n",
      "0.4575786987123231\n",
      "0.40869616277926973\n",
      "0.47812778658420274\n",
      "0.48032139425125375\n",
      "0.4410203670301606\n",
      "0.4391305829361397\n",
      "0.4091535768257846\n",
      "0.41714152010750055\n",
      "0.47716263946653814\n",
      "0.39316083191156403\n",
      "0.040249104626571504\n",
      "0.4980392006680118\n",
      "0.43399505292312035\n",
      "0.4177126526576357\n",
      "0.46010975176982094\n",
      "0.4479664930753351\n",
      "0.375392170372622\n",
      "0.4519886305078994\n",
      "0.43586537980583756\n",
      "0.45893013767400404\n",
      "0.439927617138479\n",
      "0.44436438718332794\n",
      "0.4332629758331128\n",
      "0.44535017630790535\n",
      "0.4243953874746653\n",
      "0.4084668558912934\n",
      "0.43805152438892125\n",
      "0.45947178471472305\n",
      "0.4540353956214491\n",
      "0.0\n",
      "0.0033454916636490974\n",
      "0.0\n",
      "0.3397813747573143\n",
      "0.43217616952443794\n",
      "0.0\n",
      "0.0\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4053300612207969\n",
      "0.40107105867047466\n",
      "0.44253747480059497\n",
      "0.4326407026431446\n",
      "0.44816718479730694\n",
      "0.4562166403211104\n",
      "0.42659255878637486\n",
      "0.3694649987272878\n",
      "0.4431359007590005\n",
      "0.44141016947424283\n",
      "0.4200947731824055\n",
      "0.4264109558443034\n",
      "0.37890586167593104\n",
      "0.4095556992917004\n",
      "0.13832036184604823\n",
      "0.15648708291665242\n",
      "0.43757462249760787\n",
      "0.47573053368671\n",
      "0.4439955014729312\n",
      "0.46652076918678675\n",
      "0.07568593502296905\n",
      "0.45346077882214253\n",
      "0.20637959320814925\n",
      "0.4023890889535727\n",
      "0.42410427533984496\n",
      "0.30855932227415006\n",
      "0.13910701307664036\n",
      "0.45390179402479375\n",
      "0.4779949728603851\n",
      "0.0\n",
      "0.47790192173693846\n",
      "0.46191059368960746\n",
      "0.4705068202250817\n",
      "0.27246525032646024\n",
      "0.335135683378242\n",
      "0.3667384449999694\n",
      "0.07581574743025705\n",
      "0.3749252648985174\n",
      "0.33858813441341207\n",
      "0.412482823712271\n",
      "0.4487849405619247\n",
      "0.40713170050088865\n",
      "0.407179892713565\n",
      "0.475107114632329\n",
      "0.001887286188609465\n",
      "0.3729923241568248\n",
      "0.407069056884649\n",
      "0.46076183769951246\n",
      "0.4221791439145861\n",
      "0.3629234026730509\n",
      "0.3437523437920028\n",
      "0.436203693687633\n",
      "0.40893321821389\n",
      "0.3904695325066411\n",
      "0.4504655674253585\n",
      "0.4980392006680118\n",
      "0.1218914055307795\n",
      "0.45083575350276506\n",
      "0.4682003109053405\n",
      "0.4410781348034547\n",
      "0.334862033240412\n",
      "0.32427157871088813\n",
      "0.4308162178430342\n",
      "0.4267314370797706\n",
      "0.3129940081914666\n",
      "0.08726618904204705\n",
      "0.4980392006680118\n",
      "0.26679253342308845\n",
      "0.4980392006680118\n",
      "0.0\n",
      "0.4439458554800965\n",
      "0.349378405107872\n",
      "0.4399606965368332\n",
      "0.12166502032535134\n",
      "0.4024336838702102\n",
      "0.4650943716799884\n",
      "0.44186868767025245\n",
      "6.717911858981646e-05\n",
      "0.35452407637178324\n",
      "0.4980392006680118\n",
      "0.3914504842689145\n",
      "0.012772167018423975\n",
      "0.2047346463399513\n",
      "0.44032453429050705\n",
      "0.44493519467906556\n",
      "0.4344620520929246\n",
      "0.4720846557549209\n",
      "0.46350807295433166\n",
      "0.2781104599132078\n",
      "0.41094140438315235\n",
      "0.3976325393256628\n",
      "0.462051696312675\n",
      "0.13553098688027768\n",
      "0.4640355454274708\n",
      "0.46110282155833127\n",
      "0.4550571184919148\n",
      "0.4579512799736808\n",
      "0.007843870496227064\n",
      "0.4744544704773805\n",
      "0.4647020988592811\n",
      "0.3264979408137573\n",
      "0.4211629557104391\n",
      "0.3869008918348234\n",
      "0.46194772160885034\n",
      "0.3879965042054593\n",
      "0.42691821599854246\n",
      "0.37118178829243226\n",
      "0.42422662917102777\n",
      "0.45969434897990213\n",
      "0.4672478961132613\n",
      "0.4980392006680118\n",
      "0.4377887662876868\n",
      "0.0\n",
      "0.3803458756141127\n",
      "0.4329816766921454\n",
      "0.4718807649547808\n",
      "0.4279473310789271\n",
      "0.45356211514272726\n",
      "0.4980392006680118\n",
      "0.4728200991195158\n",
      "0.03375368087486659\n",
      "0.4232847173266586\n",
      "0.4584517031472666\n",
      "0.3616288293654423\n",
      "0.41633988580893183\n",
      "0.17854812690930377\n",
      "0.4737014549570367\n",
      "0.4504041538422753\n",
      "0.4599490655730341\n",
      "0.13647637270343008\n",
      "0.45902007459045496\n",
      "0.4980392006680118\n",
      "0.47690893035809623\n",
      "0.44170037041069277\n",
      "0.46021889503935137\n",
      "0.4131059523798266\n",
      "0.46742192874112387\n",
      "0.4728021972604572\n",
      "0.44582868417613003\n",
      "0.44712526723582474\n",
      "0.4747762463373066\n",
      "0.42410163003750945\n",
      "0.479190985364384\n",
      "0.4790189671370964\n",
      "0.4444623476294082\n",
      "0.46960472196425135\n",
      "0.39716156585671625\n",
      "0.452153352503428\n",
      "0.406103028577422\n",
      "0.35065905960009947\n",
      "0.002388733620577016\n",
      "0.16650267694906948\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.48638048143360424\n",
      "0.4980392006680118\n",
      "0.47301096293106054\n",
      "0.39351712023371715\n",
      "0.4798428565676255\n",
      "0.46316308260888867\n",
      "0.41803599519074464\n",
      "0.4455647710790611\n",
      "0.43363080959088485\n",
      "0.45978036796836796\n",
      "0.4661839740874831\n",
      "0.45193154788099565\n",
      "0.3830104791561273\n",
      "0.4073479392041559\n",
      "0.44900425503958824\n",
      "0.46668388991771736\n",
      "0.44535674399034003\n",
      "0.4350644736411979\n",
      "0.4797998440280708\n",
      "0.4727486173630037\n",
      "0.48283606881209645\n",
      "0.41572206680759033\n",
      "0.42474392673463063\n",
      "0.46635212014249156\n",
      "0.4816956299343644\n",
      "0.20953845469293694\n",
      "0.4804933192539443\n",
      "0.4636746414535894\n",
      "0.4752928805028981\n",
      "0.48614350691787184\n",
      "0.4663706264441972\n",
      "0.16462775741766558\n",
      "0.398104111506736\n",
      "0.49298276039547395\n",
      "0.2881234982721065\n",
      "0.46856720483682324\n",
      "0.43907891677771194\n",
      "0.4754708691997928\n",
      "0.4184893098850751\n",
      "0.4631413073300624\n",
      "0.4643619617794605\n",
      "0.4011438966775336\n",
      "0.33740371305051603\n",
      "0.39119063922922503\n",
      "0.3899721609331185\n",
      "0.42445698715533964\n",
      "0.4004947340198139\n",
      "0.4711376129355838\n",
      "0.4632335078253862\n",
      "0.4303175311132991\n",
      "0.3902909297124093\n",
      "0.460887543681998\n",
      "0.45303955959637965\n",
      "0.44816731805878823\n",
      "0.45983325992045654\n",
      "0.4749652955477775\n",
      "0.4386953929589143\n",
      "0.0\n",
      "0.43304967278024487\n",
      "0.40605926811737747\n",
      "0.0794325862448644\n",
      "0.48534824675999805\n",
      "0.47475171835148267\n",
      "0.22465081339432416\n",
      "0.42670360386629924\n",
      "0.4601647312100231\n",
      "0.42632111988462473\n",
      "0.47301845676076243\n",
      "0.4592495276802335\n",
      "0.4629892288021085\n",
      "0.4750640034226512\n",
      "0.458316315685357\n",
      "0.45591848138977875\n",
      "0.4546804349034214\n",
      "0.35954100628799107\n",
      "0.46124105067800286\n",
      "0.4554798624390005\n",
      "0.43515224896228305\n",
      "0.4539313846700724\n",
      "0.4381122385945557\n",
      "0.47327625953548746\n",
      "0.48084388715565124\n",
      "0.47847724815965204\n",
      "0.43779428105856455\n",
      "0.4524732842655021\n",
      "0.41944374825349817\n",
      "0.0005547299409062257\n",
      "0.3937311616836576\n",
      "0.459574428002327\n",
      "0.46530973706787737\n",
      "0.45040742384338645\n",
      "0.0\n",
      "0.36314712206689664\n",
      "0.4132478417111412\n",
      "0.35906815493273536\n",
      "0.30458411350284614\n",
      "0.4306688380777018\n",
      "0.4545171036619467\n",
      "0.4980392006680118\n",
      "0.4633484270519014\n",
      "0.4597360756543536\n",
      "0.31479089896216433\n",
      "0.42362299912516793\n",
      "0.4584402620379425\n",
      "0.4980392006680118\n",
      "0.2113878085189485\n",
      "0.004369482126526849\n",
      "0.42472750870736026\n",
      "0.26702624845875467\n",
      "0.34706101654575194\n",
      "0.3444684584454464\n",
      "0.4980392006680118\n",
      "0.4116900233162155\n",
      "0.30637500869950635\n",
      "0.4403082732585917\n",
      "0.4980392006680118\n",
      "0.43358375862392706\n",
      "0.3560135951512775\n",
      "0.0\n",
      "0.394192841623597\n",
      "0.2136044130487446\n",
      "0.41274322091757093\n",
      "0.45908088592130036\n",
      "0.2649021800890446\n",
      "0.4418833739954278\n",
      "0.4093858421298689\n",
      "0.4613132458439303\n",
      "0.3727224673221105\n",
      "0.4395952299655874\n",
      "0.3900565663241929\n",
      "0.46537397745750503\n",
      "0.0\n",
      "0.46337839861193186\n",
      "0.24946552287253967\n",
      "0.2879179277578054\n",
      "0.44166724909764565\n",
      "0.2918584572634614\n",
      "0.38710500552893695\n",
      "0.48835125522325307\n",
      "0.39066098877120015\n",
      "0.481502303039999\n",
      "0.45093877331251453\n",
      "0.4600232467011719\n",
      "0.4652301535053944\n",
      "0.42465941919415723\n",
      "0.4433007266164076\n",
      "0.4557548610568701\n",
      "0.4672208945268514\n",
      "0.42978453456463994\n",
      "0.463122038944342\n",
      "0.48908961337632306\n",
      "0.4558796505762981\n",
      "0.4554993740519117\n",
      "0.4171687640932934\n",
      "0.44532006969365834\n",
      "0.21693951821802654\n",
      "0.37957249622248007\n",
      "0.43228006785785367\n",
      "0.44370082899695096\n",
      "0.46715616843217744\n",
      "0.4190067065945508\n",
      "0.47549279181457416\n",
      "0.29473563736593716\n",
      "0.43490602249291893\n",
      "0.20910282605096467\n",
      "0.455088845071201\n",
      "0.4401882512259498\n",
      "0.4509076570487845\n",
      "0.4531126147811083\n",
      "0.4454633912115638\n",
      "0.40696135417823526\n",
      "0.46376207311742385\n",
      "0.4464508226957934\n",
      "0.19028282212787875\n",
      "0.40752266816211796\n",
      "0.4244493433587576\n",
      "0.45179387705655516\n",
      "0.3197228907318793\n",
      "0.4336652713146027\n",
      "0.37899248021100235\n",
      "0.4340657260961473\n",
      "0.4211414619819757\n",
      "0.4773182643527444\n",
      "0.4315561261013642\n",
      "0.4513299009592719\n",
      "0.4406643574250823\n",
      "0.47102068763020977\n",
      "0.4810930233751399\n",
      "0.38071667672166676\n",
      "0.09222592220296613\n",
      "0.03514633812843834\n",
      "0.19222386525106697\n",
      "0.45135761753896364\n",
      "0.003816719813703021\n",
      "0.16511622537493528\n",
      "0.14952855631326278\n",
      "0.397654135537403\n",
      "0.423597466530388\n",
      "0.3961663006106708\n",
      "0.4793564852387217\n",
      "0.3256324169342472\n",
      "0.4555028862483471\n",
      "0.4566642146569362\n",
      "0.46276390640804393\n",
      "0.431936527022234\n",
      "0.43869529685202874\n",
      "0.40779846135677633\n",
      "0.41182241562282046\n",
      "0.44869067490477743\n",
      "0.4780297269927656\n",
      "0.42431508709380883\n",
      "0.38751795905459935\n",
      "0.041463588160923284\n",
      "0.42531373659256555\n",
      "0.2774303571515911\n",
      "0.48263878069276445\n",
      "0.4625602773453977\n",
      "0.0\n",
      "0.4403001448370139\n",
      "0.4451029329337431\n",
      "0.427851405807602\n",
      "0.48428771247762625\n",
      "0.4038504303229432\n",
      "0.27673003295742477\n",
      "0.4104609882137797\n",
      "0.4609804404992779\n",
      "0.07756630910667521\n",
      "0.44211941961320456\n",
      "0.4255649204704007\n",
      "0.42651828619665\n",
      "0.4633982139381807\n",
      "0.4902723450163145\n",
      "0.45469150026069544\n",
      "0.3843433094073592\n",
      "0.41010445669427853\n",
      "0.39022837048809167\n",
      "0.47683658115124555\n",
      "0.2690678232719691\n",
      "0.3684259941740401\n",
      "0.4980392006680118\n",
      "0.3607135114801423\n",
      "0.4595754848830517\n",
      "0.41111169035911566\n",
      "0.4478707210742513\n",
      "0.38067935742774967\n",
      "0.2961561472657582\n",
      "0.4364782205777257\n",
      "0.45773177881799415\n",
      "0.30447911884036516\n",
      "0.46615828607273074\n",
      "0.44185161838846604\n",
      "0.48254835180284694\n",
      "0.45187916873833156\n",
      "0.48077026659351324\n",
      "0.4751362338854277\n",
      "0.3835359468749454\n",
      "0.33905763907710373\n",
      "0.47540050743622386\n",
      "0.4355780914502009\n",
      "0.47871680747078105\n",
      "0.46639207036283126\n",
      "0.4664753984706097\n",
      "0.4708339027855717\n",
      "0.47551849480307107\n",
      "0.4761899778921511\n",
      "0.4844482325294768\n",
      "0.004619270640015905\n",
      "0.3964414356607725\n",
      "0.4344198582760325\n",
      "0.3821548126299586\n",
      "0.448107182730431\n",
      "0.45649449646573215\n",
      "0.46569648786073203\n",
      "0.015026939080009374\n",
      "0.44262601379054156\n",
      "0.44971360357918366\n",
      "0.3665941035041009\n",
      "0.4101637074660981\n",
      "0.4776985871874238\n",
      "0.4488900839436452\n",
      "0.4442651327976906\n",
      "0.4680820805254612\n",
      "0.43150273636541514\n",
      "0.45623593194267237\n",
      "0.45125076975738665\n",
      "0.44992770361345563\n",
      "0.45987175260696106\n",
      "0.44658249063468314\n",
      "0.43471125124264254\n",
      "0.4423187286544887\n",
      "0.3801466326563287\n",
      "0.4060916945513223\n",
      "0.326566924210172\n",
      "0.0\n",
      "0.4451722249085399\n",
      "0.4150511736230042\n",
      "0.4646018060211945\n",
      "0.4980392006680118\n",
      "0.015457872322855223\n",
      "0.052973353495285445\n",
      "0.0016626674611892826\n",
      "0.10615856208770591\n",
      "0.021849023712855097\n",
      "0.4667422365462728\n",
      "0.11156201361292915\n",
      "0.1815103741913061\n",
      "0.44644302631957933\n",
      "0.4980392006680118\n",
      "0.368077009849381\n",
      "0.0\n",
      "0.45720196404165486\n",
      "0.005732466234942973\n",
      "0.43437357247205943\n",
      "0.4571618367364153\n",
      "0.4660381882664275\n",
      "0.4828247320118048\n",
      "0.46575727451489685\n",
      "0.47852980075923834\n",
      "0.16052311098860103\n",
      "0.47521740539314544\n",
      "0.4612228044992853\n",
      "0.45707401958346827\n",
      "0.4126477312190779\n",
      "0.4417170951789154\n",
      "0.45544446468650335\n",
      "0.4552367672801816\n",
      "0.4665132931501337\n",
      "0.02928942508593926\n",
      "0.4261033626799919\n",
      "0.3304334805677185\n",
      "0.4645535940257431\n",
      "0.4855370158994418\n",
      "0.46316711183737613\n",
      "0.45789895606800785\n",
      "0.42001276060005205\n",
      "0.45940231725896663\n",
      "0.16095665634846096\n",
      "0.0033037657633418634\n",
      "0.4012739922950165\n",
      "0.47398242662099654\n",
      "0.4465084379907244\n",
      "0.4619064520611486\n",
      "0.4247183262904507\n",
      "0.4471630027100554\n",
      "0.43686225246074956\n",
      "0.4659959399315714\n",
      "0.45212445798463824\n",
      "0.4561410162086024\n",
      "0.47584297657486574\n",
      "0.4821808874202126\n",
      "0.4773798886372437\n",
      "0.4717394197770263\n",
      "0.4744163619598263\n",
      "0.46299114741786335\n",
      "0.4737365458686299\n",
      "0.4980392006680118\n",
      "0.4165495423023025\n",
      "0.4603809608235488\n",
      "0.4640802883554288\n",
      "0.41217394411570685\n",
      "0.45720006186634454\n",
      "0.3756436363960152\n",
      "0.0\n",
      "0.02080823081475649\n",
      "0.4463909279997897\n",
      "0.45286420495566043\n",
      "0.40637126299649756\n",
      "0.4629066744654322\n",
      "0.35478929301119316\n",
      "0.42142733373763663\n",
      "0.4736819158141546\n",
      "0.47716574195722555\n",
      "0.4777158631518327\n",
      "0.47540225015340815\n",
      "0.46332087594821797\n",
      "0.4611825813206959\n",
      "0.4980392006680118\n",
      "0.3533065865694029\n",
      "0.0\n",
      "0.0\n",
      "0.4374543173416724\n",
      "0.0\n",
      "0.451888287401905\n",
      "0.3144623886794227\n",
      "0.42061298989982704\n",
      "0.3830761849506808\n",
      "0.2775321748451496\n",
      "0.47319804912453717\n",
      "0.4862765534727137\n",
      "0.4980392006680118\n",
      "0.47062285644776497\n",
      "0.44417264624485553\n",
      "0.36305862546108264\n",
      "0.41190705839082775\n",
      "0.47368383935765207\n",
      "0.442854127040362\n",
      "0.32352161189701834\n",
      "0.4338098800030611\n",
      "0.4708572890739127\n",
      "0.39010092430530613\n",
      "0.43617354268946407\n",
      "0.4735394948724654\n",
      "0.4770199454939354\n",
      "0.4635330454797722\n",
      "0.45601033364168614\n",
      "0.0\n",
      "0.4444472747435636\n",
      "0.4599891794846044\n",
      "0.42941575416969535\n",
      "0.20767933752676648\n",
      "0.3772737096108985\n",
      "0.24803921562760897\n",
      "0.3982809947700191\n",
      "0.4980392006680118\n",
      "0.4704720445520648\n",
      "0.4612260737956575\n",
      "0.4714844796330831\n",
      "0.4726763404137215\n",
      "0.058871793558982505\n",
      "0.4845560229238158\n",
      "0.45335537623562283\n",
      "0.0\n",
      "0.2862011098094572\n",
      "0.0074475232915655444\n",
      "0.4490395154286885\n",
      "0.48003428127607284\n",
      "0.45230569358822653\n",
      "0.4489188543457121\n",
      "0.4756763325501632\n",
      "0.027743662776868128\n",
      "0.3568351845835907\n",
      "0.33096935094147845\n",
      "0.34569737960767993\n",
      "0.0\n",
      "0.011821449316463695\n",
      "0.3918941931155618\n",
      "0.47540958408216155\n",
      "0.41335661120524636\n",
      "0.30973387942039404\n",
      "0.3226504875039599\n",
      "0.339249955045945\n",
      "0.0\n",
      "0.4738043581075105\n",
      "0.46678275285406534\n",
      "0.4802725502971051\n",
      "0.4787880528057376\n",
      "0.48387039311035346\n",
      "0.4303801588550221\n",
      "0.4644556047709043\n",
      "0.19027049047396713\n",
      "0.47372348100230277\n",
      "0.4486278856746795\n",
      "0.4796346806279761\n",
      "0.3699433751148386\n",
      "0.4344445193182855\n",
      "0.2748529400983821\n",
      "0.02504432754711166\n",
      "0.002991192808004979\n",
      "0.45606351825976726\n",
      "0.3741378033710426\n",
      "0.4980392006680118\n",
      "0.0\n",
      "0.24007655174235534\n",
      "0.4499292679843467\n",
      "0.44632552456418706\n",
      "0.3952857279417197\n",
      "0.4980392006680118\n",
      "0.15373106608466372\n",
      "0.01655810821025645\n",
      "0.4575290629660499\n",
      "0.011197649062015046\n",
      "0.32392999284478\n",
      "0.36803646878675067\n",
      "0.15106236170570406\n",
      "0.050050928969841514\n",
      "0.0\n",
      "0.00030494072644969214\n",
      "0.48068008327018713\n",
      "0.4980392006680118\n",
      "0.19201346940047442\n",
      "0.2481091990363622\n",
      "0.3800897652281365\n",
      "0.43739670176021417\n",
      "0.4741570369413673\n",
      "0.4980392006680118\n",
      "0.164705897254101\n",
      "0.4980392006680118\n",
      "0.47136414427377127\n",
      "0.0\n",
      "0.24803921562760897\n",
      "0.0\n",
      "0.3892288414422565\n",
      "0.0\n",
      "0.36023007535878027\n",
      "0.44534536763932786\n",
      "0.007130917903947766\n",
      "0.46097564421566234\n",
      "0.4610530253876536\n",
      "0.4327123326381556\n",
      "0.4517410554511394\n",
      "0.47695721114629247\n",
      "0.46650266918538236\n",
      "0.4335049428429875\n",
      "0.4184081337578307\n",
      "0.4745667754194197\n",
      "0.4649831990011303\n",
      "0.4658528380092747\n",
      "0.46741894743857\n",
      "0.44406538478188823\n",
      "0.4759403992322358\n",
      "0.4793295144686854\n",
      "0.4686664138971328\n",
      "0.45504909308756525\n",
      "0.4740793512447373\n",
      "0.44806825681024437\n",
      "0.4634796934856708\n",
      "0.44496531054654004\n",
      "0.445107009751917\n",
      "0.4599953371078639\n",
      "0.4776899283260336\n",
      "0.4537704488281448\n",
      "0.4271568057724307\n",
      "0.48508853922931644\n",
      "0.46305718952183605\n",
      "0.4746709380842039\n",
      "0.46390543248216654\n",
      "0.4672430279570184\n",
      "0.41120763489207807\n",
      "0.4786400013444212\n",
      "0.4980392006680118\n",
      "0.4831172209323694\n",
      "0.41132015605415545\n",
      "0.44714789776198954\n",
      "0.4819408298759742\n",
      "0.4675531963221364\n",
      "0.442653414072249\n",
      "0.4236784490547755\n",
      "0.47337463755920584\n",
      "0.4088208110856081\n",
      "0.45175087678392234\n",
      "0.4575190072823354\n",
      "0.48174896758955643\n",
      "0.4659384081207586\n",
      "0.46710629103492846\n",
      "0.4750298438722572\n",
      "0.47311572411841263\n",
      "0.44991451089421386\n",
      "0.39441826691321974\n",
      "0.47138193839680265\n",
      "0.47216382740985346\n",
      "0.4332327338270146\n",
      "0.45031135902534564\n",
      "0.4582931800826721\n",
      "0.3824554530499915\n",
      "0.4672361091129681\n",
      "0.4699194411569159\n",
      "0.46758521176945617\n",
      "0.4642664758091812\n",
      "0.44327590788156285\n",
      "0.445379580194573\n",
      "0.44180943164498754\n",
      "0.46870706076535595\n",
      "0.43096179006547125\n",
      "0.47240131784830275\n",
      "0.4762350303658535\n",
      "0.47530903642263644\n",
      "0.4711765315906763\n",
      "0.47073516881840244\n",
      "0.47400158100108125\n",
      "0.4742549100443456\n",
      "0.47503384266721593\n",
      "0.44530670662675575\n",
      "0.4446953761437934\n",
      "0.4777778744630003\n",
      "0.47586348288306457\n",
      "0.46157059712004717\n",
      "0.46236124865767697\n",
      "0.45502520530220475\n",
      "0.477014487921191\n",
      "0.4770058376973548\n",
      "0.4526458469266519\n",
      "0.24803921562760897\n",
      "0.4980392006680118\n",
      "0.017815251948527105\n",
      "0.4642583935718833\n",
      "0.47062547668370686\n",
      "0.45571937491045117\n",
      "0.46099746350553966\n",
      "0.4572105521005488\n",
      "0.4717033767541305\n",
      "0.4585642391439863\n",
      "0.47239482408716205\n",
      "0.4607802754130118\n",
      "0.46261993635751403\n",
      "0.42899841844339826\n",
      "0.47838747591553005\n",
      "0.46181315323810196\n",
      "0.4800250861715205\n",
      "0.464495036070344\n",
      "0.4980392006680118\n",
      "0.41227397809901045\n",
      "0.45823747247234414\n",
      "0.11879857299527062\n",
      "0.4704019709229124\n",
      "0.44652131265458095\n",
      "0.45595670487874607\n",
      "0.4392678037587644\n",
      "0.44222261044339495\n",
      "0.44717091328010417\n",
      "0.4308741704008116\n",
      "0.4187695549915196\n",
      "0.44902115102286594\n",
      "0.4539205007761158\n",
      "0.42737101848550235\n",
      "0.27112500536593137\n",
      "0.42092968662690283\n",
      "0.4622504219316608\n",
      "0.464649032512883\n",
      "0.4546428953575306\n",
      "0.44689901044944624\n",
      "0.46872330616649\n",
      "0.44367620350834003\n",
      "0.32033745081176346\n",
      "0.30223389467717854\n",
      "0.3761954076856173\n",
      "0.388443360518616\n",
      "0.3290115439223399\n",
      "0.4446822598046192\n",
      "0.43494745025498605\n",
      "0.4166578167817624\n",
      "0.4708242063936324\n",
      "0.37530102339796967\n",
      "0.47570354237888146\n",
      "0.4775336881410096\n",
      "0.48156244576025875\n",
      "0.47683905789654396\n",
      "0.43187650502946506\n",
      "0.47754593160064496\n",
      "0.48374153346174137\n",
      "0.47147132103695927\n",
      "0.4778698333449383\n",
      "0.4741308866713568\n",
      "0.48193529993221607\n",
      "0.33252501351299235\n",
      "0.41430771519971227\n",
      "0.4835791769263781\n",
      "0.46791654319300735\n",
      "0.4324891128594414\n",
      "0.4629958983577596\n",
      "0.46755988934683235\n",
      "0.47087334367767697\n",
      "0.4174415210616741\n",
      "0.48117350355505356\n",
      "0.4663824122845135\n",
      "0.4780516987362404\n",
      "0.47196122401238366\n",
      "0.47158157176381355\n",
      "0.47195631047379805\n",
      "0.0\n",
      "0.4876473774462154\n",
      "0.47715895221997606\n",
      "0.48437224692652775\n",
      "0.48054671977462887\n",
      "0.4846027636438202\n",
      "0.48218552975213924\n",
      "0.4824542306549165\n",
      "0.48162153748369035\n",
      "0.4354333371654331\n",
      "0.4980392006680118\n",
      "0.4353683728499759\n",
      "0.47261288260228457\n",
      "0.47555444545329467\n",
      "0.46793842677017605\n",
      "0.46303391360949975\n",
      "0.451630921924503\n",
      "0.4552451121179865\n",
      "0.44834482742213577\n",
      "0.38392606683190766\n",
      "0.4402056179440531\n",
      "0.4020358300032916\n",
      "0.4486393535890059\n",
      "0.33694045107021936\n",
      "0.461586483835368\n",
      "0.4781316261657021\n",
      "0.45631915851417104\n",
      "0.41023466418666144\n",
      "0.4716891188237473\n",
      "0.41236710433070706\n",
      "0.4691490027849373\n",
      "0.3003124446241938\n",
      "0.3645750895254586\n",
      "0.4749718091341057\n",
      "0.4543000042587317\n",
      "0.3798773066481295\n",
      "0.4183664699400519\n",
      "0.3385227405506279\n",
      "0.12790439865035716\n",
      "0.1402333079546682\n",
      "0.002942673525263645\n",
      "0.0\n",
      "0.4708919845887883\n",
      "0.4478499308002749\n",
      "0.47361225512620797\n",
      "0.052154841848377015\n",
      "0.4408964737375815\n",
      "0.33755329910233134\n",
      "0.41335155758962466\n",
      "0.26230428293936947\n",
      "0.35851140852577046\n",
      "0.46214924084936854\n",
      "0.4277068557156675\n",
      "0.41700284952578426\n",
      "0.428534882494876\n",
      "0.4763854723887637\n",
      "0.42642181861195977\n",
      "0.38899055857334547\n",
      "0.21556012948863468\n",
      "0.3034972038726752\n",
      "0.42213730994183496\n",
      "0.4980392006680118\n",
      "0.21848141033714713\n",
      "0.413128709315622\n",
      "0.4449705891599339\n",
      "0.43181912737874684\n",
      "0.41211778364933155\n",
      "0.4738472094698407\n",
      "0.47659777969814354\n",
      "0.4109609380122126\n",
      "0.0\n",
      "0.4002040822656939\n",
      "0.0\n",
      "0.45043711477053894\n",
      "0.4509915019447147\n",
      "0.47050521865529293\n",
      "0.46432270045471774\n",
      "0.0\n",
      "0.4860704290786949\n",
      "0.2686923032585693\n",
      "0.4398014007186758\n",
      "0.45697172937191766\n",
      "0.43780356932717646\n",
      "0.4101931195296517\n",
      "0.4554312020040129\n",
      "0.43660376039521737\n",
      "0.02707288003442087\n",
      "0.07678334850565499\n",
      "0.10624386577947002\n",
      "0.42257870021069893\n",
      "0.4384118083997143\n",
      "0.40368974291811965\n",
      "0.4980392006680118\n",
      "0.0\n",
      "0.32865324890769737\n",
      "0.4456677697587718\n",
      "0.47432456936204354\n",
      "0.46470274476129\n",
      "0.45944053574643945\n",
      "0.4697223202255813\n",
      "0.4689659486025627\n",
      "0.4669067383246987\n",
      "0.45383028748818816\n",
      "0.4597584777668085\n",
      "0.44226744256846007\n",
      "0.4544511254920391\n",
      "0.4485357700759592\n",
      "0.0005931425216855746\n",
      "0.4373411379094275\n",
      "0.46078049778325797\n",
      "0.4723319567435445\n",
      "0.4608349133782575\n",
      "0.4980392006680118\n",
      "0.09933043560713696\n",
      "0.0\n",
      "0.34230393615631916\n",
      "0.0\n",
      "0.47060676013388186\n",
      "0.40782526776270195\n",
      "0.4840735996334277\n",
      "0.475361067338189\n",
      "0.477963232485612\n",
      "0.47641673627277775\n",
      "0.458669933300938\n",
      "0.47533231461519015\n",
      "0.4824036531009612\n",
      "0.3957411768275846\n",
      "0.480192834615607\n",
      "0.4774098384923022\n",
      "0.4839562128809631\n",
      "0.4797044629673098\n",
      "0.478658270107495\n",
      "0.46314479559530425\n",
      "0.4693413858388476\n",
      "0.46480446277846527\n",
      "0.07014610033341236\n",
      "0.4022466142830978\n",
      "0.27153459629447835\n",
      "0.400271925640363\n",
      "0.46559808059302676\n",
      "0.4655649865132789\n",
      "0.4809295782290594\n",
      "0.41515399839464806\n",
      "0.48346952004444815\n",
      "0.4736809135369515\n",
      "0.47524854871911626\n",
      "0.4740242417282217\n",
      "0.4546713028389074\n",
      "0.48440665913793524\n",
      "0.4850507671522372\n",
      "0.468380628380924\n",
      "0.4523016045460792\n",
      "0.4080236434140202\n",
      "0.48206534871531675\n",
      "0.47712753920051715\n",
      "0.39981006047396633\n",
      "0.338878359500893\n",
      "0.47198809868040426\n",
      "0.47284523736245043\n",
      "0.4692948518501718\n",
      "0.46870255696909\n",
      "0.41243163499223706\n",
      "0.4700752789579803\n",
      "0.4615719286130837\n",
      "0.4678174112946079\n",
      "0.4730549192378121\n",
      "0.48010147760993677\n",
      "0.44155275627338497\n",
      "0.4531218068382786\n",
      "0.47896354648931516\n",
      "0.4749054924432857\n",
      "0.4684149331339248\n",
      "0.47604530724515687\n",
      "0.4796788705088196\n",
      "0.40515400919325883\n",
      "0.47595265741921466\n",
      "0.46017242589803875\n",
      "0.4188992822616041\n",
      "0.46415639070366715\n",
      "0.48535524073448083\n",
      "0.4818575404208682\n",
      "0.4897383555488313\n",
      "0.4795190243055925\n",
      "0.4780172146433598\n",
      "0.45499772179077275\n",
      "0.4600290102056616\n",
      "0.4825825445358895\n",
      "0.4673654808828932\n",
      "0.4767881679444112\n",
      "0.4980392006680118\n",
      "0.4609725778443634\n",
      "0.48069453892884756\n",
      "0.4419944978652255\n",
      "0.47265722422559114\n",
      "0.4344602534989348\n",
      "0.4685516087921925\n",
      "0.45453619909338294\n",
      "0.46278388493793277\n",
      "0.46039598711952673\n",
      "0.4688203544458188\n",
      "0.4562793468014239\n",
      "0.47538102720805225\n",
      "0.46697955385290035\n",
      "0.473723791677492\n",
      "0.4753155223632199\n",
      "0.4682506345943606\n",
      "0.45145414352534713\n",
      "0.4740316254336958\n",
      "0.4609589224241959\n",
      "0.4561659009525077\n",
      "0.0\n",
      "0.12809937299753693\n",
      "0.4717582609884045\n",
      "0.021868376304716705\n",
      "0.0\n",
      "0.4235903799453295\n",
      "0.3092484358758977\n",
      "0.47214735214119335\n",
      "0.4732832665612227\n",
      "0.4627261862428131\n",
      "0.37218696104232646\n",
      "0.43471269468251517\n",
      "0.45146763539454393\n",
      "0.41377386645646874\n",
      "0.4596687761088069\n",
      "0.4169655596710287\n",
      "0.4743242985068585\n",
      "0.4781122551209482\n",
      "0.47261052571898254\n",
      "0.4695469987666547\n",
      "0.4487040365602369\n",
      "0.3429636546730856\n",
      "0.48316712956992186\n",
      "0.46581529313297343\n",
      "0.23481250282761804\n",
      "0.4674861650262026\n",
      "0.4800797562861788\n",
      "0.47418472907690895\n",
      "0.47127025815232304\n",
      "0.46944440892696493\n",
      "0.4761440152179142\n",
      "0.48077232558696903\n",
      "0.4694407350092517\n",
      "0.3944595034273927\n",
      "0.46285550668528763\n",
      "0.4062694249810678\n",
      "0.46803248879549014\n",
      "0.4842995989267696\n",
      "0.48334054478598043\n",
      "0.48279875579090803\n",
      "0.48634429309256516\n",
      "0.48818545740218083\n",
      "0.48520860646011454\n",
      "0.4588638252774613\n",
      "0.4039054800069088\n",
      "0.4980392006680118\n",
      "0.44476522320891254\n",
      "0.48040671252640144\n",
      "0.45218555572125696\n",
      "0.4589468976089403\n",
      "0.43067509705955304\n",
      "0.46547859266736985\n",
      "0.4141732563736119\n",
      "0.4533321148605715\n",
      "0.4703973129474555\n",
      "0.45401992516103573\n",
      "0.47797961276738093\n",
      "0.4761320403910991\n",
      "0.38934783584187216\n",
      "0.39522011838394755\n",
      "0.24055370205073096\n",
      "0.4200941115169093\n",
      "0.4607107622921994\n",
      "0.18697364294874813\n",
      "0.48230532033061846\n",
      "0.31240080892382466\n",
      "0.46123159116439577\n",
      "0.43804116999042514\n",
      "0.4619963831660939\n",
      "0.44428035247073727\n",
      "0.46403721702970874\n",
      "0.4462774199638792\n",
      "0.3827919168801252\n",
      "0.4606243014422292\n",
      "0.4301853085975841\n",
      "0.40228618690103135\n",
      "0.4729405576513751\n",
      "0.42825434524394984\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4805851193741647\n",
      "0.46513323389582445\n",
      "0.0\n",
      "0.3096211888735028\n",
      "0.33745548360766614\n",
      "0.27408540763948025\n",
      "0.46867356000541477\n",
      "0.35474396732364805\n",
      "0.25054133935145173\n",
      "0.0\n",
      "0.4980392006680118\n",
      "0.4850188886374433\n",
      "0.3333780413647002\n",
      "0.27734745511584735\n",
      "0.30097857053542676\n",
      "0.301795997513928\n",
      "0.4518806896357205\n",
      "0.3199047190351701\n",
      "0.42406784434822176\n",
      "0.4505055212713661\n",
      "0.4435207177080213\n",
      "0.44754123096004494\n",
      "0.32317192643847176\n",
      "0.34467017810565237\n",
      "0.09015124742954012\n",
      "0.4215162738950069\n",
      "0.4980392006680118\n",
      "0.46318797221128355\n",
      "0.3995792391189788\n",
      "0.0\n",
      "0.337390333731171\n",
      "0.4190556455265483\n",
      "0.4980392006680118\n",
      "0.4018837476841769\n",
      "0.28324209233470216\n",
      "0.06976051817275454\n",
      "0.47780426215397387\n",
      "0.41177627037070624\n",
      "0.3670148284776196\n",
      "0.44513454956716864\n",
      "0.4689063154006048\n",
      "0.4727125521870037\n",
      "0.4614106750983074\n",
      "0.44141550904938104\n",
      "0.4422145474404057\n",
      "0.47998357098656386\n",
      "0.4807192449534596\n",
      "0.48653013475497403\n",
      "0.4875991482421954\n",
      "0.47885188920929633\n",
      "0.4718308295737588\n",
      "0.46733212510281663\n",
      "0.47675616814666544\n",
      "0.4980392006680118\n",
      "0.44620864087940565\n",
      "0.4809654448200971\n",
      "0.4827113422057991\n",
      "0.4855225479342391\n",
      "0.48143993078092645\n",
      "0.4592923843506113\n",
      "0.48483668038651984\n",
      "0.44841562354756587\n",
      "0.4058573678231178\n",
      "0.4735502929827404\n",
      "0.45632464216500734\n",
      "0.43341048736535165\n",
      "0.0\n",
      "0.4980392006680118\n",
      "0.3225600553725199\n",
      "0.4980392006680118\n",
      "0.11127113553397874\n",
      "0.3983254889543777\n",
      "0.398805195703276\n",
      "0.0\n",
      "0.07156625862885813\n",
      "0.3972397889639128\n",
      "0.4589744003522219\n",
      "0.03833709965878095\n",
      "0.25799175700919563\n",
      "0.29508603618749046\n",
      "0.3172648771420093\n",
      "0.35895319321654423\n",
      "0.31550170173673336\n",
      "0.1292018391999526\n",
      "0.3094132153202084\n",
      "0.3763492594673128\n",
      "0.34144206574596186\n",
      "0.4367530380569911\n",
      "0.4177370028983031\n",
      "0.3975509149127942\n",
      "0.38468952556135216\n",
      "0.4365501981886769\n",
      "0.41452071531817364\n",
      "0.4547636953052615\n",
      "0.4150406527521463\n",
      "0.0\n",
      "0.4359491043198304\n",
      "0.38805658469003673\n",
      "0.0\n",
      "0.4253753785912404\n",
      "0.4980392006680118\n",
      "0.4036029253200987\n",
      "0.32643594417446703\n",
      "0.24974395511500966\n",
      "0.4980392006680118\n",
      "0.43933606569593353\n",
      "1.1496673806547179e-05\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.23952536424966714\n",
      "0.32093513347881086\n",
      "0.46409978517112127\n",
      "0.4980392006680118\n",
      "0.46042464072061184\n",
      "0.4606847155569466\n",
      "0.44750256024302315\n",
      "0.45235738347242094\n",
      "0.0\n",
      "0.4782236115464839\n",
      "0.46859870560793077\n",
      "0.4744456785084048\n",
      "0.477106920589371\n",
      "0.46540611584441755\n",
      "0.47946016700794997\n",
      "0.4815273690576487\n",
      "0.0\n",
      "0.4723867980544951\n",
      "0.4872740206457882\n",
      "0.48548124396632414\n",
      "0.4838939922089595\n",
      "0.4685790260930218\n",
      "0.47314315570639515\n",
      "0.4817144119611101\n",
      "0.38015436386730295\n",
      "0.4711199390195557\n",
      "0.3599598330505486\n",
      "0.4431906986808992\n",
      "0.4246356595531481\n",
      "0.3737126649817409\n",
      "0.35911267781751594\n",
      "0.45268771745748854\n",
      "0.4178946617545797\n",
      "0.4181394261904755\n",
      "0.3771052160664407\n",
      "0.40712057488374426\n",
      "0.37395298157618123\n",
      "0.3924035687379153\n",
      "0.38975856586573754\n",
      "0.4037210028181536\n",
      "0.47497844856873533\n",
      "0.4674134350182704\n",
      "0.4378782341911975\n",
      "0.4547124583747009\n",
      "0.4547336683814745\n",
      "0.47504112580252167\n",
      "0.43354986235148113\n",
      "0.39839790144787374\n",
      "0.4227664682577643\n",
      "0.3780863802114269\n",
      "0.4147080205374175\n",
      "0.40397941401236176\n",
      "0.41656379852231257\n",
      "0.44110835621900074\n",
      "0.35229516507215697\n",
      "0.43509286554683424\n",
      "0.37187959199982185\n",
      "0.45552961193181596\n",
      "0.46460035599365307\n",
      "0.45447796145666325\n",
      "0.4349029345553718\n",
      "0.4688003997886319\n",
      "0.41369977909574385\n",
      "0.4013496357040078\n",
      "0.38707314138093624\n",
      "0.45684410900153977\n",
      "0.450334685302788\n",
      "0.45812084048930807\n",
      "0.4980392006680118\n",
      "0.44803167930454846\n",
      "0.4047430547717028\n",
      "0.44428488554501055\n",
      "0.4545635167829065\n",
      "0.44683556914132194\n",
      "0.3883714334668089\n",
      "0.37147196277897326\n",
      "0.3908598566940379\n",
      "0.40215402623769936\n",
      "0.44237889848404605\n",
      "0.476053367752268\n",
      "0.4237302990776185\n",
      "0.4140433712735916\n",
      "0.4463450814562384\n",
      "0.4155821794355498\n",
      "0.3342738180743612\n",
      "0.04730001836431879\n",
      "0.4220640570574038\n",
      "0.0\n",
      "0.4583462222492724\n",
      "0.40718720230216693\n",
      "0.4559926254383254\n",
      "0.44556874517830763\n",
      "0.3604166706756816\n",
      "0.4303121419962138\n",
      "0.16604073082259127\n",
      "0.0\n",
      "0.34100788703854396\n",
      "0.3295350165484782\n",
      "0.4980392006680118\n",
      "0.08392245015642125\n",
      "0.4980392006680118\n",
      "0.4611642355587952\n",
      "0.43516421680219686\n",
      "0.4475889073264295\n",
      "0.4420291008721148\n",
      "0.3006320032419455\n",
      "0.4549465033579211\n",
      "0.45327977221205523\n",
      "0.37127678490512195\n",
      "0.43191478028897795\n",
      "0.02303948493939617\n",
      "0.4610821052214249\n",
      "0.26941844346875926\n",
      "0.38041991373352996\n",
      "0.402904415804269\n",
      "0.018311485642393762\n",
      "0.06556826774004597\n",
      "0.0\n",
      "0.4317035339949242\n",
      "0.4516686603165649\n",
      "0.001772535583759355\n",
      "0.4727347705748414\n",
      "0.4625965996650867\n",
      "0.46490895706053964\n",
      "0.409924631457574\n",
      "0.4713946956084691\n",
      "0.46596330755090026\n",
      "0.4565415496544077\n",
      "0.45090012590826417\n",
      "0.47557511841057337\n",
      "0.02016309768349946\n",
      "0.18844868930872496\n",
      "0.0690780771745002\n",
      "0.3495251830295962\n",
      "0.45234796684544015\n",
      "0.4060699673177447\n",
      "0.09287740368045005\n",
      "0.4447577187633247\n",
      "0.4572947109067355\n",
      "0.4491342272985521\n",
      "0.4622542088614818\n",
      "0.4595913619460185\n",
      "0.35155785124222855\n",
      "0.35239442372789787\n",
      "0.4696612912276388\n",
      "0.4980392006680118\n",
      "0.40521435724115834\n",
      "0.43989859587511876\n",
      "0.44145487915677817\n",
      "0.4542681877477126\n",
      "0.4158992717392481\n",
      "0.29895829542971986\n",
      "0.39533418309448876\n",
      "0.4822526471287059\n",
      "0.45596084923686914\n",
      "0.3591333492870513\n",
      "0.4305467959148608\n",
      "0.3975495517805837\n",
      "0.4400031579330225\n",
      "0.42466255705697853\n",
      "0.19873217942911214\n",
      "0.48182362052034244\n",
      "0.3950818633150104\n",
      "0.4131218746344337\n",
      "0.4519666623136038\n",
      "0.47691005957516086\n",
      "0.4657071307696794\n",
      "0.4721950977307037\n",
      "0.43356754314434937\n",
      "0.31549040840671927\n",
      "0.4691986992456332\n",
      "0.4765273739832425\n",
      "0.44968036183931365\n",
      "0.47245131981982896\n",
      "0.46355544679615496\n",
      "0.47567713878274764\n",
      "0.48083567761963764\n",
      "0.46119640487956653\n",
      "0.47952901021570477\n",
      "0.37056257296318557\n",
      "0.48119596916475643\n",
      "0.4746922354916355\n",
      "0.47746651925132744\n",
      "0.42491913985707447\n",
      "0.4980392006680118\n",
      "0.44913775550088486\n",
      "0.43189021512192466\n",
      "0.4980392006680118\n",
      "0.14239681059081646\n",
      "0.32498249478474456\n",
      "0.4239460795145213\n",
      "0.388534101048332\n",
      "0.44287161086234295\n",
      "0.4584035006799436\n",
      "0.0\n",
      "0.4274179929386819\n",
      "0.4445540499202406\n",
      "0.43427345097203707\n",
      "0.28035971020386075\n",
      "0.4289157232100746\n",
      "0.4304164992163359\n",
      "0.3904644485120639\n",
      "0.4980392006680118\n",
      "0.0\n",
      "0.45736479601247915\n",
      "0.42084196146453723\n",
      "0.23791251531143331\n",
      "0.43022685720178355\n",
      "0.42996135965310445\n",
      "0.45657532218292585\n",
      "0.43766820804512074\n",
      "0.4376145522448876\n",
      "0.47800867281553694\n",
      "0.46438011448789795\n",
      "0.46808856344593347\n",
      "0.3389706182007362\n",
      "0.24762268435739412\n",
      "0.3226511868392498\n",
      "0.03043242793140223\n",
      "0.020335796471207507\n",
      "0.4782870723281392\n",
      "0.4678165886889405\n",
      "0.44979164980778474\n",
      "0.36822558043681336\n",
      "0.4543053864365632\n",
      "0.30100363152451404\n",
      "0.4640618857456854\n",
      "0.4427449305261845\n",
      "0.4755769416386662\n",
      "0.4724000383097284\n",
      "0.48257567697756243\n",
      "0.46483704679652493\n",
      "0.46974012323363434\n",
      "0.47602490298634853\n",
      "0.23095255925385968\n",
      "0.4352742340734147\n",
      "0.47715147650366985\n",
      "0.4718324505524031\n",
      "0.4761967058696527\n",
      "0.48047021631595993\n",
      "0.45366786538316084\n",
      "0.4582605002508698\n",
      "0.46336111967922666\n",
      "0.46719642922121934\n",
      "0.4980392006680118\n",
      "0.4744377416063855\n",
      "0.45842747872911416\n",
      "0.42963564305927376\n",
      "0.467824292610365\n",
      "0.4694676765252902\n",
      "0.4854203673780251\n",
      "0.46875877830275536\n",
      "0.4729087376170108\n",
      "0.46528244502471056\n",
      "0.4569588408788436\n",
      "0.43968260755802335\n",
      "0.43944069316783707\n",
      "0.4613276961997836\n",
      "0.4731862903039693\n",
      "0.463008571745927\n",
      "0.4583432022359381\n",
      "0.47537873367232153\n",
      "0.4707277015103606\n",
      "0.4785730697120814\n",
      "0.46688145663974495\n",
      "0.46355012023915576\n",
      "0.0\n",
      "0.44636177141558137\n",
      "0.45890101595276284\n",
      "0.4189959066693215\n",
      "0.43831817313408966\n",
      "0.428993183122016\n",
      "0.44917520968474683\n",
      "0.4578551981391844\n",
      "0.46664604736953896\n",
      "0.47731807575411567\n",
      "0.46651793354860144\n",
      "0.46526322657649705\n",
      "0.4147506971346881\n",
      "0.0\n",
      "0.46345999868726534\n",
      "0.464033483358642\n",
      "0.4364491040285743\n",
      "0.463450648258457\n",
      "0.0\n",
      "0.407288407152899\n",
      "0.42510289449904975\n",
      "0.4980392006680118\n",
      "0.38470095749354705\n",
      "0.45171702350084203\n",
      "0.14469406004526797\n",
      "0.21463899738857833\n",
      "0.2585433803846599\n",
      "0.42719339424744734\n",
      "0.43706889929435694\n",
      "0.15585119272592493\n",
      "0.45801181140731095\n",
      "0.4216236740211353\n",
      "0.4243375716179607\n",
      "0.23847494637262298\n",
      "0.3844013600706815\n",
      "0.4729897433095109\n",
      "0.4666815363579819\n",
      "0.4486764381728785\n",
      "0.447152551005372\n",
      "0.4567227823735639\n",
      "0.4619292385619717\n",
      "0.16157607269445304\n",
      "0.49071111977818405\n",
      "0.49010900677091823\n",
      "0.4873969392585272\n",
      "0.48006764267820834\n",
      "0.46738717256388174\n",
      "0.40745971450447965\n",
      "0.4877916851539621\n",
      "0.47270408668098035\n",
      "0.4721932195134529\n",
      "0.45985611122757125\n",
      "0.4547650412357792\n",
      "0.46635867247652923\n",
      "0.48167467739480935\n",
      "0.4768718707774607\n",
      "0.4807040438882227\n",
      "0.46373077818166475\n",
      "0.4791937556584665\n",
      "0.4833249061636714\n",
      "0.48673631515105925\n",
      "0.4841846894715076\n",
      "0.47721416134786293\n",
      "0.4833496044381305\n",
      "0.4775420948155547\n",
      "0.4821341720537779\n",
      "0.4720360501325294\n",
      "0.4815332117654931\n",
      "0.3849084003267546\n",
      "0.4725758694711695\n",
      "0.47965990482793813\n",
      "0.4722496453462592\n",
      "0.48776337182284824\n",
      "0.47292717306304166\n",
      "0.4862260319286519\n",
      "0.49221833605126636\n",
      "0.4768988542145632\n",
      "0.4768629069014393\n",
      "0.4581267754562621\n",
      "0.48401506324919924\n",
      "0.45705370994424255\n",
      "0.4859012320692297\n",
      "0.478130964256334\n",
      "0.4849024344919443\n",
      "0.0\n",
      "0.4919737453559926\n",
      "0.48968339156946106\n",
      "0.4849862608137434\n",
      "0.4814807174825732\n",
      "0.4724674048152811\n",
      "0.46971855618336217\n",
      "0.44565728206036154\n",
      "0.48565237607853007\n",
      "0.47553993314803716\n",
      "0.4810808248644635\n",
      "0.47446165498667947\n",
      "0.47860658767712916\n",
      "0.47809452650584583\n",
      "0.0016383895546392348\n",
      "0.48252667747262784\n",
      "0.42075260462830616\n",
      "0.46720436926067505\n",
      "0.479883720948391\n",
      "0.4878256288900018\n",
      "0.4242931533143676\n",
      "0.46908645743759914\n",
      "0.462885009212657\n",
      "0.47708861175714207\n",
      "0.4703628856658402\n",
      "0.0\n",
      "0.48603131602792526\n",
      "0.48088699122069745\n",
      "0.48342046430039\n",
      "0.46015716842140686\n",
      "0.4697794959020935\n",
      "0.39758724153297575\n",
      "0.4529917670449134\n",
      "0.4565419343486906\n",
      "0.43769818763744395\n",
      "0.4575479430351488\n",
      "0.4756583999741903\n",
      "0.4681659048178351\n",
      "0.4776544199222197\n",
      "0.4756249782886225\n",
      "0.47372953008009505\n",
      "0.4706725534763312\n",
      "0.47841577899591214\n",
      "0.46904880595906545\n",
      "0.2899333352647651\n",
      "0.42714358777556205\n",
      "0.47183838344028595\n",
      "0.4714977998991049\n",
      "0.44751180037288363\n",
      "0.4281450747001123\n",
      "0.47894855017611054\n",
      "0.45994429734278874\n",
      "0.4055921535415119\n",
      "0.46660770709281735\n",
      "0.17251153516345302\n",
      "0.242426195571011\n",
      "0.44613975539358364\n",
      "0.44556163375317254\n",
      "0.448962090984077\n",
      "0.448374220715291\n",
      "0.0\n",
      "0.4980392006680118\n",
      "0.40370093651703026\n",
      "0.4312539483835918\n",
      "0.4369279481527079\n",
      "0.26054846309194624\n",
      "0.3633554079553994\n",
      "0.4431693925241816\n",
      "0.4696063522844226\n",
      "0.4576609784685783\n",
      "0.4651575643061636\n",
      "0.44735821052956204\n",
      "0.4665316765009404\n",
      "0.46192397260582696\n",
      "0.4399182550853781\n",
      "0.38955281010099335\n",
      "0.4260662866757428\n",
      "0.4141632760484442\n",
      "0.4273513019312833\n",
      "0.48811976008936037\n",
      "0.4693463109500226\n",
      "0.4685571864266871\n",
      "0.48095946527771194\n",
      "0.4554169232854417\n",
      "0.47160984088064334\n",
      "0.4788080930890848\n",
      "0.46487954257337855\n",
      "0.45448905424013936\n",
      "0.47868679946916376\n",
      "0.0\n",
      "0.4777025288008725\n",
      "0.4372113556630621\n",
      "0.4751658909666815\n",
      "0.47745824358100286\n",
      "0.48309198219337113\n",
      "0.38780652014340367\n",
      "0.46624098825435634\n",
      "0.48401668965320094\n",
      "0.47790874750341106\n",
      "0.48398235644953935\n",
      "0.4850117951706688\n",
      "0.4595973891534055\n",
      "0.3749482474982087\n",
      "0.4798083380093815\n",
      "0.47437532383917225\n",
      "0.48192581343708424\n",
      "0.4432228757529437\n",
      "0.4556276120771727\n",
      "0.48431461023390954\n",
      "0.48555348121751835\n",
      "0.46861626770417025\n",
      "0.22587870080023428\n",
      "0.4787984756046368\n",
      "0.47786871029331757\n",
      "0.4980392006680118\n",
      "0.4862457422524662\n",
      "0.4821458163720487\n",
      "0.4829775649590936\n",
      "0.4703627379763996\n",
      "0.47071033565455217\n",
      "0.4372051608578383\n",
      "0.4708140468289713\n",
      "0.48098683871193454\n",
      "0.47939855327274133\n",
      "0.4850094641397585\n",
      "0.4820537285952008\n",
      "0.46208231516146153\n",
      "0.4726226193790196\n",
      "0.4751086479086027\n",
      "0.45259338500437896\n",
      "0.3797931098969726\n",
      "0.4878908967623372\n",
      "0.48674047820665506\n",
      "0.4738325109309981\n",
      "0.47660424673007745\n",
      "0.0\n",
      "0.2596831968800283\n",
      "0.33500407582710223\n",
      "0.36952784914641396\n",
      "0.41448064890653785\n",
      "0.31220318821798526\n",
      "0.42060616128607414\n",
      "0.0\n",
      "0.31478560412041945\n",
      "0.4262754778980438\n",
      "0.35783281117513566\n",
      "0.3695860574655154\n",
      "0.27390055580342504\n",
      "0.43901985370293206\n",
      "0.10666019049501943\n",
      "0.4980392006680118\n",
      "0.464059977298378\n",
      "0.09192248005439826\n",
      "0.24413723119998834\n",
      "0.47328570182513524\n",
      "0.09870061623094603\n",
      "0.32456050912716794\n",
      "0.3011768370018429\n",
      "0.012427168095816546\n",
      "0.03334210136924199\n",
      "0.09206500268560536\n",
      "0.43568471650436846\n",
      "0.4508759300489511\n",
      "0.0\n",
      "0.304215435612571\n",
      "0.0967574142622243\n",
      "0.3270867155248993\n",
      "0.34087833625724656\n",
      "0.4980392006680118\n",
      "0.4731020871728317\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4143890331726065\n",
      "0.0008351631664386054\n",
      "0.12273467216848412\n",
      "0.3531328989223294\n",
      "0.4264984906947551\n",
      "0.39149339800742544\n",
      "0.052530169928259624\n",
      "0.4566499588039821\n",
      "0.46947034409594535\n",
      "0.46139707322526513\n",
      "0.46959362047450914\n",
      "0.4645342733639702\n",
      "0.44333136639663134\n",
      "0.35863757426123005\n",
      "0.32681199829115815\n",
      "0.37926419283076424\n",
      "0.39143319026630297\n",
      "0.4980392006680118\n",
      "0.4596036105012791\n",
      "0.4695259555336087\n",
      "0.4340214148496351\n",
      "0.47767676250138147\n",
      "0.47474086361344714\n",
      "0.4612707946843523\n",
      "0.4739459682041708\n",
      "0.46721528791394534\n",
      "0.4534295261390835\n",
      "0.4780854882516057\n",
      "0.43464511170575143\n",
      "0.47397852935536566\n",
      "0.46241256673112574\n",
      "0.47278658844855415\n",
      "0.4698769541024059\n",
      "0.45151604136573337\n",
      "0.428214745988977\n",
      "0.4789224830546399\n",
      "0.48020509612676837\n",
      "0.43063700352680856\n",
      "0.3859430521200946\n",
      "0.2605112294218271\n",
      "0.47403328695076674\n",
      "0.47555218655223913\n",
      "0.4669937060386473\n",
      "0.4675932189973377\n",
      "0.46243119073921635\n",
      "0.4831036580633896\n",
      "0.4416682516684908\n",
      "0.4717525082180673\n",
      "0.4742247457200685\n",
      "0.47437751264088585\n",
      "0.3622967429504774\n",
      "0.4577175875994176\n",
      "0.47974050143466307\n",
      "0.47836913610100495\n",
      "0.46349056214112716\n",
      "0.4326069976426653\n",
      "0.4521428246979522\n",
      "0.4685103305748847\n",
      "0.44219857209328944\n",
      "0.3610858036788993\n",
      "0.4736823149076279\n",
      "0.060539305389985525\n",
      "0.46254456469800886\n",
      "0.4694824387058605\n",
      "0.4594878761501621\n",
      "0.46036478865732977\n",
      "0.46515775836906836\n",
      "0.473524767098034\n",
      "0.12525462328037407\n",
      "0.45533253383187877\n",
      "0.47034279825660036\n",
      "0.46684543867878237\n",
      "0.46899443491872195\n",
      "0.44675158300479456\n",
      "0.471059221525507\n",
      "0.450109571356202\n",
      "0.44037334180665405\n",
      "0.32885133377110454\n",
      "0.4630413675724455\n",
      "0.4238342306525132\n",
      "0.4596001423644623\n",
      "0.40391216476402253\n",
      "0.46025209546168955\n",
      "0.45289267590281196\n",
      "0.45455948656065304\n",
      "0.47942682854504065\n",
      "0.4528021314305379\n",
      "0.3650454022470059\n",
      "0.45043244367622837\n",
      "0.4589273444296774\n",
      "0.4641405761540027\n",
      "0.4596603742479917\n",
      "0.4980392006680118\n",
      "0.4307983468655805\n",
      "0.4765495893779404\n",
      "0.4065833467592898\n",
      "0.44154746468820916\n",
      "0.4582119132217527\n",
      "0.45420386843905874\n",
      "0.44320886856765407\n",
      "0.48167440550560864\n",
      "0.43051801012566954\n",
      "0.4292541737578377\n",
      "0.4980392006680118\n",
      "0.4626487311426942\n",
      "0.44382575472372326\n",
      "0.3045597109232039\n",
      "0.0\n",
      "0.3767527463114551\n",
      "0.4674868550063618\n",
      "0.4599515162171119\n",
      "0.4712831254527267\n",
      "0.45132633794065485\n",
      "0.17486358712358635\n",
      "0.4348803446549139\n",
      "0.44236999292565066\n",
      "0.4473985800124931\n",
      "0.15078866967369794\n",
      "0.4256135298413033\n",
      "0.0008351631664386054\n",
      "0.4980392006680118\n",
      "0.34555130331195866\n",
      "0.4746981657096888\n",
      "0.4395487192003038\n",
      "0.418947431375987\n",
      "0.2450926127196562\n",
      "0.38268576716565583\n",
      "0.42633969300437136\n",
      "0.45379838773557996\n",
      "0.365409133780546\n",
      "0.0005421821317323243\n",
      "0.418831556878322\n",
      "0.4651676922709462\n",
      "0.4765454561339982\n",
      "0.46403938874871004\n",
      "0.44768280000564337\n",
      "0.37319878926416816\n",
      "0.464692912006766\n",
      "0.4720382629378951\n",
      "0.41723778764364816\n",
      "0.3971606800142786\n",
      "0.022804368216235257\n",
      "0.0\n",
      "0.06506613530550612\n",
      "0.4677400413055374\n",
      "0.4391667063259821\n",
      "0.014833277869117258\n",
      "0.22552362850916088\n",
      "0.4392157204414324\n",
      "0.4516359431740998\n",
      "0.4128186200889562\n",
      "0.4409618982150388\n",
      "0.41329239034510007\n",
      "0.44499537950637963\n",
      "0.47598264856308864\n",
      "0.4406607674689202\n",
      "0.4655413308522344\n",
      "0.45839183532898153\n",
      "0.4646769657451914\n",
      "0.46771062250556483\n",
      "0.46397451295977593\n",
      "0.47823967683317564\n",
      "0.4595073517046214\n",
      "0.4748481532583889\n",
      "0.4704277659760848\n",
      "0.45081359486263445\n",
      "0.46626499267320376\n",
      "0.4225697400408671\n",
      "0.41612402183664104\n",
      "0.41550470731032113\n",
      "0.4507826002065662\n",
      "0.44148135313549824\n",
      "0.4532131151697787\n",
      "0.45961320949841467\n",
      "0.11084875073198759\n",
      "0.2822205852889915\n",
      "0.3511620096901122\n",
      "0.42499293860690895\n",
      "0.43463267748345175\n",
      "0.4317124860675742\n",
      "0.0\n",
      "0.23553947579140722\n",
      "0.0\n",
      "0.36786303263953607\n",
      "0.4162971308439297\n",
      "0.4528949540191771\n",
      "0.4616334508484899\n",
      "0.4714321847162796\n",
      "0.46698081154018495\n",
      "0.46818069312824184\n",
      "0.47020688468754357\n",
      "0.4791032440026597\n",
      "0.4721470105050364\n",
      "0.47622725060375887\n",
      "0.46731521722141206\n",
      "0.46103813133079946\n",
      "0.46091108880445814\n",
      "0.3288255579432147\n",
      "0.45113760894632526\n",
      "0.46298740520497633\n",
      "0.4722163445275175\n",
      "0.46392927011335794\n",
      "0.4602270552720144\n",
      "0.468488301868573\n",
      "0.47271221528402807\n",
      "0.47424555274173347\n",
      "0.4576518380244266\n",
      "0.4065619841612932\n",
      "0.46746591949795924\n",
      "0.46160740973455167\n",
      "0.4674791186389011\n",
      "0.4744248413974056\n",
      "0.48511572677312714\n",
      "0.45889648960375645\n",
      "0.4437147025945594\n",
      "0.4453253602245473\n",
      "0.45061992533112\n",
      "0.464266075713796\n",
      "0.43357653664934803\n",
      "0.4409184842392542\n",
      "0.43293820529843774\n",
      "0.445299866444476\n",
      "0.4753456820609451\n",
      "0.43486208428864825\n",
      "0.43253255166294613\n",
      "0.4637649749024251\n",
      "0.43768840008341936\n",
      "0.40762592104238077\n",
      "0.46415070063529756\n",
      "0.45886176413217716\n",
      "0.45953887597071535\n",
      "0.4551062600977062\n",
      "0.4509965112957997\n",
      "0.3063750779384097\n",
      "0.4638048046061182\n",
      "0.47552965456113483\n",
      "0.46338125072576886\n",
      "0.4133824191122241\n",
      "0.44712814572265736\n",
      "0.4396294377458572\n",
      "0.46485227998770895\n",
      "0.45714703277252855\n",
      "0.4670879875471486\n",
      "0.4694902052878277\n",
      "0.39833471545869525\n",
      "0.47815781507122257\n",
      "0.0\n",
      "0.4736299162162032\n",
      "0.47180243994825954\n",
      "0.462444079486349\n",
      "0.46846021759349205\n",
      "0.43557501556646183\n",
      "0.46943225259741617\n",
      "0.4601978008567496\n",
      "0.47142429826858057\n",
      "0.4552419786883633\n",
      "0.3807834397664686\n",
      "0.4754897271684544\n",
      "0.4715532993202693\n",
      "0.4980392006680118\n",
      "0.3321526111957619\n",
      "0.4581425737005958\n",
      "0.0\n",
      "0.4980392006680118\n",
      "0.4459068845420421\n",
      "0.4376352527473979\n",
      "0.4707517898277017\n",
      "0.47687037919617153\n",
      "0.4547645693232211\n",
      "0.020745081875431438\n",
      "0.009403480500515631\n",
      "0.38060325175969945\n",
      "0.06549521401837276\n",
      "0.4726800053679778\n",
      "0.38000056463199955\n",
      "0.4348556300010364\n",
      "0.4468605628795798\n",
      "0.0\n",
      "0.27808474012921086\n",
      "0.40134177898594775\n",
      "0.4629442447982682\n",
      "0.461284162764558\n",
      "0.4507405110870882\n",
      "0.41463300089463934\n",
      "0.44816904554017056\n",
      "0.18860703566209538\n",
      "0.4795267382272628\n",
      "0.393525402147403\n",
      "0.3762485723185772\n",
      "0.42076574357453245\n",
      "0.007878548459047683\n",
      "0.455084244459378\n",
      "0.4048870063075778\n",
      "0.4453772599658335\n",
      "0.45636179673295724\n",
      "0.46247741414260807\n",
      "0.4314227949197959\n",
      "0.330285051516652\n",
      "0.35591306635473324\n",
      "0.0\n",
      "0.41444889129720697\n",
      "0.4285955073082291\n",
      "0.42884419840256843\n",
      "0.4239172859993944\n",
      "0.39132713898638044\n",
      "0.2244701929447331\n",
      "0.43629677499023045\n",
      "0.0075272174433427645\n",
      "0.4068150238547739\n",
      "0.4033176897341114\n",
      "0.3817809495950541\n",
      "0.3196961581023134\n",
      "0.3758899921623246\n",
      "0.46614518761879825\n",
      "0.4165797615820361\n",
      "0.2855927495813749\n",
      "0.4098660052848056\n",
      "0.42361424845348444\n",
      "0.3781472567218128\n",
      "0.2687153213316198\n",
      "0.43031586555482326\n",
      "0.32685649312484655\n",
      "0.43577910418326044\n",
      "0.4980392006680118\n",
      "0.39078409494629357\n",
      "0.2950348852683483\n",
      "0.29859366637634255\n",
      "0.3190816446441158\n",
      "0.437038734488677\n",
      "0.3544960720289594\n",
      "0.2125092935381629\n",
      "0.35660372097901116\n",
      "0.2789047434310546\n",
      "0.3229200747017696\n",
      "0.35479650994198636\n",
      "0.4279354927633909\n",
      "0.2704870097816504\n",
      "0.0\n",
      "0.4288800107758333\n",
      "0.27931689483128247\n",
      "0.43635461525543917\n",
      "0.28853998283603666\n",
      "0.42567590497528535\n",
      "0.420627532557736\n",
      "0.41677867418742454\n",
      "0.4236974847705423\n",
      "0.40581142066337844\n",
      "0.35777733870435396\n",
      "0.4485682634700581\n",
      "0.4109914706099856\n",
      "0.34967873884850775\n",
      "0.4536001820417712\n",
      "0.37085256867067073\n",
      "0.403579426951767\n",
      "0.0\n",
      "0.034970723921626465\n",
      "0.4699118091088689\n",
      "0.2762578133336401\n",
      "0.36346286785535065\n",
      "0.4150927799105357\n",
      "0.34779011536359544\n",
      "0.45238017119471813\n",
      "0.04839669396206919\n",
      "0.41998217538267746\n",
      "0.4420093669067786\n",
      "0.3340390814801928\n",
      "0.3387338923825551\n",
      "0.42729718729752253\n",
      "0.2715298568593534\n",
      "0.4574114414045808\n",
      "0.41221614256554684\n",
      "0.4409301433753129\n",
      "0.47970827354282525\n",
      "0.45176162331365216\n",
      "0.4590453357136138\n",
      "0.4533204192787314\n",
      "0.46054355445425926\n",
      "0.10639312370922625\n",
      "0.0\n",
      "0.44946418010451294\n",
      "0.0\n",
      "0.11074574372055727\n",
      "0.27575367404744877\n",
      "0.03027565526779376\n",
      "0.38513208004059174\n",
      "0.3454692387904005\n",
      "0.46931854447359134\n",
      "0.3553685922632589\n",
      "0.42390525269498736\n",
      "0.008386154836907394\n",
      "0.04427874810820363\n",
      "0.4526607025716164\n",
      "0.44392824534140335\n",
      "0.33378744935352267\n",
      "0.1710908183917303\n",
      "0.46985456453061547\n",
      "0.3196811011960193\n",
      "0.34988225523055017\n",
      "0.4726965050449431\n",
      "0.45448805946737136\n",
      "0.4475111582532659\n",
      "0.32925320755471693\n",
      "0.15272775212344458\n",
      "0.44831452802104277\n",
      "0.39528875371452105\n",
      "0.4469109772344601\n",
      "0.4526088296669083\n",
      "0.37778749789006366\n",
      "0.35304731020244423\n",
      "0.46730796356402554\n",
      "0.35564681316669894\n",
      "0.2224376849719112\n",
      "0.4063638757976187\n",
      "0.4345447897588174\n",
      "0.2983724481641038\n",
      "0.0015860336734045083\n",
      "0.0\n",
      "0.434650716553215\n",
      "0.30411889935417724\n",
      "0.30125286851330124\n",
      "0.01528099893337878\n",
      "0.10585671361681044\n",
      "0.08353405026046061\n",
      "0.1693451542306325\n",
      "0.4302896891880115\n",
      "0.24880653401791886\n",
      "0.30971905539797373\n",
      "0.46055141424122975\n",
      "0.48365698866371476\n",
      "0.16155727933525993\n",
      "0.4110610966586639\n",
      "0.3051959974030693\n",
      "0.4643981572599841\n",
      "0.4550805569523092\n",
      "0.1631344856977849\n",
      "0.41488205403027434\n",
      "0.20690227446156045\n",
      "0.129178327976911\n",
      "0.23923291586899306\n",
      "0.32927044951313084\n",
      "0.3699065219336946\n",
      "0.226163716153237\n",
      "0.43069736280983106\n",
      "0.43623914860723734\n",
      "0.4576457543076772\n",
      "0.4181989459058478\n",
      "0.44472019116234\n",
      "0.4684821725463701\n",
      "0.44149047187831303\n",
      "0.2810725853868922\n",
      "0.3388387502352949\n",
      "0.2569382075530691\n",
      "0.2942011954903697\n",
      "0.19520078623245093\n",
      "0.4400207520554327\n",
      "0.468108982179322\n",
      "0.2928657971078445\n",
      "0.39354351951227245\n",
      "0.4469479940817253\n",
      "0.340350792615025\n",
      "0.31719178773673395\n",
      "0.41225017579381906\n",
      "0.4623536265906529\n",
      "0.48110314547393934\n",
      "0.4697804437152388\n",
      "0.3430314165296781\n",
      "0.33273270089886126\n",
      "0.43441505523038\n",
      "0.3925770608757413\n",
      "0.46564883862286205\n",
      "0.425300855534061\n",
      "0.4015410898339309\n",
      "0.0\n",
      "0.09526251524176103\n",
      "0.42514801374976585\n",
      "0.36351082156559217\n",
      "0.14428917794227333\n",
      "0.4457431442950174\n",
      "0.45873947244736335\n",
      "0.43479837398838633\n",
      "0.3630668329505776\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.18120543077214654\n",
      "0.08907946589823063\n",
      "0.41417727683614325\n",
      "0.42551513447381095\n",
      "0.47423106398924003\n",
      "0.45441486790231206\n",
      "0.420818793545669\n",
      "0.3953785303123499\n",
      "0.1254138780770906\n",
      "0.0\n",
      "0.4980392006680118\n",
      "0.4527678705202155\n",
      "0.0\n",
      "0.40959277852174714\n",
      "0.36039620218573093\n",
      "0.34602805280783455\n",
      "0.3322955981271633\n",
      "0.0\n",
      "0.3698178758652094\n",
      "0.45272651545335013\n",
      "0.4169180104960685\n",
      "0.426496349035594\n",
      "0.4589623171048783\n",
      "0.45665613809346994\n",
      "0.37668488998235894\n",
      "0.0\n",
      "0.39255768530809754\n",
      "0.0\n",
      "0.09297203144605373\n",
      "0.021863088978272764\n",
      "0.44809158455838766\n",
      "0.4381407060950873\n",
      "0.4252044431872946\n",
      "0.37597039359602363\n",
      "0.24720086102280206\n",
      "0.4348674378555549\n",
      "0.4489962950432957\n",
      "0.4152366055997015\n",
      "0.44695208900399985\n",
      "0.4786566202115467\n",
      "0.4063712730093132\n",
      "0.3447074063132929\n",
      "0.46942563201289944\n",
      "0.46264719293458856\n",
      "0.44536907030936557\n",
      "0.47264788609023556\n",
      "0.44685239779492997\n",
      "0.36051386596793467\n",
      "0.46879726125776683\n",
      "0.4380398126757925\n",
      "0.4646658219134171\n",
      "0.4097298432761447\n",
      "0.42043997433848973\n",
      "0.46164167057021\n",
      "0.4266714848719529\n",
      "0.4574482509463347\n",
      "0.37651011807239443\n",
      "0.4342797209382561\n",
      "0.30516681777065874\n",
      "0.43699400299539226\n",
      "0.0\n",
      "0.4735210479504708\n",
      "0.2875049622040435\n",
      "0.44258578125115156\n",
      "0.46544991386050605\n",
      "0.47340624278864224\n",
      "0.43282753190650347\n",
      "0.44466090793454394\n",
      "0.4371731659239396\n",
      "0.4680822598911513\n",
      "0.45531626315701484\n",
      "0.4668469153353336\n",
      "0.4598301022722156\n",
      "0.44564285589927516\n",
      "0.26093251013746344\n",
      "0.46828909337944047\n",
      "0.4508222251481771\n",
      "0.43982134489988506\n",
      "0.4337469653543043\n",
      "0.44768986779563635\n",
      "0.44961314200351066\n",
      "0.4537183087741874\n",
      "0.4817717664068751\n",
      "0.4635236563920306\n",
      "0.4121917433318621\n",
      "0.4248140946306112\n",
      "0.43092664177111806\n",
      "0.38506363340311783\n",
      "0.38530643775187506\n",
      "0.008235541153021006\n",
      "0.290914745847877\n",
      "0.47230722591363766\n",
      "0.4741378111586384\n",
      "0.09777072359525146\n",
      "0.38953363754673526\n",
      "0.189805989812661\n",
      "0.4637370770008143\n",
      "0.3390851788743305\n",
      "0.20423426618259338\n",
      "0.45945317667604185\n",
      "0.46379951621655835\n",
      "0.4688742493266802\n",
      "0.475167126688305\n",
      "0.47609218329115005\n",
      "0.4980392006680118\n",
      "0.3036797429625633\n",
      "0.2575546244543801\n",
      "0.46194548453855566\n",
      "0.4000549808452202\n",
      "0.33705821227623556\n",
      "0.32700898684215357\n",
      "0.4118447835868342\n",
      "0.43361186002152\n",
      "0.46804751118252635\n",
      "0.43915707894188905\n",
      "0.453304420436482\n",
      "0.4566429094401754\n",
      "0.4481626549658067\n",
      "0.4083978101245535\n",
      "0.4586483625165258\n",
      "0.41173056053864393\n",
      "0.4053324060549919\n",
      "0.4455609561194796\n",
      "0.45271162899201806\n",
      "0.4265066260277858\n",
      "0.338947192446879\n",
      "0.40036781464132787\n",
      "0.4367254230814484\n",
      "0.46161135464035813\n",
      "0.4289742151992156\n",
      "0.38520791783960023\n",
      "0.4980392006680118\n",
      "0.4587270072119206\n",
      "0.4703545999931686\n",
      "0.46539710164778375\n",
      "0.4686637343345021\n",
      "0.465832235891881\n",
      "0.0420951577767061\n",
      "0.3355247360870819\n",
      "0.474374274124242\n",
      "0.027394801600048847\n",
      "0.4385409257739512\n",
      "0.46924648443525707\n",
      "0.36985360794598676\n",
      "0.46034623755653375\n",
      "0.47843212376118754\n",
      "0.47954427463824534\n",
      "0.4614759752280708\n",
      "0.4665642644651856\n",
      "0.4781579270653662\n",
      "0.4498714858449675\n",
      "0.47814121740658194\n",
      "0.39369959980693336\n",
      "0.4199974509787938\n",
      "0.45509921763512806\n",
      "0.4445717476595722\n",
      "0.4502076896445535\n",
      "0.46241670269889745\n",
      "0.4488366179493476\n",
      "0.45244666977666814\n",
      "0.44189230140731983\n",
      "0.4238202011164757\n",
      "0.24279022420236046\n",
      "0.2326123557833434\n",
      "0.2928497364531219\n",
      "0.36793934790591476\n",
      "0.35690604066396553\n",
      "0.3507090098959959\n",
      "0.38959106196748167\n",
      "0.38038684797639905\n",
      "0.44698993227512523\n",
      "0.39767928519036866\n",
      "0.3652223616503576\n",
      "0.34893714858745145\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.3793666349610601\n",
      "0.36092338885852127\n",
      "0.3834576820556822\n",
      "0.4630521434419655\n",
      "0.46654202618766694\n",
      "0.4526801360410029\n",
      "0.470946674138845\n",
      "0.4604303053860832\n",
      "0.4640077701510598\n",
      "0.4980392006680118\n",
      "0.443797983092407\n",
      "0.43447744020509105\n",
      "0.41981554550965056\n",
      "0.4625658138111487\n",
      "0.43341964768265173\n",
      "0.3918742171124123\n",
      "0.46594107402376866\n",
      "0.46860075092560166\n",
      "0.46550133003024136\n",
      "0.4601683524936182\n",
      "0.1305148733453023\n",
      "0.47093389671746183\n",
      "0.4537907374239606\n",
      "0.4477355643682818\n",
      "0.4705095690512629\n",
      "0.0\n",
      "0.46816708009611174\n",
      "0.43209324146868255\n",
      "0.44346678671186873\n",
      "0.47376061413180987\n",
      "0.39096299717314753\n",
      "0.0\n",
      "0.42630779547330466\n",
      "0.4156784590246229\n",
      "0.4384634263192932\n",
      "0.44135448278961537\n",
      "0.4469031679859495\n",
      "0.446022009000539\n",
      "0.4538434217226916\n",
      "0.45333619929596614\n",
      "0.44576730322777003\n",
      "0.23587437383960552\n",
      "0.44013826830309843\n",
      "0.4559910997420063\n",
      "0.47383977690445345\n",
      "0.45742970697691065\n",
      "0.0\n",
      "0.4651058199605464\n",
      "0.4258561857651405\n",
      "0.4745531246044351\n",
      "0.4980392006680118\n",
      "0.4493984240756436\n",
      "0.43492008291101886\n",
      "0.4431808950106902\n",
      "0.4601706254499484\n",
      "0.4449659954560663\n",
      "0.4405692854633476\n",
      "0.47903350515415966\n",
      "0.0\n",
      "0.44917675319851685\n",
      "0.4633055919457737\n",
      "0.4750630015263459\n",
      "0.401946393571678\n",
      "0.40952474705885794\n",
      "0.4396437473486174\n",
      "0.44237970276148425\n",
      "0.43687467809967506\n",
      "0.4831023327435107\n",
      "0.31758059388190496\n",
      "0.3112270959649658\n",
      "0.25527392407360244\n",
      "0.401266667762701\n",
      "0.4588008528575676\n",
      "0.36578460300895904\n",
      "0.4518792109057238\n",
      "0.0\n",
      "0.44981367915583\n",
      "0.0\n",
      "0.45096269236091996\n",
      "0.44921035815120103\n",
      "0.4468238981136322\n",
      "0.4328206188365716\n",
      "0.3978559057178077\n",
      "0.0\n",
      "0.4308068820713093\n",
      "0.4558628657408575\n",
      "0.44152306224909926\n",
      "0.0022236391500233214\n",
      "0.4266048743565718\n",
      "0.4604919955326302\n",
      "0.0\n",
      "0.43644043016956613\n",
      "0.4385520855393561\n",
      "0.46029812650054663\n",
      "0.4742001638660958\n",
      "0.3580638780622149\n",
      "0.14509509193502243\n",
      "0.4401782523446373\n",
      "0.4473015493536471\n",
      "0.4126160110960045\n",
      "0.4668302893010448\n",
      "0.4849194312805672\n",
      "0.0007301441270572695\n",
      "0.4980392006680118\n",
      "0.46448537145442537\n",
      "0.4735517821566531\n",
      "0.4739206588756148\n",
      "0.004290383179894007\n",
      "0.0\n",
      "0.4708394872109185\n",
      "0.4750148399362699\n",
      "0.4776956984999458\n",
      "0.4678909529929426\n",
      "0.4841758475649782\n",
      "0.48234226525076845\n",
      "0.4804216938538472\n",
      "0.47389146266938814\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.0026234003683870133\n",
      "0.3506258428165065\n",
      "0.42757729645281706\n",
      "0.48185496928369687\n",
      "0.4688950226816038\n",
      "0.47268080249709854\n",
      "0.47967582900820577\n",
      "0.46666029977821605\n",
      "0.1747211471559582\n",
      "0.4755474321944404\n",
      "0.4766536091106083\n",
      "0.31829562976763\n",
      "0.46743898328621647\n",
      "0.45103367369414427\n",
      "0.45212533340651423\n",
      "0.4407818084630024\n",
      "0.46763968325186683\n",
      "0.4765589868340985\n",
      "0.46854646167710656\n",
      "0.45089227374425445\n",
      "0.4592860406718793\n",
      "0.45722253392474893\n",
      "0.4511863280534556\n",
      "0.3942019833428009\n",
      "0.4258325115931523\n",
      "0.3841520583537738\n",
      "0.4795242053831401\n",
      "0.4374067686350721\n",
      "0.44878668152152945\n",
      "0.4527886094932338\n",
      "0.46060326011772007\n",
      "0.4509811305049979\n",
      "0.4114344959122889\n",
      "0.43761116841402525\n",
      "0.0\n",
      "0.4201118475244363\n",
      "0.4531397096366921\n",
      "0.45127812806478057\n",
      "0.0\n",
      "0.4503411994463133\n",
      "0.39056036955978995\n",
      "0.4579108089925017\n",
      "0.43390009767956617\n",
      "0.4334352837542615\n",
      "0.459311520478376\n",
      "0.23004861995384812\n",
      "0.4661441429572733\n",
      "0.4657068995196123\n",
      "0.47382356043083873\n",
      "0.46375386613165287\n",
      "0.4435462533108696\n",
      "0.46163781864537684\n",
      "0.01328004862246298\n",
      "0.46699014816675677\n",
      "0.4601064691208249\n",
      "0.408202417939403\n",
      "0.35462253124694487\n",
      "0.3732803742113302\n",
      "0.45636586781280525\n",
      "0.42315402341038166\n",
      "0.4327554012490203\n",
      "0.43231530022255404\n",
      "0.40517223821656245\n",
      "0.026505095329403285\n",
      "0.4813529394881032\n",
      "0.42270070911417695\n",
      "0.4730634804775676\n",
      "0.0\n",
      "0.454619531922991\n",
      "0.4500816891493333\n",
      "0.46614605566870176\n",
      "0.4288192833722879\n",
      "0.44891716009052707\n",
      "0.4396451163832913\n",
      "1.1105375691288812e-06\n",
      "0.002713684852893044\n",
      "0.4652715323277708\n",
      "0.48786387977181483\n",
      "0.4040579878245242\n",
      "0.4638692279310563\n",
      "0.47639872020150775\n",
      "0.4811406668458947\n",
      "0.2499857696333051\n",
      "0.4638129942577516\n",
      "0.4148712744077673\n",
      "0.0\n",
      "0.47874291872805425\n",
      "0.4693861903281037\n",
      "0.48205717350855376\n",
      "0.4763042818734359\n",
      "0.33366722184364533\n",
      "0.4539869744972258\n",
      "0.48769720901144487\n",
      "0.4675694799865222\n",
      "0.4980392006680118\n",
      "0.164705897254101\n",
      "0.0\n",
      "0.1105999425674809\n",
      "0.36112634978817404\n",
      "0.34114166855529704\n",
      "0.4334936656386821\n",
      "0.45642177399032324\n",
      "0.17790911472490673\n",
      "0.39246225648238514\n",
      "0.44466546803240137\n",
      "0.4980392006680118\n",
      "0.46514379760919083\n",
      "0.3787087365597494\n",
      "0.40554582804919964\n",
      "0.3730548432674672\n",
      "0.37219898744583085\n",
      "0.38921705202471457\n",
      "0.448133257869021\n",
      "0.3931175879624522\n",
      "0.32235297866884255\n",
      "0.3929762285204643\n",
      "0.4167995610106165\n",
      "0.42932597190601346\n",
      "0.43015748265101955\n",
      "0.4571267842379467\n",
      "0.44283683692757453\n",
      "0.4753358808843104\n",
      "0.45760009717772165\n",
      "0.45769821386014264\n",
      "0.45000046783220593\n",
      "0.4583084263858914\n",
      "0.12567551243779174\n",
      "0.4980392006680118\n",
      "0.37658825850354505\n",
      "0.01977866026666925\n",
      "0.0\n",
      "0.08056382687535801\n",
      "0.0\n",
      "0.3520037630162204\n",
      "0.4313464745052494\n",
      "0.006748844364492679\n",
      "0.4980392006680118\n",
      "0.00010892824699118694\n",
      "0.4055955831390434\n",
      "0.0\n",
      "0.3923405141772256\n",
      "0.4089503061508575\n",
      "0.28867544754203656\n",
      "0.4980392006680118\n",
      "0.0\n",
      "0.4285937706331365\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.41542785427702067\n",
      "0.4309271740871085\n",
      "0.46690517044972984\n",
      "0.47158782717250003\n",
      "0.4575093585213882\n",
      "0.0\n",
      "0.15113718610204632\n",
      "0.24164561749031427\n",
      "0.24512041191649814\n",
      "0.4980392006680118\n",
      "0.36152707443084836\n",
      "0.0\n",
      "0.41245524616995305\n",
      "0.39056588125971514\n",
      "0.4980392006680118\n",
      "0.4980392006680118\n",
      "0.4367440881892361\n",
      "0.005615931092609721\n",
      "0.0\n",
      "0.35310042571433803\n",
      "0.4980392006680118\n",
      "0.4220100791049865\n",
      "0.46512244446258566\n",
      "0.48091639774208567\n",
      "0.4315023646672815\n",
      "0.4980392006680118\n",
      "0.4021982684910239\n",
      "0.4301592709107268\n",
      "0.36654992031737693\n",
      "0.44201522258743997\n",
      "---: 0\n"
     ]
    }
   ],
   "source": [
    "row_count = 0\n",
    "z_count = 0\n",
    "for row in scorematrix:\n",
    "    if np.max(row) == 0.5:\n",
    "        z_count += 1\n",
    "        print(np.max(row))\n",
    "    row_count += 1\n",
    "print('---:', z_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_loc = np.zeros(n)\n",
    "loc = 0\n",
    "for eachrow in merged_score:\n",
    "    max_loc[loc] = np.argmax(eachrow)\n",
    "    loc += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcb0d1199d0>]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.sort(max_loc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6541286785479757\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(merged_score[1212], 'o')\n",
    "print(merged_score[168][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1.  1. ...  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "for i in decision:\n",
    "    if i==-1:\n",
    "        new_data_i = os.path.join(working_parent_folder, str(i), PARAM_SUB_FOLDER_POLAR)\n",
    "        temp_folder_path = os.path.join(working_parent_folder,'temp')\n",
    "        os.mkdir(temp_folder_path)\n",
    "        os.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
