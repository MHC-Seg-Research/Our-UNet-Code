{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Approach for Unet Training \n",
    "\n",
    "------\n",
    "\n",
    "## Overview\n",
    "\n",
    "\n",
    "\n",
    "The purpose of this learn.ipynb notebook is to investigate whether an image can exhibit a preference for being segmented more effectively using a UNet model trained on polar or cartesian-dominant images.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Structure\n",
    "```\n",
    "data\n",
    "└── endoscopic\n",
    "    ├── cartesian\n",
    "    │   ├── image\n",
    "    │   └── label\n",
    "    └── polar\n",
    "        ├── image\n",
    "        └── label\n",
    "```\n",
    "\n",
    "Inside of each end folder there are 956 images, named as `0.tif` to `955.tif`\n",
    "and I believe, for now, the naming of the images are one to one correctly matched, meaning the ``/data/endoscopic/**cartesian**/image/0.tif`` is transformed from `/data/endoscopic/**polar**/image/0.tif`\n",
    "\n",
    "Instead of putting a seperate set of images aside to be test set, we chose to use k-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from defines import *\n",
    "from model import *\n",
    "from data import *\n",
    "import sys\n",
    "\n",
    "from numpy import loadtxt\n",
    "from keras.models import load_model\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from skimage.io import imread\n",
    "from skimage import img_as_ubyte\n",
    "from skimage.transform import resize\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import math\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from files and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this if your computer has a cuda visible device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PARAM_SYSTEM_TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test code for correct file structure setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize folder tree in current directory\n",
    "os.system(\"tree -d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count of files in data directories\n",
    "os.system(\"ls \" + os.path.join(PARAM_PATH_CARTE,PARAM_IMG_FOLDER) + \" | wc -l\")\n",
    "os.system(\"ls \" + os.path.join(PARAM_PATH_CARTE,PARAM_MSK_FOLDER) + \" | wc -l\")\n",
    "os.system(\"ls \" + os.path.join(PARAM_PATH_POLAR,PARAM_IMG_FOLDER) + \" | wc -l\")\n",
    "os.system(\"ls \" + os.path.join(PARAM_PATH_POLAR,PARAM_MSK_FOLDER) + \" | wc -l\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Expected Output:\n",
    "7404\\\n",
    "7404\\\n",
    "7404\\\n",
    "7404\\\n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Relocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following block, the code loads in one analysis file from previous research. \n",
    "\n",
    "### #File name postfix\n",
    "_C_ is the dice scores of the predictions generated by Unet C: this Unet C is trained using all 7404 images, in their cartesian form. The raw image was directly input into the Unet and the prediction was generated.\n",
    "\n",
    "_P_ is the dice scores of the predictions generated by Unet P: this Unet P is trained using all 7404 images but in their polar form. The raw images were transformed, and then input for prediction. The prediction is in polar space.\n",
    "\n",
    "_P2C_ is the dice scores of the predictions generated by the same Unet P as mentioned above, but the dice score is generated by transforming the prediction back to cartesian, and compared to their original label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name = 'analysis_dice_back_Test_C.npy'\n",
    "#file_name = 'analysis_dice_back_Test_P.npy'\n",
    "#file_name = 'analysis_dice_back_Test_P2C.npy'\n",
    "file_name = 'analysis_dice_back_Train_P.npy'\n",
    "\n",
    "np_file = os.path.join(PARAM_PATH_SCORES, file_name)\n",
    "#load npy file\n",
    "img_score = np.load(np_file)\n",
    "\n",
    "#sort scores in descending order and store index\n",
    "sorted_score = np.flip(np.argsort(img_score))\n",
    "\n",
    "#------DEBUG--------\n",
    "#print(len(sorted_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sorted_score` should be a list of length 7404, sorted by the method we chose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_score = pd.DataFrame(sorted_score)\n",
    "\n",
    "#fetch top polar dominant and non-polar dominant image\n",
    "num_polar = round(len(sorted_score)/2)\n",
    "num_cartesian = len(sorted_score) - num_polar\n",
    "dfPolar = sorted_score.head(num_polar)\n",
    "dfCartesian = sorted_score.tail(num_cartesian)\n",
    "#print(\"Polar: \\n\", dfPolar)\n",
    "#print(\"Cartesian: \\n\", dfCartesian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in `dfPolar` should be the best half of images (filename), which performs better than the other half, according to the data we used above. In `dfCartesian` there's the other half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from filePrep import *\n",
    "\n",
    "K = 5\n",
    "\n",
    "checkNcreateTempFolder(PARAM_PATH_TEMP_POLAR, K)\n",
    "checkNcreateTempFolder(PARAM_PATH_TEMP_CARTE, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two lines creates the new temporary folder.\n",
    "Instead of making kfolds later, the kfolds is assigned now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits = K, shuffle = True, random_state = 42) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the KFold package to assign the paramters like n_splits and so.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for train_index,test_index in kf.split(dfPolar):\n",
    "    fillFolder(test_index, dfPolar, PARAM_PATH_POLAR, PARAM_PATH_CARTE, PARAM_PATH_TEMP_POLAR, i)\n",
    "    i += 1\n",
    "i = 0\n",
    "print('------------------------------------')\n",
    "for train_index,test_index in kf.split(dfCartesian):\n",
    "    fillFolder(test_index, dfCartesian, PARAM_PATH_POLAR, PARAM_PATH_CARTE, PARAM_PATH_TEMP_CARTE, i)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we should have all `k-folds` set up. The next step is to write a training loop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the limitation of flow_from_directory, which, it does not allow the combination of multiple directories. And, the limitation of my knowledge of flow_from_dataframe. **I believe this is solvable using flow_from_dataframe** I decide to move training set into a separate temporary folder when training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "PARAM_BETA_TEST_NUM = 6\n",
    "data_gen_args = dict(rotation_range = 50,      # TODO: improve the data augmentation\n",
    "                width_shift_range =0.2,\n",
    "                height_shift_range =0.2,\n",
    "                shear_range = 0.35,\n",
    "                zoom_range = 0.05,\n",
    "                horizontal_flip = True,\n",
    "                fill_mode = 'nearest',\n",
    "                rescale = 1./255)\n",
    "#Train polar models\n",
    "working_parent_folder = PARAM_PATH_TEMP_POLAR\n",
    "Polar_history = []\n",
    "for i in range(K):\n",
    "    working_test_folder_i = os.path.join(working_parent_folder, str(i), PARAM_SUB_FOLDER_POLAR)\n",
    "    temp_folder_path = os.path.join(working_parent_folder,'temp')\n",
    "    os.mkdir(temp_folder_path)\n",
    "    for j in range(K):\n",
    "        if i != j:\n",
    "            for subfolder_name in ['image','label']:\n",
    "                subfolder_path = os.path.join(working_parent_folder,str(j),'polar',subfolder_name)\n",
    "                temp_subfolder_path = os.path.join(temp_folder_path,subfolder_name)\n",
    "                for root, dirs, files in os.walk(subfolder_path):\n",
    "                    for file in files:\n",
    "                        src_file = os.path.join(root, file)\n",
    "                        dest_file = os.path.join(temp_subfolder_path,os.path.relpath(src_file, subfolder_path))\n",
    "                        os.makedirs(os.path.dirname(dest_file), exist_ok=True)\n",
    "                        shutil.copy(src_file, dest_file)\n",
    "    test_gene = trainGenerator(batch_size, temp_folder_path, PARAM_IMG_FOLDER, PARAM_MSK_FOLDER, data_gen_args)\n",
    "    model = unet(PARAM_BETA1[PARAM_BETA_TEST_NUM], PARAM_BETA2[PARAM_BETA_TEST_NUM]) \n",
    "    model_checkpoint = ModelCheckpoint(os.path.join(working_parent_folder,str(i),'checkpoint.hdf5'), monitor = 'loss', verbose=1, save_best_only=True)\n",
    "    test_run = model.fit(test_gene, verbose = 1, steps_per_epoch = 100, epochs = 100, callbacks = [model_checkpoint])\n",
    "    Polar_history.append(test_run)\n",
    "    shutil.rmtree(temp_folder_path)\n",
    "    \n",
    "#Train cartesian models\n",
    "data_gen_args = dict(rotation_range = 80,      # TODO: improve the data augmentation\n",
    "                width_shift_range =0.02,\n",
    "                height_shift_range =0.02,\n",
    "                shear_range = 0.35,\n",
    "                zoom_range = 0.075,\n",
    "                horizontal_flip = True,\n",
    "                fill_mode = 'nearest',\n",
    "                rescale = 1./255)\n",
    "working_parent_folder = PARAM_PATH_TEMP_CARTE\n",
    "Cartesian_history = []\n",
    "for i in range(K):\n",
    "    working_test_folder_i = os.path.join(working_parent_folder, str(i), PARAM_SUB_FOLDER_CARTE)\n",
    "    temp_folder_path = os.path.join(working_parent_folder,'temp')\n",
    "    os.mkdir(temp_folder_path)\n",
    "    for j in range(K):\n",
    "        if i != j:\n",
    "            for subfolder_name in ['image','label']:\n",
    "                subfolder_path = os.path.join(working_parent_folder,str(j),'carte',subfolder_name)\n",
    "                temp_subfolder_path = os.path.join(temp_folder_path,subfolder_name)\n",
    "                for root, dirs, files in os.walk(subfolder_path):\n",
    "                    for file in files:\n",
    "                        src_file = os.path.join(root, file)\n",
    "                        dest_file = os.path.join(temp_subfolder_path,os.path.relpath(src_file, subfolder_path))\n",
    "                        os.makedirs(os.path.dirname(dest_file), exist_ok=True)\n",
    "                        shutil.copy(src_file, dest_file)\n",
    "    test_gene = trainGenerator(batch_size, temp_folder_path, PARAM_IMG_FOLDER, PARAM_MSK_FOLDER, data_gen_args)\n",
    "    model = unet(PARAM_BETA1[PARAM_BETA_TEST_NUM], PARAM_BETA2[PARAM_BETA_TEST_NUM]) \n",
    "    model_checkpoint = ModelCheckpoint(os.path.join(working_parent_folder,str(i),'checkpoint.hdf5'), monitor = 'loss', verbose=1, save_best_only=True)\n",
    "    test_run = model.fit(test_gene, verbose = 1, steps_per_epoch = 100, epochs = 100, callbacks = [model_checkpoint])\n",
    "    Cartesian_history.append(test_run)\n",
    "    shutil.rmtree(temp_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last block there trained all 10 models, the 10 models' checkpoints are saved in the temp folder of each test model. For example, if stored in folder `temp/polar_Dom/0/`, the model is assumed to be trained on all folders except the current folder -- namingly, `folder 1,2,3,4`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for single_run in Polar_history:\n",
    "    plt.plot(single_run.history['loss'])\n",
    "    plt.plot(single_run.history['accuracy'])\n",
    "    plt.title('Polar Run')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['loss', 'accuracy'], loc='upper left')\n",
    "    plt.show()\n",
    "print('________________________________________________')\n",
    "for single_run in Cartesian_history:\n",
    "    plt.plot(single_run.history['loss'])\n",
    "    plt.plot(single_run.history['accuracy'])\n",
    "    plt.title('Cartesian Run')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['loss', 'accuracy'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, all of the ten models from each group are trained, the next step is to load each of them and predict using every other images that are not in the training set as inputs. Note that `polar dominant models` should only take `polar` images as inputs and `cartesian dominant models` should only take `cartesian` images as inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block is supposed to get and check the number of images and masks in the source folder. *could be moved to previous blocks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WE have a matching number of images and labels here, count =  7404\n"
     ]
    }
   ],
   "source": [
    "#Predict using polar dominant images\n",
    "image_extension = 'tif'\n",
    "image_count = 0\n",
    "img_pattern = os.path.join(PARAM_PATH_POLAR, PARAM_IMG_FOLDER, f'*.{image_extension}')\n",
    "image_files = glob.glob(img_pattern)\n",
    "msk_pattern = os.path.join(PARAM_PATH_POLAR, PARAM_MSK_FOLDER, f'*.{image_extension}')\n",
    "msk_files = glob.glob(msk_pattern)\n",
    "#length matching check\n",
    "if len(image_files) == len(msk_files):\n",
    "    print('WE have a matching number of images and labels here, count = ', len(msk_files))\n",
    "else:\n",
    "    print('Something is wrong with the original data set. [Unmatching number of images/masks.]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5 #if we don't want to train again, run this\n",
    "PARAM_BETA_TEST_NUM = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a numpy array to showcase the distribution of files\n",
    "\n",
    "n = len(image_files)\n",
    "m = K * 2\n",
    "\n",
    "filematrix = np.zeros((n,m))\n",
    "for img_type in ['polar', 'carte']:\n",
    "    #\n",
    "    img_extenstion = 'tif'\n",
    "    #\n",
    "    for_counter = 0\n",
    "    if img_type == 'polar':\n",
    "        working_parent_folder = PARAM_PATH_TEMP_POLAR\n",
    "    else:\n",
    "        working_parent_folder = PARAM_PATH_TEMP_CARTE\n",
    "        for_counter += 1\n",
    "    for i in range(K):\n",
    "        image_path = os.path.join(working_parent_folder, str(i), img_type, PARAM_IMG_FOLDER)\n",
    "        img_pattern = os.path.join(image_path, f'*.{image_extension}')\n",
    "        image_files = glob.glob(img_pattern)\n",
    "        for file_name in image_files:\n",
    "            file_name_shorten = os.path.basename(file_name)\n",
    "            file_name_raw, ext = os.path.splitext(file_name_shorten)\n",
    "            filematrix[int(file_name_raw),i + for_counter * K] = 1\n",
    "        number_of_ones = np.count_nonzero(filematrix == 1)\n",
    "        #print(number_of_ones)  #uncomment this line when png file is not satisfactory, we can track the number of ones during each step  \n",
    "plt.imsave('filematrix.png', filematrix, cmap = 'binary')\n",
    "row_indices, col_indices = np.where(filematrix == 0)\n",
    "indices = list(zip(row_indices, col_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we will have a .png file saved to the root directory, zoom in and we should be able to see only one pixel is colored black in each row.\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(image1, image2):\n",
    "    # Ensure the input images have the same shape\n",
    "    if image1.shape != image2.shape:\n",
    "        raise ValueError(\"Input images must have the same shape.\")\n",
    "\n",
    "    # Calculate the intersection (logical AND) between the two binary images\n",
    "    intersection = np.logical_and(image1, image2).sum()\n",
    "\n",
    "    # Calculate the sum of pixels in each image\n",
    "    sum_image1 = image1.sum()\n",
    "    sum_image2 = image2.sum()\n",
    "\n",
    "    # Calculate the Dice coefficient\n",
    "    dice = (2.0 * intersection) / (sum_image1 + sum_image2)\n",
    "\n",
    "    return dice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next block, we will loop through all sub folders, register all images appears in the training set, and reverse, and take the images from that specific big class, directly from the original data folder, and throw them to a temporary folder.\n",
    "\n",
    "Now that we have the `filematrix`, we can go over each coloumn, pull out files into a temporary folder and make a round of prediction using that model if the value is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f9404244290> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f9404244290> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/panda/anaconda3/envs/unet/lib/python3.7/site-packages/ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with folder  0\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f94042444d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f94042444d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Done with folder  1\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f93e4342440> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f93e4342440> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Done with folder  2\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f93e4342cb0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f93e4342cb0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Done with folder  3\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f93d6543b90> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f93d6543b90> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Done with folder  4\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f93e4373b00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f93e4373b00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Done with folder  5\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f94040a5a70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f94040a5a70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Done with folder  6\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f940ec16dd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f940ec16dd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Done with folder  7\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f9404128050> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f9404128050> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Done with folder  8\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f945c05ed40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f945c05ed40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with folder  9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scorematrix = np.zeros((n,m))\n",
    "for img_type in ['polar', 'carte']:\n",
    "    for_counter = 0\n",
    "    if img_type == 'polar':\n",
    "        working_parent_folder = PARAM_PATH_TEMP_POLAR\n",
    "        src_folder = PARAM_PATH_POLAR\n",
    "    else:\n",
    "        working_parent_folder = PARAM_PATH_TEMP_CARTE\n",
    "        src_folder = PARAM_PATH_CARTE\n",
    "        for_counter += 1\n",
    "    for i in range(K):\n",
    "        current_folder_index = i + for_counter * K\n",
    "        temp_test_folder_name = 'temptest'\n",
    "        #print(working_parent_folder)\n",
    "        if os.path.exists(temp_test_folder_name):\n",
    "            shutil.rmtree(temp_test_folder_name)\n",
    "        temp_test_img_folder = os.path.join(temp_test_folder_name,PARAM_IMG_FOLDER)\n",
    "        temp_test_msk_folder = os.path.join(temp_test_folder_name,PARAM_MSK_FOLDER)\n",
    "        os.makedirs(temp_test_img_folder)\n",
    "        os.makedirs(temp_test_msk_folder)\n",
    "        test_n_count = 0\n",
    "        for indice in indices:\n",
    "            if indice[1] == current_folder_index:\n",
    "                test_n_count += 1\n",
    "                img_name = str(indice[0]) + '.' + img_extenstion\n",
    "                src = os.path.join(src_folder,PARAM_IMG_FOLDER,img_name)\n",
    "                shutil.copy2(src, temp_test_img_folder)\n",
    "                src = os.path.join(src_folder,PARAM_MSK_FOLDER,img_name)\n",
    "                shutil.copy2(src, temp_test_msk_folder)\n",
    "        \n",
    "        model_path = os.path.join(working_parent_folder, str(i), 'checkpoint.hdf5')\n",
    "        current_model = unet(PARAM_BETA1[PARAM_BETA_TEST_NUM], PARAM_BETA2[PARAM_BETA_TEST_NUM])\n",
    "        current_model.load_weights(model_path) \n",
    "        for test_image_name in os.listdir(temp_test_img_folder):\n",
    "            test_image_name_raw, ext = os.path.splitext(test_image_name)\n",
    "            image_path = os.path.join(temp_test_img_folder, test_image_name)\n",
    "            ground_truth_mask_path = os.path.join(temp_test_msk_folder, test_image_name)\n",
    "            \n",
    "            test_image = cv2.imread(image_path)\n",
    "            test_image = cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB)\n",
    "            test_image = test_image / 255.0\n",
    "            test_image = np.expand_dims(test_image,axis = 0)\n",
    "\n",
    "            ground_truth_mask = cv2.imread(ground_truth_mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "            ground_truth_mask = ground_truth_mask / 255.0\n",
    "            ground_truth_mask = ground_truth_mask.astype(np.uint8)\n",
    "            \n",
    "            prediction = current_model.predict(test_image, verbose = 0)\n",
    "            \n",
    "            threshold = 0.5\n",
    "            binary_mask = (prediction > threshold).astype(np.uint8)\n",
    "            binary_mask = binary_mask[0,:,:,0]\n",
    "            dice = dice_coefficient(ground_truth_mask, binary_mask)\n",
    "            scorematrix[int(test_image_name_raw), current_folder_index] = dice\n",
    "        print('Done with folder ', current_folder_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(scorematrix[9,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuiling's K-fold function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K fold Validation (obtain training & testing sets)\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold = KFold(n_splits=PARAM_SPLIT_NUM)\n",
    "for train_index,test_index in kfold.split(dfPolar):\n",
    "    polar_train,polar_test=dfPolar.iloc[train_index, :],dfPolar.iloc[test_index, :]\n",
    "    cartesian_train,cartesian_test=dfCartesian.iloc[train_index, :],dfCartesian.iloc[test_index, :]\n",
    "    #print(\"polar train: \", polar_train, \"polar test: \", polar_test)\n",
    "    #print(\"cartesian train\" , cartesian_train, \"cartesian test\", cartesian_test)\n",
    "    batch_size = 3\n",
    "    PARAM_BETA_TEST_NUM = 6\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "    data_gen_args = dict(rotation_range = 80,      # TODO: improve the data augmentation\n",
    "                width_shift_range =0.02,\n",
    "                height_shift_range =0.02,\n",
    "                shear_range = 0.35,\n",
    "                zoom_range = 0.075,\n",
    "                horizontal_flip = True,\n",
    "                fill_mode = 'nearest',\n",
    "                rescale = 1./255)\n",
    "    print('-------------------------------------------------------------')\n",
    "    test_model = unet(PARAM_BETA1[PARAM_BETA_TEST_NUM], PARAM_BETA2[PARAM_BETA_TEST_NUM]) \n",
    "    test_run = test_model.fit(polar_train, polar_test, verbose = 1, steps_per_epoch = 50, epochs = 5, callbacks = [early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training test\n",
    "\n",
    "This part is used to see if we can train a model using the current configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Superparameters (temporary) for a test run of model training test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "PARAM_BETA_TEST_NUM = 6\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "data_gen_args = dict(rotation_range = 80,      # TODO: improve the data augmentation\n",
    "                width_shift_range =0.02,\n",
    "                height_shift_range =0.02,\n",
    "                shear_range = 0.35,\n",
    "                zoom_range = 0.075,\n",
    "                horizontal_flip = True,\n",
    "                fill_mode = 'nearest',\n",
    "                rescale = 1./255)\n",
    "test_gene = trainGenerator(batch_size, PARAM_PATH_CARTE, PARAM_IMG_FOLDER, PARAM_MSK_FOLDER, data_gen_args)\n",
    "test_model = unet(PARAM_BETA1[PARAM_BETA_TEST_NUM], PARAM_BETA2[PARAM_BETA_TEST_NUM]) \n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output\n",
    "<details>\n",
    "    <summary><b><font color=\"green\">Click here to expand</font></b></summary>\n",
    "    <code>\n",
    "Model: \"model_5\"\n",
    "__________________________________________________________________________________________________\n",
    " Layer (type)                   Output Shape         Param #     Connected to                     \n",
    "==================================================================================================\n",
    " input_6 (InputLayer)           [(None, 256, 256, 3  0           []                               \n",
    "                                )]                                                                \n",
    "                                                                                                  \n",
    " conv2d_120 (Conv2D)            (None, 256, 256, 64  1792        ['input_6[0][0]']                \n",
    "                                )                                                                 \n",
    "                                                                                                  \n",
    " conv2d_121 (Conv2D)            (None, 256, 256, 64  36928       ['conv2d_120[0][0]']             \n",
    "                                )                                                                 \n",
    "                                                                                                  \n",
    " max_pooling2d_20 (MaxPooling2D  (None, 128, 128, 64  0          ['conv2d_121[0][0]']             \n",
    " )                              )                                                                 \n",
    "                                                                                                  \n",
    " conv2d_122 (Conv2D)            (None, 128, 128, 12  73856       ['max_pooling2d_20[0][0]']       \n",
    "                                8)                                                                \n",
    "                                                                                                  \n",
    " conv2d_123 (Conv2D)            (None, 128, 128, 12  147584      ['conv2d_122[0][0]']             \n",
    "                                8)                                                                \n",
    "                                                                                                  \n",
    " max_pooling2d_21 (MaxPooling2D  (None, 64, 64, 128)  0          ['conv2d_123[0][0]']             \n",
    " )                                                                                                \n",
    "                                                                                                  \n",
    " conv2d_124 (Conv2D)            (None, 64, 64, 256)  295168      ['max_pooling2d_21[0][0]']       \n",
    "                                                                                                  \n",
    " conv2d_125 (Conv2D)            (None, 64, 64, 256)  590080      ['conv2d_124[0][0]']             \n",
    "                                                                                                  \n",
    " max_pooling2d_22 (MaxPooling2D  (None, 32, 32, 256)  0          ['conv2d_125[0][0]']             \n",
    " )                                                                                                \n",
    "                                                                                                  \n",
    " conv2d_126 (Conv2D)            (None, 32, 32, 512)  1180160     ['max_pooling2d_22[0][0]']       \n",
    "                                                                                                  \n",
    " conv2d_127 (Conv2D)            (None, 32, 32, 512)  2359808     ['conv2d_126[0][0]']             \n",
    "                                                                                                  \n",
    " dropout_10 (Dropout)           (None, 32, 32, 512)  0           ['conv2d_127[0][0]']             \n",
    "                                                                                                  \n",
    " max_pooling2d_23 (MaxPooling2D  (None, 16, 16, 512)  0          ['dropout_10[0][0]']             \n",
    " )                                                                                                \n",
    "                                                                                                  \n",
    " conv2d_128 (Conv2D)            (None, 16, 16, 1024  4719616     ['max_pooling2d_23[0][0]']       \n",
    "                                )                                                                 \n",
    "                                                                                                  \n",
    " conv2d_129 (Conv2D)            (None, 16, 16, 1024  9438208     ['conv2d_128[0][0]']             \n",
    "                                )                                                                 \n",
    "                                                                                                  \n",
    " dropout_11 (Dropout)           (None, 16, 16, 1024  0           ['conv2d_129[0][0]']             \n",
    "                                )                                                                 \n",
    "                                                                                                  \n",
    " up_sampling2d_20 (UpSampling2D  (None, 32, 32, 1024  0          ['dropout_11[0][0]']             \n",
    " )                              )                                                                 \n",
    "                                                                                                  \n",
    " conv2d_130 (Conv2D)            (None, 32, 32, 512)  2097664     ['up_sampling2d_20[0][0]']       \n",
    "                                                                                                  \n",
    " concatenate_20 (Concatenate)   (None, 32, 32, 1024  0           ['dropout_10[0][0]',             \n",
    "                                )                                 'conv2d_130[0][0]']             \n",
    "                                                                                                  \n",
    " conv2d_131 (Conv2D)            (None, 32, 32, 512)  4719104     ['concatenate_20[0][0]']         \n",
    "                                                                                                  \n",
    " conv2d_132 (Conv2D)            (None, 32, 32, 512)  2359808     ['conv2d_131[0][0]']             \n",
    "                                                                                                  \n",
    " up_sampling2d_21 (UpSampling2D  (None, 64, 64, 512)  0          ['conv2d_132[0][0]']             \n",
    " )                                                                                                \n",
    "                                                                                                  \n",
    " conv2d_133 (Conv2D)            (None, 64, 64, 256)  524544      ['up_sampling2d_21[0][0]']       \n",
    "                                                                                                  \n",
    " concatenate_21 (Concatenate)   (None, 64, 64, 512)  0           ['conv2d_125[0][0]',             \n",
    "                                                                  'conv2d_133[0][0]']             \n",
    "                                                                                                  \n",
    " conv2d_134 (Conv2D)            (None, 64, 64, 256)  1179904     ['concatenate_21[0][0]']         \n",
    "                                                                                                  \n",
    " conv2d_135 (Conv2D)            (None, 64, 64, 256)  590080      ['conv2d_134[0][0]']             \n",
    "                                                                                                  \n",
    " up_sampling2d_22 (UpSampling2D  (None, 128, 128, 25  0          ['conv2d_135[0][0]']             \n",
    " )                              6)                                                                \n",
    "                                                                                                  \n",
    " conv2d_136 (Conv2D)            (None, 128, 128, 12  131200      ['up_sampling2d_22[0][0]']       \n",
    "                                8)                                                                \n",
    "                                                                                                  \n",
    " concatenate_22 (Concatenate)   (None, 128, 128, 25  0           ['conv2d_123[0][0]',             \n",
    "                                6)                                'conv2d_136[0][0]']             \n",
    "                                                                                                  \n",
    " conv2d_137 (Conv2D)            (None, 128, 128, 12  295040      ['concatenate_22[0][0]']         \n",
    "                                8)                                                                \n",
    "                                                                                                  \n",
    " conv2d_138 (Conv2D)            (None, 128, 128, 12  147584      ['conv2d_137[0][0]']             \n",
    "                                8)                                                                \n",
    "                                                                                                  \n",
    " up_sampling2d_23 (UpSampling2D  (None, 256, 256, 12  0          ['conv2d_138[0][0]']             \n",
    " )                              8)                                                                \n",
    "                                                                                                  \n",
    " conv2d_139 (Conv2D)            (None, 256, 256, 64  32832       ['up_sampling2d_23[0][0]']       \n",
    "                                )                                                                 \n",
    "                                                                                                  \n",
    " concatenate_23 (Concatenate)   (None, 256, 256, 12  0           ['conv2d_121[0][0]',             \n",
    "                                8)                                'conv2d_139[0][0]']             \n",
    "                                                                                                  \n",
    " conv2d_140 (Conv2D)            (None, 256, 256, 64  73792       ['concatenate_23[0][0]']         \n",
    "                                )                                                                 \n",
    "                                                                                                  \n",
    " conv2d_141 (Conv2D)            (None, 256, 256, 64  36928       ['conv2d_140[0][0]']             \n",
    "                                )                                                                 \n",
    "                                                                                                  \n",
    " conv2d_142 (Conv2D)            (None, 256, 256, 9)  5193        ['conv2d_141[0][0]']             \n",
    "                                                                                                  \n",
    " conv2d_143 (Conv2D)            (None, 256, 256, 3)  30          ['conv2d_142[0][0]']             \n",
    "                                                                                                  \n",
    "==================================================================================================\n",
    "Total params: 31,036,903\n",
    "Trainable params: 31,036,903\n",
    "Non-trainable params: 0\n",
    "</code>\n",
    "</details>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = ModelCheckpoint('unet_endoscopic.hdf5', monitor = 'loss', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item1, item2 in test_gene:\n",
    "    print(item1.shape)\n",
    "    print(item2.shape)\n",
    "    print('----------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_run = test_model.fit(test_gene, verbose = 1, steps_per_epoch = 100, epochs = 100, callbacks = [model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(test_run.history.keys())\n",
    "plt.plot(test_run.history['loss'])\n",
    "plt.plot(test_run.history['accuracy'])\n",
    "plt.title('Test Run')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['loss', 'accuracy'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAM_PATH_TEST = './test'\n",
    "image_name = '14.tif'\n",
    "img = io.imread(os.path.join(PARAM_PATH_TEST,image_name),as_gray = False)\n",
    "img = trans.resize(img,[256,256])\n",
    "img = np.reshape(img,(1,)+img.shape)\n",
    "\n",
    "results = test_model.predict(img,1,verbose=1)\n",
    "#saveResult(Path,results)\n",
    "img = results[0,:,:]\n",
    "print(results.shape)\n",
    "io.imsave(os.path.join(PARAM_PATH_TEST,\"result.png\"),img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
